{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model for Microsoft\n",
    "Sentiment is built upon the Reuters titles dataset.\n",
    "Historical data is taken from yahoo finance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\48570\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:20: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.*` instead of `tqdm._tqdm_notebook.*`\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pickle\n",
    "import nltk\n",
    "import string\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import requests\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "import time\n",
    "import sys\n",
    "import time\n",
    "from tqdm._tqdm_notebook import tqdm_notebook\n",
    "from keras.models import Sequential, load_model\n",
    "from keras import layers\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, CSVLogger\n",
    "from keras import optimizers\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "from io import StringIO\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping historical data from yahoo finance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YahooFinanceHistory:\n",
    "    timeout = 2\n",
    "    crumb_link = 'https://finance.yahoo.com/quote/{0}/history?p={0}'\n",
    "    crumble_regex = r'CrumbStore\":{\"crumb\":\"(.*?)\"}'\n",
    "    quote_link = 'https://query1.finance.yahoo.com/v7/finance/download/{quote}?period1={dfrom}&period2={dto}&interval=1d&events=history&crumb={crumb}'\n",
    "\n",
    "    def __init__(self, symbol, days_back=7):\n",
    "        self.symbol = symbol\n",
    "        self.session = requests.Session()\n",
    "        self.dt = timedelta(days=days_back)\n",
    "\n",
    "#requesting crumb and cookie\n",
    "    def get_crumb(self):\n",
    "        response = self.session.get(self.crumb_link.format(self.symbol), timeout=self.timeout)\n",
    "        response.raise_for_status()\n",
    "        match = re.search(self.crumble_regex, response.text)\n",
    "        if not match:\n",
    "            raise ValueError('Could not get crumb from Yahoo Finance')\n",
    "        else:\n",
    "            self.crumb = match.group(1)\n",
    "\n",
    "#requesting data\n",
    "    def get_quote(self):\n",
    "        if not hasattr(self, 'crumb') or len(self.session.cookies) == 0:\n",
    "            self.get_crumb()\n",
    "        now = datetime.utcnow()\n",
    "        dateto = int(now.timestamp())\n",
    "        datefrom = int((now - self.dt).timestamp())\n",
    "        url = self.quote_link.format(quote=self.symbol, dfrom=datefrom, dto=dateto, crumb=self.crumb)\n",
    "        response = self.session.get(url)\n",
    "        response.raise_for_status()\n",
    "        return pd.read_csv(StringIO(response.text), parse_dates=['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting data about Microsoft from 400 days back\n",
    "df_v = YahooFinanceHistory('MSFT', days_back=4000).get_quote()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2009-06-04</td>\n",
       "      <td>21.770000</td>\n",
       "      <td>21.900000</td>\n",
       "      <td>21.580000</td>\n",
       "      <td>21.830000</td>\n",
       "      <td>16.964777</td>\n",
       "      <td>42330000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2009-06-05</td>\n",
       "      <td>21.959999</td>\n",
       "      <td>22.309999</td>\n",
       "      <td>21.809999</td>\n",
       "      <td>22.139999</td>\n",
       "      <td>17.205687</td>\n",
       "      <td>59579200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2009-06-08</td>\n",
       "      <td>21.980000</td>\n",
       "      <td>22.320000</td>\n",
       "      <td>21.629999</td>\n",
       "      <td>22.049999</td>\n",
       "      <td>17.135746</td>\n",
       "      <td>49000600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2009-06-09</td>\n",
       "      <td>22.059999</td>\n",
       "      <td>22.320000</td>\n",
       "      <td>21.879999</td>\n",
       "      <td>22.080000</td>\n",
       "      <td>17.159063</td>\n",
       "      <td>50887700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2009-06-10</td>\n",
       "      <td>22.170000</td>\n",
       "      <td>22.620001</td>\n",
       "      <td>22.120001</td>\n",
       "      <td>22.549999</td>\n",
       "      <td>17.524307</td>\n",
       "      <td>61297200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2752</th>\n",
       "      <td>2020-05-11</td>\n",
       "      <td>183.149994</td>\n",
       "      <td>187.509995</td>\n",
       "      <td>182.850006</td>\n",
       "      <td>186.740005</td>\n",
       "      <td>186.740005</td>\n",
       "      <td>30809400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2753</th>\n",
       "      <td>2020-05-12</td>\n",
       "      <td>186.800003</td>\n",
       "      <td>187.039993</td>\n",
       "      <td>182.300003</td>\n",
       "      <td>182.509995</td>\n",
       "      <td>182.509995</td>\n",
       "      <td>32038200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2754</th>\n",
       "      <td>2020-05-13</td>\n",
       "      <td>182.550003</td>\n",
       "      <td>184.050003</td>\n",
       "      <td>176.539993</td>\n",
       "      <td>179.750000</td>\n",
       "      <td>179.750000</td>\n",
       "      <td>44711500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2755</th>\n",
       "      <td>2020-05-14</td>\n",
       "      <td>177.539993</td>\n",
       "      <td>180.690002</td>\n",
       "      <td>175.679993</td>\n",
       "      <td>180.529999</td>\n",
       "      <td>180.529999</td>\n",
       "      <td>41873900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2756</th>\n",
       "      <td>2020-05-15</td>\n",
       "      <td>179.059998</td>\n",
       "      <td>187.059998</td>\n",
       "      <td>177.000000</td>\n",
       "      <td>183.160004</td>\n",
       "      <td>183.160004</td>\n",
       "      <td>46597900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2757 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date        Open        High         Low       Close   Adj Close  \\\n",
       "0    2009-06-04   21.770000   21.900000   21.580000   21.830000   16.964777   \n",
       "1    2009-06-05   21.959999   22.309999   21.809999   22.139999   17.205687   \n",
       "2    2009-06-08   21.980000   22.320000   21.629999   22.049999   17.135746   \n",
       "3    2009-06-09   22.059999   22.320000   21.879999   22.080000   17.159063   \n",
       "4    2009-06-10   22.170000   22.620001   22.120001   22.549999   17.524307   \n",
       "...         ...         ...         ...         ...         ...         ...   \n",
       "2752 2020-05-11  183.149994  187.509995  182.850006  186.740005  186.740005   \n",
       "2753 2020-05-12  186.800003  187.039993  182.300003  182.509995  182.509995   \n",
       "2754 2020-05-13  182.550003  184.050003  176.539993  179.750000  179.750000   \n",
       "2755 2020-05-14  177.539993  180.690002  175.679993  180.529999  180.529999   \n",
       "2756 2020-05-15  179.059998  187.059998  177.000000  183.160004  183.160004   \n",
       "\n",
       "        Volume  \n",
       "0     42330000  \n",
       "1     59579200  \n",
       "2     49000600  \n",
       "3     50887700  \n",
       "4     61297200  \n",
       "...        ...  \n",
       "2752  30809400  \n",
       "2753  32038200  \n",
       "2754  44711500  \n",
       "2755  41873900  \n",
       "2756  46597900  \n",
       "\n",
       "[2757 rows x 7 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sorting dates from the latest to earliest\n",
    "df_v.sort_values(by='Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date         datetime64[ns]\n",
       "Open                float64\n",
       "High                float64\n",
       "Low                 float64\n",
       "Close               float64\n",
       "Adj Close           float64\n",
       "Volume                int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_v.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2009-06-04</td>\n",
       "      <td>21.770000</td>\n",
       "      <td>21.900000</td>\n",
       "      <td>21.580000</td>\n",
       "      <td>21.830000</td>\n",
       "      <td>16.964777</td>\n",
       "      <td>42330000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2009-06-05</td>\n",
       "      <td>21.959999</td>\n",
       "      <td>22.309999</td>\n",
       "      <td>21.809999</td>\n",
       "      <td>22.139999</td>\n",
       "      <td>17.205687</td>\n",
       "      <td>59579200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2009-06-08</td>\n",
       "      <td>21.980000</td>\n",
       "      <td>22.320000</td>\n",
       "      <td>21.629999</td>\n",
       "      <td>22.049999</td>\n",
       "      <td>17.135746</td>\n",
       "      <td>49000600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2009-06-09</td>\n",
       "      <td>22.059999</td>\n",
       "      <td>22.320000</td>\n",
       "      <td>21.879999</td>\n",
       "      <td>22.080000</td>\n",
       "      <td>17.159063</td>\n",
       "      <td>50887700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2009-06-10</td>\n",
       "      <td>22.170000</td>\n",
       "      <td>22.620001</td>\n",
       "      <td>22.120001</td>\n",
       "      <td>22.549999</td>\n",
       "      <td>17.524307</td>\n",
       "      <td>61297200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2752</th>\n",
       "      <td>2020-05-11</td>\n",
       "      <td>183.149994</td>\n",
       "      <td>187.509995</td>\n",
       "      <td>182.850006</td>\n",
       "      <td>186.740005</td>\n",
       "      <td>186.740005</td>\n",
       "      <td>30809400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2753</th>\n",
       "      <td>2020-05-12</td>\n",
       "      <td>186.800003</td>\n",
       "      <td>187.039993</td>\n",
       "      <td>182.300003</td>\n",
       "      <td>182.509995</td>\n",
       "      <td>182.509995</td>\n",
       "      <td>32038200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2754</th>\n",
       "      <td>2020-05-13</td>\n",
       "      <td>182.550003</td>\n",
       "      <td>184.050003</td>\n",
       "      <td>176.539993</td>\n",
       "      <td>179.750000</td>\n",
       "      <td>179.750000</td>\n",
       "      <td>44711500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2755</th>\n",
       "      <td>2020-05-14</td>\n",
       "      <td>177.539993</td>\n",
       "      <td>180.690002</td>\n",
       "      <td>175.679993</td>\n",
       "      <td>180.529999</td>\n",
       "      <td>180.529999</td>\n",
       "      <td>41873900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2756</th>\n",
       "      <td>2020-05-15</td>\n",
       "      <td>179.059998</td>\n",
       "      <td>187.059998</td>\n",
       "      <td>177.000000</td>\n",
       "      <td>183.160004</td>\n",
       "      <td>183.160004</td>\n",
       "      <td>46597900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2757 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date        Open        High         Low       Close   Adj Close  \\\n",
       "0    2009-06-04   21.770000   21.900000   21.580000   21.830000   16.964777   \n",
       "1    2009-06-05   21.959999   22.309999   21.809999   22.139999   17.205687   \n",
       "2    2009-06-08   21.980000   22.320000   21.629999   22.049999   17.135746   \n",
       "3    2009-06-09   22.059999   22.320000   21.879999   22.080000   17.159063   \n",
       "4    2009-06-10   22.170000   22.620001   22.120001   22.549999   17.524307   \n",
       "...         ...         ...         ...         ...         ...         ...   \n",
       "2752 2020-05-11  183.149994  187.509995  182.850006  186.740005  186.740005   \n",
       "2753 2020-05-12  186.800003  187.039993  182.300003  182.509995  182.509995   \n",
       "2754 2020-05-13  182.550003  184.050003  176.539993  179.750000  179.750000   \n",
       "2755 2020-05-14  177.539993  180.690002  175.679993  180.529999  180.529999   \n",
       "2756 2020-05-15  179.059998  187.059998  177.000000  183.160004  183.160004   \n",
       "\n",
       "        Volume  \n",
       "0     42330000  \n",
       "1     59579200  \n",
       "2     49000600  \n",
       "3     50887700  \n",
       "4     61297200  \n",
       "...        ...  \n",
       "2752  30809400  \n",
       "2753  32038200  \n",
       "2754  44711500  \n",
       "2755  41873900  \n",
       "2756  46597900  \n",
       "\n",
       "[2757 rows x 7 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment for all the articles with \"Microsoft\" in the body of an article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uploading file\n",
    "df2 = pd.read_csv('df_MS.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Date</th>\n",
       "      <th>compound_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2011-07-06</td>\n",
       "      <td>0.075921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2011-07-07</td>\n",
       "      <td>0.765000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2011-07-08</td>\n",
       "      <td>0.427933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2011-07-09</td>\n",
       "      <td>0.075921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2011-07-11</td>\n",
       "      <td>0.446600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>820</th>\n",
       "      <td>820</td>\n",
       "      <td>2016-12-05</td>\n",
       "      <td>0.075921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>821</th>\n",
       "      <td>821</td>\n",
       "      <td>2016-12-06</td>\n",
       "      <td>0.670500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>822</th>\n",
       "      <td>822</td>\n",
       "      <td>2016-12-13</td>\n",
       "      <td>0.381800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>823</th>\n",
       "      <td>823</td>\n",
       "      <td>2016-12-20</td>\n",
       "      <td>0.571900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>824</th>\n",
       "      <td>824</td>\n",
       "      <td>2017-01-13</td>\n",
       "      <td>0.617000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>825 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0        Date  compound_mean\n",
       "0             0  2011-07-06       0.075921\n",
       "1             1  2011-07-07       0.765000\n",
       "2             2  2011-07-08       0.427933\n",
       "3             3  2011-07-09       0.075921\n",
       "4             4  2011-07-11       0.446600\n",
       "..          ...         ...            ...\n",
       "820         820  2016-12-05       0.075921\n",
       "821         821  2016-12-06       0.670500\n",
       "822         822  2016-12-13       0.381800\n",
       "823         823  2016-12-20       0.571900\n",
       "824         824  2017-01-13       0.617000\n",
       "\n",
       "[825 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# column Unnamed isn't needed\n",
    "df2 = df2.drop(['Unnamed: 0'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>compound_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011-07-06</td>\n",
       "      <td>0.075921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011-07-07</td>\n",
       "      <td>0.765000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011-07-08</td>\n",
       "      <td>0.427933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2011-07-09</td>\n",
       "      <td>0.075921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2011-07-11</td>\n",
       "      <td>0.446600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>820</th>\n",
       "      <td>2016-12-05</td>\n",
       "      <td>0.075921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>821</th>\n",
       "      <td>2016-12-06</td>\n",
       "      <td>0.670500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>822</th>\n",
       "      <td>2016-12-13</td>\n",
       "      <td>0.381800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>823</th>\n",
       "      <td>2016-12-20</td>\n",
       "      <td>0.571900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>824</th>\n",
       "      <td>2017-01-13</td>\n",
       "      <td>0.617000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>825 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date  compound_mean\n",
       "0    2011-07-06       0.075921\n",
       "1    2011-07-07       0.765000\n",
       "2    2011-07-08       0.427933\n",
       "3    2011-07-09       0.075921\n",
       "4    2011-07-11       0.446600\n",
       "..          ...            ...\n",
       "820  2016-12-05       0.075921\n",
       "821  2016-12-06       0.670500\n",
       "822  2016-12-13       0.381800\n",
       "823  2016-12-20       0.571900\n",
       "824  2017-01-13       0.617000\n",
       "\n",
       "[825 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date              object\n",
       "compound_mean    float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#changing column Date type to datetime type\n",
    "df2.Date=pd.to_datetime(df2['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merging dataframe with historical data with dataframe with sentiments \n",
    "df3 = pd.merge(df_v,df2,on='Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>compound_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011-07-06</td>\n",
       "      <td>25.969999</td>\n",
       "      <td>26.370001</td>\n",
       "      <td>25.959999</td>\n",
       "      <td>26.330000</td>\n",
       "      <td>21.362141</td>\n",
       "      <td>48744200</td>\n",
       "      <td>0.075921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011-07-07</td>\n",
       "      <td>26.490000</td>\n",
       "      <td>26.879999</td>\n",
       "      <td>26.360001</td>\n",
       "      <td>26.770000</td>\n",
       "      <td>21.719128</td>\n",
       "      <td>51946500</td>\n",
       "      <td>0.765000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011-07-08</td>\n",
       "      <td>26.540001</td>\n",
       "      <td>26.980000</td>\n",
       "      <td>26.510000</td>\n",
       "      <td>26.920000</td>\n",
       "      <td>21.840824</td>\n",
       "      <td>58320700</td>\n",
       "      <td>0.427933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2011-07-11</td>\n",
       "      <td>26.620001</td>\n",
       "      <td>26.799999</td>\n",
       "      <td>26.490000</td>\n",
       "      <td>26.629999</td>\n",
       "      <td>21.605539</td>\n",
       "      <td>43999800</td>\n",
       "      <td>0.446600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2011-07-12</td>\n",
       "      <td>26.549999</td>\n",
       "      <td>26.790001</td>\n",
       "      <td>26.340000</td>\n",
       "      <td>26.540001</td>\n",
       "      <td>21.532520</td>\n",
       "      <td>47319300</td>\n",
       "      <td>0.440400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>747</th>\n",
       "      <td>2016-12-05</td>\n",
       "      <td>59.700001</td>\n",
       "      <td>60.590000</td>\n",
       "      <td>59.560001</td>\n",
       "      <td>60.220001</td>\n",
       "      <td>56.903275</td>\n",
       "      <td>23552700</td>\n",
       "      <td>0.075921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>748</th>\n",
       "      <td>2016-12-06</td>\n",
       "      <td>60.430000</td>\n",
       "      <td>60.459999</td>\n",
       "      <td>59.799999</td>\n",
       "      <td>59.950001</td>\n",
       "      <td>56.648148</td>\n",
       "      <td>19907000</td>\n",
       "      <td>0.670500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>749</th>\n",
       "      <td>2016-12-13</td>\n",
       "      <td>62.500000</td>\n",
       "      <td>63.419998</td>\n",
       "      <td>62.240002</td>\n",
       "      <td>62.980000</td>\n",
       "      <td>59.511261</td>\n",
       "      <td>35718900</td>\n",
       "      <td>0.381800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750</th>\n",
       "      <td>2016-12-20</td>\n",
       "      <td>63.689999</td>\n",
       "      <td>63.799999</td>\n",
       "      <td>63.029999</td>\n",
       "      <td>63.540001</td>\n",
       "      <td>60.040417</td>\n",
       "      <td>26028400</td>\n",
       "      <td>0.571900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>751</th>\n",
       "      <td>2017-01-13</td>\n",
       "      <td>62.619999</td>\n",
       "      <td>62.869999</td>\n",
       "      <td>62.349998</td>\n",
       "      <td>62.700001</td>\n",
       "      <td>59.246685</td>\n",
       "      <td>19422300</td>\n",
       "      <td>0.617000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>752 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date       Open       High        Low      Close  Adj Close  \\\n",
       "0   2011-07-06  25.969999  26.370001  25.959999  26.330000  21.362141   \n",
       "1   2011-07-07  26.490000  26.879999  26.360001  26.770000  21.719128   \n",
       "2   2011-07-08  26.540001  26.980000  26.510000  26.920000  21.840824   \n",
       "3   2011-07-11  26.620001  26.799999  26.490000  26.629999  21.605539   \n",
       "4   2011-07-12  26.549999  26.790001  26.340000  26.540001  21.532520   \n",
       "..         ...        ...        ...        ...        ...        ...   \n",
       "747 2016-12-05  59.700001  60.590000  59.560001  60.220001  56.903275   \n",
       "748 2016-12-06  60.430000  60.459999  59.799999  59.950001  56.648148   \n",
       "749 2016-12-13  62.500000  63.419998  62.240002  62.980000  59.511261   \n",
       "750 2016-12-20  63.689999  63.799999  63.029999  63.540001  60.040417   \n",
       "751 2017-01-13  62.619999  62.869999  62.349998  62.700001  59.246685   \n",
       "\n",
       "       Volume  compound_mean  \n",
       "0    48744200       0.075921  \n",
       "1    51946500       0.765000  \n",
       "2    58320700       0.427933  \n",
       "3    43999800       0.446600  \n",
       "4    47319300       0.440400  \n",
       "..        ...            ...  \n",
       "747  23552700       0.075921  \n",
       "748  19907000       0.670500  \n",
       "749  35718900       0.381800  \n",
       "750  26028400       0.571900  \n",
       "751  19422300       0.617000  \n",
       "\n",
       "[752 rows x 8 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning for prediction of label for the next day "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#deepcopying dataframe, so there would be no need to run everything from the beggining\n",
    "df = copy.deepcopy(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label indicates wheter the price will go up(1) or down(0) next day\n",
    "def add_label(dfi):\n",
    "    idx = len(dfi.columns)\n",
    "    new_col = np.where(dfi['Close'] >= dfi['Close'].shift(1), 1, 0)  \n",
    "    dfi.insert(loc=idx, column='Label', value=new_col)\n",
    "    dfi = dfi.fillna(0)\n",
    "    df['Label'] =  df['Label'].shift(-1, axis = 0)\n",
    "    df.drop(df.index[len(df)-1], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_label(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date             datetime64[ns]\n",
       "Open                    float64\n",
       "High                    float64\n",
       "Low                     float64\n",
       "Close                   float64\n",
       "Adj Close               float64\n",
       "Volume                    int64\n",
       "compound_mean           float64\n",
       "Label                   float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "array = df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and test\n",
    "X = array[:,1:8]\n",
    "Y = array[:,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sklearn.preprocessing.MinMaxScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = Y.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.04891574 0.04782501 0.05530067 0.05940344 0.04408596 0.16596441\n",
      " 0.52541799]\n",
      "Index(['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume', 'compound_mean',\n",
      "       'Label'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(X[0])\n",
    "print(df.columns[1:9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.06  0.06  0.066 0.066 0.04  0.001 0.051]\n",
      "[[0.049 0.048 0.055 0.059 0.525]\n",
      " [0.062 0.061 0.065 0.071 0.938]\n",
      " [0.063 0.063 0.069 0.074 0.736]\n",
      " [0.065 0.059 0.069 0.067 0.747]\n",
      " [0.064 0.059 0.065 0.065 0.744]]\n"
     ]
    }
   ],
   "source": [
    "# choosing best features for the model\n",
    "test = SelectKBest(score_func=chi2, k=5)\n",
    "fit = test.fit(X, Y)\n",
    "np.set_printoptions(precision=3)\n",
    "print(fit.scores_)\n",
    "features = fit.transform(X)\n",
    "print(features[0:5,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.049, 0.048, 0.055, 0.059, 0.525],\n",
       "       [0.062, 0.061, 0.065, 0.071, 0.938],\n",
       "       [0.063, 0.063, 0.069, 0.074, 0.736],\n",
       "       ...,\n",
       "       [0.918, 0.915, 0.918, 0.909, 0.881],\n",
       "       [0.97 , 0.99 , 0.98 , 0.986, 0.708],\n",
       "       [1.   , 1.   , 1.   , 1.   , 0.822]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Open, High, Low, Close and compound mean give the most information\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 563 samples, validate on 188 samples\n",
      "Epoch 1/100\n",
      "563/563 [==============================] - ETA: 23s - loss: 0.7019 - accuracy: 0.500 - ETA: 1s - loss: 0.6953 - accuracy: 0.486 - ETA: 0s - loss: 0.6929 - accuracy: 0.50 - 2s 3ms/step - loss: 0.6935 - accuracy: 0.4920 - val_loss: 0.6971 - val_accuracy: 0.4734\n",
      "Epoch 2/100\n",
      "563/563 [==============================] - ETA: 0s - loss: 0.6782 - accuracy: 0.62 - ETA: 0s - loss: 0.6934 - accuracy: 0.46 - ETA: 0s - loss: 0.6968 - accuracy: 0.48 - 0s 294us/step - loss: 0.6959 - accuracy: 0.4938 - val_loss: 0.6920 - val_accuracy: 0.5266\n",
      "Epoch 3/100\n",
      "563/563 [==============================] - ETA: 0s - loss: 0.7037 - accuracy: 0.43 - ETA: 0s - loss: 0.6974 - accuracy: 0.49 - ETA: 0s - loss: 0.6974 - accuracy: 0.50 - 0s 269us/step - loss: 0.6988 - accuracy: 0.5080 - val_loss: 0.6919 - val_accuracy: 0.5266\n",
      "Epoch 4/100\n",
      "563/563 [==============================] - ETA: 0s - loss: 0.6783 - accuracy: 0.65 - ETA: 0s - loss: 0.6954 - accuracy: 0.53 - ETA: 0s - loss: 0.6941 - accuracy: 0.52 - 0s 289us/step - loss: 0.6934 - accuracy: 0.5275 - val_loss: 0.6926 - val_accuracy: 0.5266\n",
      "Epoch 5/100\n",
      "563/563 [==============================] - ETA: 0s - loss: 0.6964 - accuracy: 0.46 - ETA: 0s - loss: 0.6935 - accuracy: 0.46 - ETA: 0s - loss: 0.6947 - accuracy: 0.49 - ETA: 0s - loss: 0.6954 - accuracy: 0.50 - 0s 413us/step - loss: 0.6948 - accuracy: 0.5027 - val_loss: 0.6939 - val_accuracy: 0.4681\n",
      "Epoch 6/100\n",
      "563/563 [==============================] - ETA: 0s - loss: 0.6871 - accuracy: 0.62 - ETA: 0s - loss: 0.6942 - accuracy: 0.50 - ETA: 0s - loss: 0.6933 - accuracy: 0.50 - 0s 285us/step - loss: 0.6944 - accuracy: 0.4867 - val_loss: 0.6915 - val_accuracy: 0.5266\n",
      "Epoch 7/100\n",
      "563/563 [==============================] - ETA: 0s - loss: 0.6904 - accuracy: 0.46 - ETA: 0s - loss: 0.6964 - accuracy: 0.49 - ETA: 0s - loss: 0.6997 - accuracy: 0.49 - 0s 287us/step - loss: 0.6996 - accuracy: 0.4902 - val_loss: 0.6932 - val_accuracy: 0.5213\n",
      "Epoch 8/100\n",
      "563/563 [==============================] - ETA: 0s - loss: 0.6854 - accuracy: 0.56 - ETA: 0s - loss: 0.6915 - accuracy: 0.53 - ETA: 0s - loss: 0.6923 - accuracy: 0.53 - 0s 291us/step - loss: 0.6931 - accuracy: 0.5169 - val_loss: 0.6957 - val_accuracy: 0.4734\n",
      "Epoch 9/100\n",
      "563/563 [==============================] - ETA: 0s - loss: 0.6979 - accuracy: 0.43 - ETA: 0s - loss: 0.6980 - accuracy: 0.47 - ETA: 0s - loss: 0.6966 - accuracy: 0.49 - 0s 301us/step - loss: 0.6954 - accuracy: 0.4956 - val_loss: 0.6930 - val_accuracy: 0.5319\n",
      "Epoch 10/100\n",
      "563/563 [==============================] - ETA: 0s - loss: 0.6848 - accuracy: 0.62 - ETA: 0s - loss: 0.6959 - accuracy: 0.52 - ETA: 0s - loss: 0.6961 - accuracy: 0.50 - ETA: 0s - loss: 0.6955 - accuracy: 0.50 - 0s 445us/step - loss: 0.6966 - accuracy: 0.4991 - val_loss: 0.6935 - val_accuracy: 0.4734\n",
      "Epoch 11/100\n",
      "563/563 [==============================] - ETA: 0s - loss: 0.6980 - accuracy: 0.50 - ETA: 0s - loss: 0.6908 - accuracy: 0.53 - ETA: 0s - loss: 0.6944 - accuracy: 0.50 - 0s 338us/step - loss: 0.6945 - accuracy: 0.5009 - val_loss: 0.6935 - val_accuracy: 0.4947\n",
      "Epoch 12/100\n",
      "563/563 [==============================] - ETA: 0s - loss: 0.7002 - accuracy: 0.40 - ETA: 0s - loss: 0.6900 - accuracy: 0.53 - ETA: 0s - loss: 0.6955 - accuracy: 0.46 - 0s 287us/step - loss: 0.6956 - accuracy: 0.4636 - val_loss: 0.6942 - val_accuracy: 0.4787\n",
      "Epoch 13/100\n",
      "563/563 [==============================] - ETA: 0s - loss: 0.6865 - accuracy: 0.56 - ETA: 0s - loss: 0.6912 - accuracy: 0.55 - ETA: 0s - loss: 0.6933 - accuracy: 0.52 - 0s 275us/step - loss: 0.6936 - accuracy: 0.5044 - val_loss: 0.6940 - val_accuracy: 0.4840\n",
      "Epoch 14/100\n",
      "563/563 [==============================] - ETA: 0s - loss: 0.7047 - accuracy: 0.37 - ETA: 0s - loss: 0.6941 - accuracy: 0.49 - ETA: 0s - loss: 0.6957 - accuracy: 0.48 - 0s 278us/step - loss: 0.6951 - accuracy: 0.4885 - val_loss: 0.6947 - val_accuracy: 0.4681\n",
      "Epoch 15/100\n",
      "563/563 [==============================] - ETA: 0s - loss: 0.6890 - accuracy: 0.46 - ETA: 0s - loss: 0.6935 - accuracy: 0.47 - ETA: 0s - loss: 0.6945 - accuracy: 0.48 - 0s 289us/step - loss: 0.6943 - accuracy: 0.4902 - val_loss: 0.6948 - val_accuracy: 0.4840\n",
      "Epoch 16/100\n",
      "563/563 [==============================] - ETA: 0s - loss: 0.7052 - accuracy: 0.43 - ETA: 0s - loss: 0.6935 - accuracy: 0.51 - ETA: 0s - loss: 0.6936 - accuracy: 0.50 - 0s 276us/step - loss: 0.6937 - accuracy: 0.5098 - val_loss: 0.6937 - val_accuracy: 0.4574\n",
      "Epoch 17/100\n",
      "563/563 [==============================] - ETA: 0s - loss: 0.6920 - accuracy: 0.43 - ETA: 0s - loss: 0.6894 - accuracy: 0.50 - ETA: 0s - loss: 0.6919 - accuracy: 0.49 - 0s 402us/step - loss: 0.6917 - accuracy: 0.4991 - val_loss: 0.6941 - val_accuracy: 0.4734\n",
      "Epoch 18/100\n",
      "563/563 [==============================] - ETA: 0s - loss: 0.6876 - accuracy: 0.56 - ETA: 0s - loss: 0.6944 - accuracy: 0.50 - ETA: 0s - loss: 0.6935 - accuracy: 0.52 - 0s 306us/step - loss: 0.6935 - accuracy: 0.5311 - val_loss: 0.6950 - val_accuracy: 0.4521\n",
      "Epoch 19/100\n",
      "563/563 [==============================] - ETA: 0s - loss: 0.6858 - accuracy: 0.53 - ETA: 0s - loss: 0.6920 - accuracy: 0.51 - ETA: 0s - loss: 0.6894 - accuracy: 0.53 - 0s 278us/step - loss: 0.6894 - accuracy: 0.5346 - val_loss: 0.6952 - val_accuracy: 0.4574\n",
      "Epoch 20/100\n",
      "563/563 [==============================] - ETA: 0s - loss: 0.6925 - accuracy: 0.53 - ETA: 0s - loss: 0.6920 - accuracy: 0.54 - ETA: 0s - loss: 0.6919 - accuracy: 0.52 - 0s 299us/step - loss: 0.6930 - accuracy: 0.5187 - val_loss: 0.6943 - val_accuracy: 0.4628\n",
      "Epoch 21/100\n",
      "563/563 [==============================] - ETA: 0s - loss: 0.6988 - accuracy: 0.43 - ETA: 0s - loss: 0.6944 - accuracy: 0.48 - ETA: 0s - loss: 0.6935 - accuracy: 0.50 - 0s 283us/step - loss: 0.6943 - accuracy: 0.4938 - val_loss: 0.6935 - val_accuracy: 0.5213\n",
      "Epoch 22/100\n",
      "563/563 [==============================] - ETA: 0s - loss: 0.7096 - accuracy: 0.40 - ETA: 0s - loss: 0.6989 - accuracy: 0.45 - ETA: 0s - loss: 0.6948 - accuracy: 0.48 - 0s 356us/step - loss: 0.6937 - accuracy: 0.5027 - val_loss: 0.6949 - val_accuracy: 0.4628\n",
      "Epoch 23/100\n",
      "563/563 [==============================] - ETA: 0s - loss: 0.6910 - accuracy: 0.50 - ETA: 0s - loss: 0.6900 - accuracy: 0.51 - ETA: 0s - loss: 0.6919 - accuracy: 0.50 - ETA: 0s - loss: 0.6934 - accuracy: 0.47 - 0s 438us/step - loss: 0.6931 - accuracy: 0.4813 - val_loss: 0.6953 - val_accuracy: 0.4734\n",
      "Epoch 24/100\n",
      "563/563 [==============================] - ETA: 0s - loss: 0.6849 - accuracy: 0.53 - ETA: 0s - loss: 0.6912 - accuracy: 0.49 - ETA: 0s - loss: 0.6938 - accuracy: 0.48 - 0s 278us/step - loss: 0.6944 - accuracy: 0.4813 - val_loss: 0.6952 - val_accuracy: 0.4787\n",
      "Epoch 25/100\n",
      "563/563 [==============================] - ETA: 0s - loss: 0.6883 - accuracy: 0.62 - ETA: 0s - loss: 0.6953 - accuracy: 0.46 - ETA: 0s - loss: 0.6934 - accuracy: 0.48 - 0s 273us/step - loss: 0.6921 - accuracy: 0.5009 - val_loss: 0.6954 - val_accuracy: 0.4628\n",
      "Epoch 26/100\n",
      "563/563 [==============================] - ETA: 0s - loss: 0.6796 - accuracy: 0.68 - ETA: 0s - loss: 0.6914 - accuracy: 0.55 - ETA: 0s - loss: 0.6925 - accuracy: 0.53 - 0s 280us/step - loss: 0.6904 - accuracy: 0.5435 - val_loss: 0.6970 - val_accuracy: 0.4681\n",
      "Epoch 27/100\n",
      "563/563 [==============================] - ETA: 0s - loss: 0.6853 - accuracy: 0.50 - ETA: 0s - loss: 0.6976 - accuracy: 0.47 - ETA: 0s - loss: 0.6961 - accuracy: 0.48 - 0s 291us/step - loss: 0.6952 - accuracy: 0.4867 - val_loss: 0.6965 - val_accuracy: 0.4574\n",
      "Epoch 28/100\n",
      "563/563 [==============================] - ETA: 0s - loss: 0.7049 - accuracy: 0.53 - ETA: 0s - loss: 0.6952 - accuracy: 0.51 - ETA: 0s - loss: 0.6927 - accuracy: 0.52 - 0s 296us/step - loss: 0.6926 - accuracy: 0.5240 - val_loss: 0.6963 - val_accuracy: 0.4787\n",
      "Epoch 29/100\n",
      "563/563 [==============================] - ETA: 1s - loss: 0.7028 - accuracy: 0.43 - ETA: 0s - loss: 0.6912 - accuracy: 0.52 - ETA: 0s - loss: 0.6881 - accuracy: 0.54 - 0s 427us/step - loss: 0.6910 - accuracy: 0.5275 - val_loss: 0.6972 - val_accuracy: 0.4628\n",
      "Epoch 30/100\n",
      "563/563 [==============================] - ETA: 0s - loss: 0.6922 - accuracy: 0.46 - ETA: 0s - loss: 0.6959 - accuracy: 0.46 - ETA: 0s - loss: 0.6985 - accuracy: 0.46 - 0s 308us/step - loss: 0.6962 - accuracy: 0.4778 - val_loss: 0.6956 - val_accuracy: 0.4681\n",
      "Epoch 31/100\n",
      "563/563 [==============================] - ETA: 0s - loss: 0.6943 - accuracy: 0.53 - ETA: 0s - loss: 0.6925 - accuracy: 0.50 - ETA: 0s - loss: 0.6875 - accuracy: 0.53 - 0s 305us/step - loss: 0.6892 - accuracy: 0.5133 - val_loss: 0.6963 - val_accuracy: 0.4681\n",
      "Epoch 32/100\n",
      "563/563 [==============================] - ETA: 0s - loss: 0.7054 - accuracy: 0.50 - ETA: 0s - loss: 0.6902 - accuracy: 0.57 - ETA: 0s - loss: 0.6914 - accuracy: 0.54 - 0s 335us/step - loss: 0.6927 - accuracy: 0.5329 - val_loss: 0.6968 - val_accuracy: 0.4574\n",
      "Epoch 33/100\n",
      "563/563 [==============================] - ETA: 0s - loss: 0.6859 - accuracy: 0.56 - ETA: 0s - loss: 0.6982 - accuracy: 0.53 - ETA: 0s - loss: 0.6952 - accuracy: 0.51 - ETA: 0s - loss: 0.6921 - accuracy: 0.51 - 0s 409us/step - loss: 0.6933 - accuracy: 0.5133 - val_loss: 0.6949 - val_accuracy: 0.4681\n",
      "Epoch 34/100\n",
      "563/563 [==============================] - ETA: 0s - loss: 0.6912 - accuracy: 0.50 - ETA: 0s - loss: 0.6895 - accuracy: 0.51 - ETA: 0s - loss: 0.6911 - accuracy: 0.53 - ETA: 0s - loss: 0.6936 - accuracy: 0.52 - ETA: 0s - loss: 0.6976 - accuracy: 0.50 - 0s 563us/step - loss: 0.6979 - accuracy: 0.4991 - val_loss: 0.6955 - val_accuracy: 0.4681\n",
      "Epoch 35/100\n",
      "563/563 [==============================] - ETA: 0s - loss: 0.6982 - accuracy: 0.43 - ETA: 0s - loss: 0.6965 - accuracy: 0.47 - ETA: 0s - loss: 0.6960 - accuracy: 0.48 - ETA: 0s - loss: 0.6956 - accuracy: 0.47 - 0s 361us/step - loss: 0.6954 - accuracy: 0.4849 - val_loss: 0.6944 - val_accuracy: 0.4574\n",
      "Epoch 36/100\n",
      "563/563 [==============================] - ETA: 0s - loss: 0.6579 - accuracy: 0.65 - ETA: 0s - loss: 0.6905 - accuracy: 0.50 - ETA: 0s - loss: 0.6900 - accuracy: 0.51 - 0s 264us/step - loss: 0.6907 - accuracy: 0.5151 - val_loss: 0.6939 - val_accuracy: 0.4840\n",
      "Epoch 37/100\n",
      "563/563 [==============================] - ETA: 0s - loss: 0.6868 - accuracy: 0.50 - ETA: 0s - loss: 0.6884 - accuracy: 0.59 - ETA: 0s - loss: 0.6908 - accuracy: 0.54 - ETA: 0s - loss: 0.6910 - accuracy: 0.53 - 0s 367us/step - loss: 0.6911 - accuracy: 0.5311 - val_loss: 0.6946 - val_accuracy: 0.4681\n",
      "Epoch 38/100\n",
      "563/563 [==============================] - ETA: 0s - loss: 0.6912 - accuracy: 0.59 - ETA: 0s - loss: 0.6889 - accuracy: 0.54 - ETA: 0s - loss: 0.6919 - accuracy: 0.52 - 0s 326us/step - loss: 0.6916 - accuracy: 0.5187 - val_loss: 0.6956 - val_accuracy: 0.4628\n",
      "Epoch 39/100\n",
      "563/563 [==============================] - ETA: 0s - loss: 0.6950 - accuracy: 0.50 - ETA: 0s - loss: 0.6915 - accuracy: 0.50 - ETA: 0s - loss: 0.6905 - accuracy: 0.50 - ETA: 0s - loss: 0.6898 - accuracy: 0.53 - ETA: 0s - loss: 0.6891 - accuracy: 0.53 - ETA: 0s - loss: 0.6901 - accuracy: 0.52 - 0s 700us/step - loss: 0.6908 - accuracy: 0.5222 - val_loss: 0.6960 - val_accuracy: 0.4681\n",
      "Epoch 40/100\n",
      "563/563 [==============================] - ETA: 0s - loss: 0.6772 - accuracy: 0.56 - ETA: 0s - loss: 0.6900 - accuracy: 0.46 - ETA: 0s - loss: 0.6879 - accuracy: 0.48 - ETA: 0s - loss: 0.6878 - accuracy: 0.50 - 0s 445us/step - loss: 0.6893 - accuracy: 0.5009 - val_loss: 0.6983 - val_accuracy: 0.4734\n",
      "Epoch 41/100\n",
      "563/563 [==============================] - ETA: 0s - loss: 0.6927 - accuracy: 0.56 - ETA: 0s - loss: 0.6859 - accuracy: 0.52 - ETA: 0s - loss: 0.6957 - accuracy: 0.52 - ETA: 0s - loss: 0.6970 - accuracy: 0.51 - ETA: 0s - loss: 0.6944 - accuracy: 0.50 - 0s 501us/step - loss: 0.6939 - accuracy: 0.5133 - val_loss: 0.6954 - val_accuracy: 0.4787\n",
      "Epoch 42/100\n",
      "563/563 [==============================] - ETA: 0s - loss: 0.6717 - accuracy: 0.59 - ETA: 0s - loss: 0.6874 - accuracy: 0.54 - ETA: 0s - loss: 0.6891 - accuracy: 0.52 - ETA: 0s - loss: 0.6911 - accuracy: 0.51 - 0s 400us/step - loss: 0.6902 - accuracy: 0.5115 - val_loss: 0.6964 - val_accuracy: 0.4734\n",
      "Epoch 43/100\n",
      "563/563 [==============================] - ETA: 1s - loss: 0.6806 - accuracy: 0.56 - ETA: 1s - loss: 0.6944 - accuracy: 0.54 - ETA: 0s - loss: 0.6930 - accuracy: 0.54 - ETA: 0s - loss: 0.6919 - accuracy: 0.53 - ETA: 0s - loss: 0.6945 - accuracy: 0.50 - 0s 740us/step - loss: 0.6948 - accuracy: 0.5044 - val_loss: 0.6972 - val_accuracy: 0.4681\n",
      "Epoch 44/100\n",
      "563/563 [==============================] - ETA: 0s - loss: 0.6915 - accuracy: 0.56 - ETA: 0s - loss: 0.6969 - accuracy: 0.51 - ETA: 0s - loss: 0.6930 - accuracy: 0.50 - ETA: 0s - loss: 0.6948 - accuracy: 0.50 - 0s 478us/step - loss: 0.6931 - accuracy: 0.5151 - val_loss: 0.6963 - val_accuracy: 0.4734\n",
      "Epoch 45/100\n",
      "563/563 [==============================] - ETA: 0s - loss: 0.6904 - accuracy: 0.50 - ETA: 0s - loss: 0.6868 - accuracy: 0.51 - ETA: 0s - loss: 0.6859 - accuracy: 0.50 - ETA: 0s - loss: 0.6882 - accuracy: 0.52 - ETA: 0s - loss: 0.6907 - accuracy: 0.51 - 0s 588us/step - loss: 0.6913 - accuracy: 0.5133 - val_loss: 0.6956 - val_accuracy: 0.4734\n",
      "Epoch 46/100\n",
      "563/563 [==============================] - ETA: 0s - loss: 0.6745 - accuracy: 0.62 - ETA: 0s - loss: 0.6889 - accuracy: 0.55 - ETA: 0s - loss: 0.6896 - accuracy: 0.53 - ETA: 0s - loss: 0.6912 - accuracy: 0.53 - ETA: 0s - loss: 0.6928 - accuracy: 0.51 - ETA: 0s - loss: 0.6905 - accuracy: 0.53 - 0s 744us/step - loss: 0.6920 - accuracy: 0.5240 - val_loss: 0.6949 - val_accuracy: 0.4734\n",
      "Epoch 47/100\n",
      "563/563 [==============================] - ETA: 0s - loss: 0.6817 - accuracy: 0.71 - ETA: 0s - loss: 0.6896 - accuracy: 0.56 - ETA: 0s - loss: 0.6945 - accuracy: 0.52 - ETA: 0s - loss: 0.6945 - accuracy: 0.52 - 0s 413us/step - loss: 0.6949 - accuracy: 0.5204 - val_loss: 0.6954 - val_accuracy: 0.4894\n",
      "Epoch 48/100\n",
      "563/563 [==============================] - ETA: 0s - loss: 0.6876 - accuracy: 0.53 - ETA: 0s - loss: 0.6948 - accuracy: 0.50 - ETA: 0s - loss: 0.6962 - accuracy: 0.49 - ETA: 0s - loss: 0.6948 - accuracy: 0.51 - ETA: 0s - loss: 0.6939 - accuracy: 0.52 - 0s 492us/step - loss: 0.6937 - accuracy: 0.5240 - val_loss: 0.6949 - val_accuracy: 0.4681\n",
      "Epoch 49/100\n",
      "563/563 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.40 - ETA: 0s - loss: 0.6898 - accuracy: 0.53 - ETA: 0s - loss: 0.6918 - accuracy: 0.52 - ETA: 0s - loss: 0.6910 - accuracy: 0.50 - ETA: 0s - loss: 0.6916 - accuracy: 0.48 - 0s 489us/step - loss: 0.6915 - accuracy: 0.4902 - val_loss: 0.6947 - val_accuracy: 0.4840\n",
      "Epoch 50/100\n",
      "563/563 [==============================] - ETA: 0s - loss: 0.6872 - accuracy: 0.46 - ETA: 0s - loss: 0.6913 - accuracy: 0.53 - ETA: 0s - loss: 0.6911 - accuracy: 0.52 - ETA: 0s - loss: 0.6930 - accuracy: 0.50 - ETA: 0s - loss: 0.6952 - accuracy: 0.49 - 0s 668us/step - loss: 0.6949 - accuracy: 0.4902 - val_loss: 0.6946 - val_accuracy: 0.4840\n",
      "Epoch 51/100\n",
      "563/563 [==============================] - ETA: 0s - loss: 0.6901 - accuracy: 0.56 - ETA: 0s - loss: 0.6978 - accuracy: 0.48 - ETA: 0s - loss: 0.6938 - accuracy: 0.51 - ETA: 0s - loss: 0.6952 - accuracy: 0.48 - ETA: 0s - loss: 0.6967 - accuracy: 0.48 - 0s 482us/step - loss: 0.6965 - accuracy: 0.4831 - val_loss: 0.6934 - val_accuracy: 0.4681\n",
      "Epoch 52/100\n",
      "563/563 [==============================] - ETA: 0s - loss: 0.6817 - accuracy: 0.65 - ETA: 0s - loss: 0.6883 - accuracy: 0.54 - ETA: 0s - loss: 0.6902 - accuracy: 0.52 - ETA: 0s - loss: 0.6916 - accuracy: 0.51 - 0s 484us/step - loss: 0.6916 - accuracy: 0.5027 - val_loss: 0.6949 - val_accuracy: 0.4894\n",
      "Epoch 53/100\n",
      "563/563 [==============================] - ETA: 0s - loss: 0.6959 - accuracy: 0.46 - ETA: 0s - loss: 0.6894 - accuracy: 0.53 - ETA: 0s - loss: 0.6924 - accuracy: 0.52 - ETA: 0s - loss: 0.6912 - accuracy: 0.55 - ETA: 0s - loss: 0.6927 - accuracy: 0.53 - 0s 677us/step - loss: 0.6927 - accuracy: 0.5417 - val_loss: 0.6940 - val_accuracy: 0.4787\n",
      "Epoch 54/100\n",
      "563/563 [==============================] - ETA: 0s - loss: 0.7000 - accuracy: 0.50 - ETA: 0s - loss: 0.7039 - accuracy: 0.51 - ETA: 0s - loss: 0.6998 - accuracy: 0.50 - ETA: 0s - loss: 0.6974 - accuracy: 0.49 - ETA: 0s - loss: 0.6957 - accuracy: 0.49 - 0s 553us/step - loss: 0.6953 - accuracy: 0.5009 - val_loss: 0.6948 - val_accuracy: 0.4840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/100\n",
      "563/563 [==============================] - ETA: 0s - loss: 0.6985 - accuracy: 0.50 - ETA: 0s - loss: 0.6925 - accuracy: 0.52 - ETA: 0s - loss: 0.6933 - accuracy: 0.53 - ETA: 0s - loss: 0.6928 - accuracy: 0.53 - ETA: 0s - loss: 0.6924 - accuracy: 0.53 - 0s 498us/step - loss: 0.6915 - accuracy: 0.5400 - val_loss: 0.6944 - val_accuracy: 0.4734\n",
      "Epoch 56/100\n",
      "563/563 [==============================] - ETA: 0s - loss: 0.6929 - accuracy: 0.53 - ETA: 0s - loss: 0.6894 - accuracy: 0.55 - ETA: 0s - loss: 0.6906 - accuracy: 0.52 - ETA: 0s - loss: 0.6920 - accuracy: 0.51 - ETA: 0s - loss: 0.6932 - accuracy: 0.50 - 0s 524us/step - loss: 0.6939 - accuracy: 0.4973 - val_loss: 0.6952 - val_accuracy: 0.4681\n",
      "Epoch 57/100\n",
      "563/563 [==============================] - ETA: 0s - loss: 0.6763 - accuracy: 0.59 - ETA: 0s - loss: 0.6945 - accuracy: 0.46 - ETA: 0s - loss: 0.6897 - accuracy: 0.52 - ETA: 0s - loss: 0.6878 - accuracy: 0.53 - ETA: 0s - loss: 0.6900 - accuracy: 0.53 - ETA: 0s - loss: 0.6893 - accuracy: 0.52 - 0s 870us/step - loss: 0.6899 - accuracy: 0.5400 - val_loss: 0.6949 - val_accuracy: 0.4787\n",
      "Epoch 58/100\n",
      "563/563 [==============================] - ETA: 0s - loss: 0.7050 - accuracy: 0.53 - ETA: 0s - loss: 0.6935 - accuracy: 0.54 - ETA: 0s - loss: 0.6900 - accuracy: 0.55 - ETA: 0s - loss: 0.6949 - accuracy: 0.52 - 0s 462us/step - loss: 0.6947 - accuracy: 0.5080 - val_loss: 0.6965 - val_accuracy: 0.4787\n",
      "Epoch 59/100\n",
      "563/563 [==============================] - ETA: 0s - loss: 0.6775 - accuracy: 0.50 - ETA: 0s - loss: 0.6925 - accuracy: 0.53 - ETA: 0s - loss: 0.6938 - accuracy: 0.49 - ETA: 0s - loss: 0.6924 - accuracy: 0.50 - 0s 381us/step - loss: 0.6917 - accuracy: 0.5187 - val_loss: 0.6957 - val_accuracy: 0.4840\n",
      "Epoch 60/100\n",
      "563/563 [==============================] - ETA: 0s - loss: 0.6970 - accuracy: 0.50 - ETA: 0s - loss: 0.6992 - accuracy: 0.51 - ETA: 0s - loss: 0.6958 - accuracy: 0.51 - ETA: 0s - loss: 0.6927 - accuracy: 0.51 - 0s 452us/step - loss: 0.6924 - accuracy: 0.5169 - val_loss: 0.6958 - val_accuracy: 0.4894\n",
      "Epoch 61/100\n",
      "563/563 [==============================] - ETA: 0s - loss: 0.6808 - accuracy: 0.62 - ETA: 0s - loss: 0.6865 - accuracy: 0.60 - ETA: 1s - loss: 0.6918 - accuracy: 0.58 - ETA: 0s - loss: 0.6946 - accuracy: 0.54 - ETA: 0s - loss: 0.6970 - accuracy: 0.51 - ETA: 0s - loss: 0.6940 - accuracy: 0.52 - 0s 776us/step - loss: 0.6926 - accuracy: 0.5240 - val_loss: 0.6941 - val_accuracy: 0.4734\n",
      "Epoch 62/100\n",
      "563/563 [==============================] - ETA: 0s - loss: 0.6853 - accuracy: 0.53 - ETA: 0s - loss: 0.6936 - accuracy: 0.49 - ETA: 0s - loss: 0.6894 - accuracy: 0.52 - ETA: 0s - loss: 0.6914 - accuracy: 0.51 - 0s 411us/step - loss: 0.6919 - accuracy: 0.5133 - val_loss: 0.6952 - val_accuracy: 0.4734\n",
      "Epoch 63/100\n",
      "563/563 [==============================] - ETA: 0s - loss: 0.6792 - accuracy: 0.59 - ETA: 0s - loss: 0.6882 - accuracy: 0.52 - ETA: 0s - loss: 0.6968 - accuracy: 0.50 - ETA: 0s - loss: 0.6959 - accuracy: 0.48 - ETA: 0s - loss: 0.6940 - accuracy: 0.48 - 0s 515us/step - loss: 0.6941 - accuracy: 0.4885 - val_loss: 0.6944 - val_accuracy: 0.4840\n",
      "Epoch 64/100\n",
      "563/563 [==============================] - ETA: 0s - loss: 0.7028 - accuracy: 0.53 - ETA: 0s - loss: 0.6915 - accuracy: 0.51 - ETA: 0s - loss: 0.6895 - accuracy: 0.55 - ETA: 0s - loss: 0.6933 - accuracy: 0.52 - ETA: 0s - loss: 0.6937 - accuracy: 0.52 - ETA: 0s - loss: 0.6938 - accuracy: 0.52 - 0s 813us/step - loss: 0.6931 - accuracy: 0.5275 - val_loss: 0.6939 - val_accuracy: 0.4681\n",
      "Epoch 65/100\n",
      "563/563 [==============================] - ETA: 0s - loss: 0.6843 - accuracy: 0.68 - ETA: 0s - loss: 0.6860 - accuracy: 0.54 - ETA: 0s - loss: 0.6895 - accuracy: 0.52 - ETA: 0s - loss: 0.6887 - accuracy: 0.52 - 0s 413us/step - loss: 0.6899 - accuracy: 0.5222 - val_loss: 0.6954 - val_accuracy: 0.4787\n",
      "Epoch 66/100\n",
      "563/563 [==============================] - ETA: 0s - loss: 0.7005 - accuracy: 0.43 - ETA: 0s - loss: 0.6887 - accuracy: 0.48 - ETA: 0s - loss: 0.6934 - accuracy: 0.46 - ETA: 0s - loss: 0.6912 - accuracy: 0.48 - ETA: 0s - loss: 0.6923 - accuracy: 0.48 - 0s 500us/step - loss: 0.6921 - accuracy: 0.4796 - val_loss: 0.6951 - val_accuracy: 0.4734\n",
      "Epoch 67/100\n",
      "563/563 [==============================] - ETA: 0s - loss: 0.7053 - accuracy: 0.50 - ETA: 0s - loss: 0.6936 - accuracy: 0.49 - ETA: 0s - loss: 0.6957 - accuracy: 0.48 - ETA: 0s - loss: 0.6953 - accuracy: 0.46 - ETA: 0s - loss: 0.6963 - accuracy: 0.46 - 0s 533us/step - loss: 0.6955 - accuracy: 0.4689 - val_loss: 0.6944 - val_accuracy: 0.4681\n",
      "Epoch 68/100\n",
      "563/563 [==============================] - ETA: 0s - loss: 0.6817 - accuracy: 0.62 - ETA: 0s - loss: 0.6955 - accuracy: 0.50 - ETA: 0s - loss: 0.6956 - accuracy: 0.51 - ETA: 0s - loss: 0.6962 - accuracy: 0.49 - ETA: 0s - loss: 0.6950 - accuracy: 0.50 - ETA: 0s - loss: 0.6928 - accuracy: 0.52 - 0s 749us/step - loss: 0.6933 - accuracy: 0.5240 - val_loss: 0.6956 - val_accuracy: 0.4894\n",
      "Epoch 69/100\n",
      "563/563 [==============================] - ETA: 0s - loss: 0.6893 - accuracy: 0.59 - ETA: 0s - loss: 0.6840 - accuracy: 0.61 - ETA: 0s - loss: 0.6884 - accuracy: 0.56 - ETA: 0s - loss: 0.6879 - accuracy: 0.55 - 0s 438us/step - loss: 0.6896 - accuracy: 0.5400 - val_loss: 0.6955 - val_accuracy: 0.4734\n",
      "Epoch 70/100\n",
      "563/563 [==============================] - ETA: 0s - loss: 0.6782 - accuracy: 0.65 - ETA: 0s - loss: 0.6904 - accuracy: 0.51 - ETA: 0s - loss: 0.6900 - accuracy: 0.52 - ETA: 0s - loss: 0.6922 - accuracy: 0.52 - 0s 448us/step - loss: 0.6941 - accuracy: 0.5293 - val_loss: 0.6947 - val_accuracy: 0.4840\n",
      "Epoch 71/100\n",
      "563/563 [==============================] - ETA: 0s - loss: 0.6941 - accuracy: 0.59 - ETA: 0s - loss: 0.6946 - accuracy: 0.51 - ETA: 0s - loss: 0.6959 - accuracy: 0.50 - ETA: 0s - loss: 0.6944 - accuracy: 0.50 - 0s 448us/step - loss: 0.6937 - accuracy: 0.5098 - val_loss: 0.6948 - val_accuracy: 0.4840\n",
      "Epoch 72/100\n",
      "563/563 [==============================] - ETA: 0s - loss: 0.6945 - accuracy: 0.53 - ETA: 0s - loss: 0.6888 - accuracy: 0.57 - ETA: 0s - loss: 0.6877 - accuracy: 0.57 - ETA: 0s - loss: 0.6878 - accuracy: 0.56 - ETA: 0s - loss: 0.6903 - accuracy: 0.54 - ETA: 0s - loss: 0.6890 - accuracy: 0.53 - 0s 771us/step - loss: 0.6903 - accuracy: 0.5204 - val_loss: 0.6946 - val_accuracy: 0.4894\n",
      "Epoch 73/100\n",
      "563/563 [==============================] - ETA: 0s - loss: 0.6968 - accuracy: 0.40 - ETA: 0s - loss: 0.6964 - accuracy: 0.49 - ETA: 0s - loss: 0.6939 - accuracy: 0.51 - ETA: 0s - loss: 0.6932 - accuracy: 0.51 - 0s 446us/step - loss: 0.6938 - accuracy: 0.4956 - val_loss: 0.6946 - val_accuracy: 0.4734\n",
      "Epoch 74/100\n",
      "563/563 [==============================] - ETA: 0s - loss: 0.7001 - accuracy: 0.46 - ETA: 0s - loss: 0.6920 - accuracy: 0.52 - ETA: 0s - loss: 0.6932 - accuracy: 0.50 - ETA: 0s - loss: 0.6950 - accuracy: 0.47 - 0s 436us/step - loss: 0.6943 - accuracy: 0.4849 - val_loss: 0.6950 - val_accuracy: 0.4894\n",
      "Epoch 75/100\n",
      "563/563 [==============================] - ETA: 0s - loss: 0.7004 - accuracy: 0.40 - ETA: 0s - loss: 0.6900 - accuracy: 0.51 - ETA: 0s - loss: 0.6883 - accuracy: 0.54 - ETA: 0s - loss: 0.6918 - accuracy: 0.51 - 0s 450us/step - loss: 0.6918 - accuracy: 0.5009 - val_loss: 0.6953 - val_accuracy: 0.4894\n",
      "Epoch 76/100\n",
      "563/563 [==============================] - ETA: 1s - loss: 0.6886 - accuracy: 0.43 - ETA: 1s - loss: 0.6900 - accuracy: 0.53 - ETA: 0s - loss: 0.6911 - accuracy: 0.53 - ETA: 0s - loss: 0.6929 - accuracy: 0.50 - ETA: 0s - loss: 0.6915 - accuracy: 0.51 - 0s 643us/step - loss: 0.6921 - accuracy: 0.5115 - val_loss: 0.6941 - val_accuracy: 0.4840\n",
      "Epoch 77/100\n",
      "563/563 [==============================] - ETA: 0s - loss: 0.6903 - accuracy: 0.50 - ETA: 0s - loss: 0.6836 - accuracy: 0.60 - ETA: 0s - loss: 0.6902 - accuracy: 0.53 - ETA: 0s - loss: 0.6933 - accuracy: 0.52 - ETA: 0s - loss: 0.6934 - accuracy: 0.52 - 0s 547us/step - loss: 0.6936 - accuracy: 0.5151 - val_loss: 0.6943 - val_accuracy: 0.4894\n",
      "Epoch 78/100\n",
      "563/563 [==============================] - ETA: 0s - loss: 0.6909 - accuracy: 0.50 - ETA: 0s - loss: 0.6938 - accuracy: 0.49 - ETA: 0s - loss: 0.6929 - accuracy: 0.53 - ETA: 0s - loss: 0.6917 - accuracy: 0.52 - 0s 413us/step - loss: 0.6915 - accuracy: 0.5222 - val_loss: 0.6947 - val_accuracy: 0.4894\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79/100\n",
      "563/563 [==============================] - ETA: 0s - loss: 0.6852 - accuracy: 0.50 - ETA: 0s - loss: 0.6910 - accuracy: 0.56 - ETA: 0s - loss: 0.6903 - accuracy: 0.56 - ETA: 0s - loss: 0.6912 - accuracy: 0.54 - ETA: 0s - loss: 0.6903 - accuracy: 0.55 - 0s 693us/step - loss: 0.6909 - accuracy: 0.5506 - val_loss: 0.6944 - val_accuracy: 0.4840\n",
      "Epoch 80/100\n",
      "563/563 [==============================] - ETA: 0s - loss: 0.6868 - accuracy: 0.53 - ETA: 0s - loss: 0.6869 - accuracy: 0.51 - ETA: 0s - loss: 0.6897 - accuracy: 0.50 - ETA: 0s - loss: 0.6864 - accuracy: 0.51 - ETA: 0s - loss: 0.6893 - accuracy: 0.50 - 0s 521us/step - loss: 0.6899 - accuracy: 0.5027 - val_loss: 0.6950 - val_accuracy: 0.4840\n",
      "Epoch 81/100\n",
      "563/563 [==============================] - ETA: 0s - loss: 0.6794 - accuracy: 0.56 - ETA: 0s - loss: 0.6916 - accuracy: 0.52 - ETA: 0s - loss: 0.6949 - accuracy: 0.50 - ETA: 0s - loss: 0.6937 - accuracy: 0.50 - 0s 397us/step - loss: 0.6940 - accuracy: 0.5044 - val_loss: 0.6960 - val_accuracy: 0.4840\n",
      "Epoch 82/100\n",
      "563/563 [==============================] - ETA: 0s - loss: 0.6909 - accuracy: 0.62 - ETA: 0s - loss: 0.6920 - accuracy: 0.48 - ETA: 0s - loss: 0.6963 - accuracy: 0.45 - ETA: 0s - loss: 0.6977 - accuracy: 0.44 - ETA: 0s - loss: 0.6953 - accuracy: 0.49 - 0s 549us/step - loss: 0.6936 - accuracy: 0.5151 - val_loss: 0.6945 - val_accuracy: 0.4787\n",
      "Epoch 83/100\n",
      "563/563 [==============================] - ETA: 0s - loss: 0.6930 - accuracy: 0.59 - ETA: 0s - loss: 0.6943 - accuracy: 0.48 - ETA: 0s - loss: 0.6945 - accuracy: 0.47 - ETA: 0s - loss: 0.6931 - accuracy: 0.48 - ETA: 0s - loss: 0.6933 - accuracy: 0.49 - ETA: 0s - loss: 0.6911 - accuracy: 0.50 - 0s 852us/step - loss: 0.6918 - accuracy: 0.5009 - val_loss: 0.6948 - val_accuracy: 0.4734\n",
      "Epoch 84/100\n",
      "563/563 [==============================] - ETA: 0s - loss: 0.7068 - accuracy: 0.34 - ETA: 0s - loss: 0.6839 - accuracy: 0.57 - ETA: 0s - loss: 0.6855 - accuracy: 0.52 - ETA: 0s - loss: 0.6873 - accuracy: 0.53 - ETA: 0s - loss: 0.6864 - accuracy: 0.54 - 0s 491us/step - loss: 0.6868 - accuracy: 0.5435 - val_loss: 0.6958 - val_accuracy: 0.4787\n",
      "Epoch 85/100\n",
      "563/563 [==============================] - ETA: 0s - loss: 0.7002 - accuracy: 0.40 - ETA: 0s - loss: 0.6920 - accuracy: 0.51 - ETA: 0s - loss: 0.6883 - accuracy: 0.53 - ETA: 0s - loss: 0.6910 - accuracy: 0.50 - ETA: 0s - loss: 0.6941 - accuracy: 0.50 - 0s 508us/step - loss: 0.6932 - accuracy: 0.5151 - val_loss: 0.6960 - val_accuracy: 0.4947\n",
      "Epoch 86/100\n",
      "563/563 [==============================] - ETA: 0s - loss: 0.6784 - accuracy: 0.56 - ETA: 0s - loss: 0.6980 - accuracy: 0.53 - ETA: 0s - loss: 0.6944 - accuracy: 0.54 - ETA: 0s - loss: 0.6953 - accuracy: 0.53 - ETA: 0s - loss: 0.6962 - accuracy: 0.53 - 0s 517us/step - loss: 0.6949 - accuracy: 0.5329 - val_loss: 0.6954 - val_accuracy: 0.4894\n",
      "Epoch 87/100\n",
      "563/563 [==============================] - ETA: 1s - loss: 0.6960 - accuracy: 0.43 - ETA: 0s - loss: 0.6897 - accuracy: 0.53 - ETA: 0s - loss: 0.6874 - accuracy: 0.54 - ETA: 0s - loss: 0.6963 - accuracy: 0.52 - ETA: 0s - loss: 0.6954 - accuracy: 0.51 - ETA: 0s - loss: 0.6944 - accuracy: 0.51 - 0s 748us/step - loss: 0.6947 - accuracy: 0.5115 - val_loss: 0.6953 - val_accuracy: 0.5000\n",
      "Epoch 88/100\n",
      "563/563 [==============================] - ETA: 0s - loss: 0.6864 - accuracy: 0.53 - ETA: 0s - loss: 0.6917 - accuracy: 0.54 - ETA: 0s - loss: 0.6929 - accuracy: 0.51 - ETA: 0s - loss: 0.6927 - accuracy: 0.52 - ETA: 0s - loss: 0.6909 - accuracy: 0.53 - 0s 487us/step - loss: 0.6913 - accuracy: 0.5275 - val_loss: 0.6943 - val_accuracy: 0.4894\n",
      "Epoch 89/100\n",
      "563/563 [==============================] - ETA: 0s - loss: 0.6888 - accuracy: 0.53 - ETA: 0s - loss: 0.6870 - accuracy: 0.54 - ETA: 0s - loss: 0.6874 - accuracy: 0.54 - ETA: 0s - loss: 0.6888 - accuracy: 0.55 - ETA: 0s - loss: 0.6888 - accuracy: 0.55 - 0s 519us/step - loss: 0.6906 - accuracy: 0.5400 - val_loss: 0.6934 - val_accuracy: 0.4681\n",
      "Epoch 90/100\n",
      "563/563 [==============================] - ETA: 0s - loss: 0.6952 - accuracy: 0.59 - ETA: 0s - loss: 0.7001 - accuracy: 0.50 - ETA: 0s - loss: 0.6997 - accuracy: 0.49 - ETA: 0s - loss: 0.6998 - accuracy: 0.49 - ETA: 0s - loss: 0.6980 - accuracy: 0.49 - ETA: 0s - loss: 0.6982 - accuracy: 0.48 - ETA: 0s - loss: 0.6946 - accuracy: 0.52 - ETA: 0s - loss: 0.6948 - accuracy: 0.52 - 0s 882us/step - loss: 0.6948 - accuracy: 0.5115 - val_loss: 0.6953 - val_accuracy: 0.4894\n",
      "Epoch 91/100\n",
      "563/563 [==============================] - ETA: 0s - loss: 0.6938 - accuracy: 0.53 - ETA: 0s - loss: 0.6896 - accuracy: 0.56 - ETA: 0s - loss: 0.6887 - accuracy: 0.54 - ETA: 0s - loss: 0.6885 - accuracy: 0.54 - ETA: 0s - loss: 0.6896 - accuracy: 0.54 - 0s 585us/step - loss: 0.6923 - accuracy: 0.5275 - val_loss: 0.6950 - val_accuracy: 0.4681\n",
      "Epoch 92/100\n",
      "563/563 [==============================] - ETA: 0s - loss: 0.6881 - accuracy: 0.59 - ETA: 0s - loss: 0.6934 - accuracy: 0.53 - ETA: 0s - loss: 0.6940 - accuracy: 0.52 - ETA: 0s - loss: 0.6917 - accuracy: 0.51 - ETA: 0s - loss: 0.6908 - accuracy: 0.50 - ETA: 0s - loss: 0.6914 - accuracy: 0.50 - ETA: 0s - loss: 0.6916 - accuracy: 0.50 - 0s 765us/step - loss: 0.6920 - accuracy: 0.5062 - val_loss: 0.6954 - val_accuracy: 0.4787\n",
      "Epoch 93/100\n",
      "563/563 [==============================] - ETA: 0s - loss: 0.7018 - accuracy: 0.59 - ETA: 0s - loss: 0.6897 - accuracy: 0.57 - ETA: 0s - loss: 0.6900 - accuracy: 0.54 - ETA: 0s - loss: 0.6916 - accuracy: 0.52 - ETA: 0s - loss: 0.6932 - accuracy: 0.50 - ETA: 0s - loss: 0.6939 - accuracy: 0.48 - ETA: 0s - loss: 0.6939 - accuracy: 0.49 - 0s 795us/step - loss: 0.6938 - accuracy: 0.4938 - val_loss: 0.6947 - val_accuracy: 0.4840\n",
      "Epoch 94/100\n",
      "563/563 [==============================] - ETA: 0s - loss: 0.6961 - accuracy: 0.50 - ETA: 0s - loss: 0.6924 - accuracy: 0.51 - ETA: 0s - loss: 0.6904 - accuracy: 0.52 - ETA: 0s - loss: 0.6894 - accuracy: 0.52 - ETA: 0s - loss: 0.6914 - accuracy: 0.51 - 0s 530us/step - loss: 0.6916 - accuracy: 0.5115 - val_loss: 0.6947 - val_accuracy: 0.4734\n",
      "Epoch 95/100\n",
      "563/563 [==============================] - ETA: 0s - loss: 0.7027 - accuracy: 0.50 - ETA: 0s - loss: 0.6894 - accuracy: 0.53 - ETA: 0s - loss: 0.6913 - accuracy: 0.55 - ETA: 0s - loss: 0.6880 - accuracy: 0.55 - ETA: 0s - loss: 0.6908 - accuracy: 0.54 - 0s 544us/step - loss: 0.6912 - accuracy: 0.5471 - val_loss: 0.6953 - val_accuracy: 0.4734\n",
      "Epoch 96/100\n",
      "563/563 [==============================] - ETA: 0s - loss: 0.6950 - accuracy: 0.56 - ETA: 0s - loss: 0.6986 - accuracy: 0.48 - ETA: 0s - loss: 0.6943 - accuracy: 0.51 - ETA: 0s - loss: 0.6935 - accuracy: 0.53 - ETA: 0s - loss: 0.6915 - accuracy: 0.51 - ETA: 0s - loss: 0.6918 - accuracy: 0.49 - ETA: 0s - loss: 0.6932 - accuracy: 0.49 - 0s 802us/step - loss: 0.6910 - accuracy: 0.4973 - val_loss: 0.6930 - val_accuracy: 0.5106\n",
      "Epoch 97/100\n",
      "563/563 [==============================] - ETA: 0s - loss: 0.6801 - accuracy: 0.53 - ETA: 0s - loss: 0.6900 - accuracy: 0.49 - ETA: 0s - loss: 0.6877 - accuracy: 0.55 - ETA: 0s - loss: 0.6887 - accuracy: 0.54 - ETA: 0s - loss: 0.6920 - accuracy: 0.51 - 0s 503us/step - loss: 0.6918 - accuracy: 0.5222 - val_loss: 0.6945 - val_accuracy: 0.4840\n",
      "Epoch 98/100\n",
      "563/563 [==============================] - ETA: 0s - loss: 0.6857 - accuracy: 0.59 - ETA: 0s - loss: 0.7004 - accuracy: 0.54 - ETA: 0s - loss: 0.6971 - accuracy: 0.52 - ETA: 0s - loss: 0.6978 - accuracy: 0.50 - ETA: 0s - loss: 0.6950 - accuracy: 0.52 - 0s 501us/step - loss: 0.6951 - accuracy: 0.5258 - val_loss: 0.6948 - val_accuracy: 0.4894\n",
      "Epoch 99/100\n",
      "563/563 [==============================] - ETA: 0s - loss: 0.6960 - accuracy: 0.43 - ETA: 0s - loss: 0.6898 - accuracy: 0.54 - ETA: 0s - loss: 0.6899 - accuracy: 0.53 - ETA: 0s - loss: 0.6895 - accuracy: 0.52 - ETA: 0s - loss: 0.6888 - accuracy: 0.52 - ETA: 0s - loss: 0.6886 - accuracy: 0.52 - ETA: 0s - loss: 0.6896 - accuracy: 0.51 - ETA: 0s - loss: 0.6908 - accuracy: 0.50 - 1s 889us/step - loss: 0.6914 - accuracy: 0.5009 - val_loss: 0.6948 - val_accuracy: 0.4787\n",
      "Epoch 100/100\n",
      "563/563 [==============================] - ETA: 0s - loss: 0.6831 - accuracy: 0.50 - ETA: 0s - loss: 0.6914 - accuracy: 0.46 - ETA: 0s - loss: 0.6951 - accuracy: 0.45 - ETA: 0s - loss: 0.6916 - accuracy: 0.48 - 0s 464us/step - loss: 0.6919 - accuracy: 0.4654 - val_loss: 0.6937 - val_accuracy: 0.4681\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#building and training a model\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, Y, test_size=0.25)\n",
    "model = Sequential()\n",
    "model.add(Dense(16, input_shape=(5,) ))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,\n",
    "                       patience=100, min_delta=0.0001, restore_best_weights = True)\n",
    "history = model.fit(X_train, y_train, epochs=100,\n",
    "                    validation_data= (X_test,y_test),\n",
    "                              callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188/188 [==============================] - ETA:  - 0s 149us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6936830586575448, 0.4680851101875305]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#evaluating the model\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM AND GRU METHOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_df1 = copy.deepcopy(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_df1 = normalized_df1[['Open','High','Low','Close','compound_mean']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizing function\n",
    "def normalized_df(df):\n",
    "    normalized_df=(df-df.mean())/df.std()\n",
    "    return normalized_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for later\n",
    "normalized_df2 = copy.deepcopy(normalized_df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>compound_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>25.969999</td>\n",
       "      <td>26.370001</td>\n",
       "      <td>25.959999</td>\n",
       "      <td>26.330000</td>\n",
       "      <td>0.075921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>26.490000</td>\n",
       "      <td>26.879999</td>\n",
       "      <td>26.360001</td>\n",
       "      <td>26.770000</td>\n",
       "      <td>0.765000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>26.540001</td>\n",
       "      <td>26.980000</td>\n",
       "      <td>26.510000</td>\n",
       "      <td>26.920000</td>\n",
       "      <td>0.427933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>26.620001</td>\n",
       "      <td>26.799999</td>\n",
       "      <td>26.490000</td>\n",
       "      <td>26.629999</td>\n",
       "      <td>0.446600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>26.549999</td>\n",
       "      <td>26.790001</td>\n",
       "      <td>26.340000</td>\n",
       "      <td>26.540001</td>\n",
       "      <td>0.440400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>747</td>\n",
       "      <td>59.700001</td>\n",
       "      <td>60.590000</td>\n",
       "      <td>59.560001</td>\n",
       "      <td>60.220001</td>\n",
       "      <td>0.075921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>748</td>\n",
       "      <td>60.430000</td>\n",
       "      <td>60.459999</td>\n",
       "      <td>59.799999</td>\n",
       "      <td>59.950001</td>\n",
       "      <td>0.670500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>749</td>\n",
       "      <td>62.500000</td>\n",
       "      <td>63.419998</td>\n",
       "      <td>62.240002</td>\n",
       "      <td>62.980000</td>\n",
       "      <td>0.381800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>63.689999</td>\n",
       "      <td>63.799999</td>\n",
       "      <td>63.029999</td>\n",
       "      <td>63.540001</td>\n",
       "      <td>0.571900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>751</td>\n",
       "      <td>62.619999</td>\n",
       "      <td>62.869999</td>\n",
       "      <td>62.349998</td>\n",
       "      <td>62.700001</td>\n",
       "      <td>0.617000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>752 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Open       High        Low      Close  compound_mean\n",
       "0    25.969999  26.370001  25.959999  26.330000       0.075921\n",
       "1    26.490000  26.879999  26.360001  26.770000       0.765000\n",
       "2    26.540001  26.980000  26.510000  26.920000       0.427933\n",
       "3    26.620001  26.799999  26.490000  26.629999       0.446600\n",
       "4    26.549999  26.790001  26.340000  26.540001       0.440400\n",
       "..         ...        ...        ...        ...            ...\n",
       "747  59.700001  60.590000  59.560001  60.220001       0.075921\n",
       "748  60.430000  60.459999  59.799999  59.950001       0.670500\n",
       "749  62.500000  63.419998  62.240002  62.980000       0.381800\n",
       "750  63.689999  63.799999  63.029999  63.540001       0.571900\n",
       "751  62.619999  62.869999  62.349998  62.700001       0.617000\n",
       "\n",
       "[752 rows x 5 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = normalized_df1.mean(axis = 0)\n",
    "normalized_df1 -= mean\n",
    "std = normalized_df1.std(axis=0)\n",
    "normalized_df1 /= std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding label: up or down or steady\n",
    "def add_label(df):\n",
    "    idx = len(df.columns)\n",
    "    new_col = np.where(df['Close'] >= df['Close'].shift(1), 1, 0)  \n",
    "    df.insert(loc=idx, column='Label', value=new_col)\n",
    "    df = df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_label(normalized_df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_df1 = normalized_df1.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.234, -1.22 , -1.208, -1.198, -0.012,  0.   ],\n",
       "       [-1.181, -1.169, -1.167, -1.154,  2.354,  1.   ],\n",
       "       [-1.176, -1.159, -1.152, -1.139,  1.197,  1.   ],\n",
       "       ...,\n",
       "       [ 2.43 ,  2.473,  2.452,  2.472,  1.038,  1.   ],\n",
       "       [ 2.549,  2.511,  2.531,  2.528,  1.691,  1.   ],\n",
       "       [ 2.442,  2.418,  2.463,  2.444,  1.846,  0.   ]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "# defining our generator\n",
    "def generator(data, lookback, delay, min_index, max_index,\n",
    "              shuffle=False, batch_size=32, step=5):\n",
    "    if max_index is None:\n",
    "        max_index = len(data) - delay - 1\n",
    "    i = min_index + lookback\n",
    "    while 1:\n",
    "        if shuffle:\n",
    "            rows = np.random.randint(\n",
    "                min_index + lookback, max_index, size=batch_size)\n",
    "        else:\n",
    "            if i + batch_size >= max_index:\n",
    "                i = min_index + lookback\n",
    "            rows = np.arange(i, min(i + batch_size, max_index))\n",
    "            i += len(rows)\n",
    "        samples = np.zeros((len(rows),\n",
    "                           lookback // step,\n",
    "                           data.shape[-1]))\n",
    "        targets = np.zeros((len(rows),))\n",
    "        for j, row in enumerate(rows):\n",
    "            indices = range(rows[j] - lookback, rows[j], step)\n",
    "            samples[j] = data[indices]\n",
    "            targets[j] = data[rows[j] + delay][-1]\n",
    "        yield samples, to_categorical(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookback = 30\n",
    "step = 10\n",
    "delay = 1\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting data into train, test and validation set\n",
    "train_gen = generator(normalized_df1,\n",
    "                      lookback=lookback,\n",
    "                      delay=delay,\n",
    "                      min_index=0,\n",
    "                      max_index=round(0.6*len(normalized_df1)),\n",
    "                      shuffle=False,\n",
    "                      step=step,\n",
    "                      batch_size=batch_size)\n",
    "val_gen = generator(normalized_df1,\n",
    "                    lookback=lookback,\n",
    "                    delay=delay,\n",
    "                    min_index=round(0.6*len(normalized_df1))+1,\n",
    "                    max_index=round(0.8*len(normalized_df1)),\n",
    "                    step=step,\n",
    "                    batch_size=batch_size)\n",
    "test_gen = generator(normalized_df1,\n",
    "                     lookback=lookback,\n",
    "                     delay=delay,\n",
    "                     min_index=round(0.8*len(normalized_df1))+1,\n",
    "                     max_index=None,\n",
    "                     step=step,\n",
    "                     batch_size=batch_size)\n",
    "\n",
    "val_steps = (round(0.8*len(normalized_df1)) - round(0.6*len(normalized_df1))+1 - lookback) # how many steps to draw from val_gen in order to see the entire validation set\n",
    "test_steps = (len(normalized_df1) - round(0.8*len(normalized_df1))+1 - lookback)\n",
    "# How many steps to draw from test_gen in order to see the entire test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "2/2 [==============================] - ETA: 2s - loss: 0.2497 - accuracy: 0.56 - 3s 2s/step - loss: 0.2484 - accuracy: 0.5625 - val_loss: 0.2511 - val_accuracy: 0.4380\n",
      "Epoch 2/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2511 - accuracy: 0.50 - 1s 267ms/step - loss: 0.2522 - accuracy: 0.4375 - val_loss: 0.2498 - val_accuracy: 0.4375\n",
      "Epoch 3/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2488 - accuracy: 0.68 - 0s 226ms/step - loss: 0.2499 - accuracy: 0.5781 - val_loss: 0.2499 - val_accuracy: 0.4267\n",
      "Epoch 4/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2491 - accuracy: 0.56 - 1s 328ms/step - loss: 0.2496 - accuracy: 0.5312 - val_loss: 0.2507 - val_accuracy: 0.4380\n",
      "Epoch 5/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2482 - accuracy: 0.53 - 0s 223ms/step - loss: 0.2483 - accuracy: 0.5469 - val_loss: 0.2499 - val_accuracy: 0.4375\n",
      "Epoch 6/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2494 - accuracy: 0.56 - 1s 264ms/step - loss: 0.2498 - accuracy: 0.5312 - val_loss: 0.2508 - val_accuracy: 0.4370\n",
      "Epoch 7/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2502 - accuracy: 0.50 - 1s 275ms/step - loss: 0.2494 - accuracy: 0.5312 - val_loss: 0.2510 - val_accuracy: 0.4380\n",
      "Epoch 8/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2482 - accuracy: 0.56 - 1s 271ms/step - loss: 0.2493 - accuracy: 0.5312 - val_loss: 0.2497 - val_accuracy: 0.4375\n",
      "Epoch 9/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2540 - accuracy: 0.37 - 1s 284ms/step - loss: 0.2526 - accuracy: 0.4062 - val_loss: 0.2508 - val_accuracy: 0.4370\n",
      "Epoch 10/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2527 - accuracy: 0.40 - 1s 316ms/step - loss: 0.2523 - accuracy: 0.4219 - val_loss: 0.2504 - val_accuracy: 0.4380\n",
      "Epoch 11/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2506 - accuracy: 0.53 - 0s 225ms/step - loss: 0.2501 - accuracy: 0.5312 - val_loss: 0.2499 - val_accuracy: 0.4375\n",
      "Epoch 12/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2511 - accuracy: 0.53 - 1s 285ms/step - loss: 0.2501 - accuracy: 0.5469 - val_loss: 0.2509 - val_accuracy: 0.4370\n",
      "Epoch 13/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2501 - accuracy: 0.50 - 0s 210ms/step - loss: 0.2501 - accuracy: 0.5000 - val_loss: 0.2509 - val_accuracy: 0.4380\n",
      "Epoch 14/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2458 - accuracy: 0.59 - 1s 257ms/step - loss: 0.2472 - accuracy: 0.5781 - val_loss: 0.2498 - val_accuracy: 0.4375\n",
      "Epoch 15/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2504 - accuracy: 0.50 - 0s 221ms/step - loss: 0.2519 - accuracy: 0.4375 - val_loss: 0.2506 - val_accuracy: 0.4370\n",
      "Epoch 16/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2516 - accuracy: 0.50 - 0s 218ms/step - loss: 0.2526 - accuracy: 0.4375 - val_loss: 0.2503 - val_accuracy: 0.4698\n",
      "Epoch 17/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2505 - accuracy: 0.53 - 1s 300ms/step - loss: 0.2491 - accuracy: 0.5781 - val_loss: 0.2498 - val_accuracy: 0.4375\n",
      "Epoch 18/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2506 - accuracy: 0.50 - 1s 311ms/step - loss: 0.2496 - accuracy: 0.5156 - val_loss: 0.2507 - val_accuracy: 0.4370\n",
      "Epoch 19/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2490 - accuracy: 0.56 - 1s 294ms/step - loss: 0.2499 - accuracy: 0.5312 - val_loss: 0.2508 - val_accuracy: 0.4380\n",
      "Epoch 20/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2504 - accuracy: 0.50 - 1s 344ms/step - loss: 0.2491 - accuracy: 0.5469 - val_loss: 0.2499 - val_accuracy: 0.4375\n",
      "Epoch 21/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2481 - accuracy: 0.56 - 1s 291ms/step - loss: 0.2495 - accuracy: 0.5312 - val_loss: 0.2514 - val_accuracy: 0.4370\n",
      "Epoch 22/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2558 - accuracy: 0.37 - 1s 342ms/step - loss: 0.2535 - accuracy: 0.4375 - val_loss: 0.2505 - val_accuracy: 0.4380\n",
      "Epoch 23/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2529 - accuracy: 0.40 - 1s 295ms/step - loss: 0.2506 - accuracy: 0.4688 - val_loss: 0.2499 - val_accuracy: 0.4375\n",
      "Epoch 24/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2470 - accuracy: 0.62 - 1s 350ms/step - loss: 0.2487 - accuracy: 0.5625 - val_loss: 0.2506 - val_accuracy: 0.4370\n",
      "Epoch 25/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2487 - accuracy: 0.53 - 1s 377ms/step - loss: 0.2488 - accuracy: 0.5469 - val_loss: 0.2508 - val_accuracy: 0.4380\n",
      "Epoch 26/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2510 - accuracy: 0.50 - 1s 318ms/step - loss: 0.2507 - accuracy: 0.5000 - val_loss: 0.2499 - val_accuracy: 0.4375\n",
      "Epoch 27/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2452 - accuracy: 0.59 - 1s 349ms/step - loss: 0.2459 - accuracy: 0.5781 - val_loss: 0.2518 - val_accuracy: 0.4370\n",
      "Epoch 28/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2515 - accuracy: 0.50 - 1s 333ms/step - loss: 0.2532 - accuracy: 0.4375 - val_loss: 0.2503 - val_accuracy: 0.4380\n",
      "Epoch 29/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2510 - accuracy: 0.50 - 1s 301ms/step - loss: 0.2516 - accuracy: 0.4531 - val_loss: 0.2497 - val_accuracy: 0.4793\n",
      "Epoch 30/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2490 - accuracy: 0.56 - 1s 349ms/step - loss: 0.2482 - accuracy: 0.5938 - val_loss: 0.2508 - val_accuracy: 0.4370\n",
      "Epoch 31/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2496 - accuracy: 0.50 - 1s 291ms/step - loss: 0.2490 - accuracy: 0.5156 - val_loss: 0.2503 - val_accuracy: 0.4380\n",
      "Epoch 32/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2486 - accuracy: 0.56 - 1s 348ms/step - loss: 0.2496 - accuracy: 0.5312 - val_loss: 0.2498 - val_accuracy: 0.4375\n",
      "Epoch 33/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2506 - accuracy: 0.50 - 1s 300ms/step - loss: 0.2486 - accuracy: 0.5469 - val_loss: 0.2515 - val_accuracy: 0.4370\n",
      "Epoch 34/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2473 - accuracy: 0.56 - 1s 328ms/step - loss: 0.2481 - accuracy: 0.5312 - val_loss: 0.2505 - val_accuracy: 0.4380\n",
      "Epoch 35/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2556 - accuracy: 0.37 - 1s 355ms/step - loss: 0.2534 - accuracy: 0.4375 - val_loss: 0.2497 - val_accuracy: 0.4375\n",
      "Epoch 36/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2520 - accuracy: 0.40 - 1s 297ms/step - loss: 0.2502 - accuracy: 0.4844 - val_loss: 0.2508 - val_accuracy: 0.4677\n",
      "Epoch 37/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2471 - accuracy: 0.62 - 1s 353ms/step - loss: 0.2489 - accuracy: 0.5625 - val_loss: 0.2503 - val_accuracy: 0.4380\n",
      "Epoch 38/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2489 - accuracy: 0.53 - 1s 305ms/step - loss: 0.2484 - accuracy: 0.5469 - val_loss: 0.2498 - val_accuracy: 0.4375\n",
      "Epoch 39/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2499 - accuracy: 0.50 - 1s 329ms/step - loss: 0.2500 - accuracy: 0.5000 - val_loss: 0.2516 - val_accuracy: 0.4370\n",
      "Epoch 40/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2437 - accuracy: 0.59 - 1s 349ms/step - loss: 0.2441 - accuracy: 0.5781 - val_loss: 0.2507 - val_accuracy: 0.4380\n",
      "Epoch 41/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2500 - accuracy: 0.50 - 1s 299ms/step - loss: 0.2520 - accuracy: 0.4375 - val_loss: 0.2497 - val_accuracy: 0.4375\n",
      "Epoch 42/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2476 - accuracy: 0.56 - 1s 354ms/step - loss: 0.2499 - accuracy: 0.4688 - val_loss: 0.2504 - val_accuracy: 0.5207\n",
      "Epoch 43/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2530 - accuracy: 0.40 - 1s 310ms/step - loss: 0.2494 - accuracy: 0.5625 - val_loss: 0.2501 - val_accuracy: 0.4905\n",
      "Epoch 44/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2510 - accuracy: 0.50 - 1s 336ms/step - loss: 0.2504 - accuracy: 0.4844 - val_loss: 0.2497 - val_accuracy: 0.4690\n",
      "Epoch 45/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2483 - accuracy: 0.62 - 1s 353ms/step - loss: 0.2499 - accuracy: 0.5625 - val_loss: 0.2515 - val_accuracy: 0.4370\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2503 - accuracy: 0.46 - 1s 290ms/step - loss: 0.2492 - accuracy: 0.5312 - val_loss: 0.2504 - val_accuracy: 0.4380\n",
      "Epoch 47/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2475 - accuracy: 0.56 - 1s 355ms/step - loss: 0.2487 - accuracy: 0.5312 - val_loss: 0.2496 - val_accuracy: 0.4375\n",
      "Epoch 48/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2555 - accuracy: 0.37 - 1s 275ms/step - loss: 0.2525 - accuracy: 0.4375 - val_loss: 0.2514 - val_accuracy: 0.4370\n",
      "Epoch 49/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2518 - accuracy: 0.40 - 1s 352ms/step - loss: 0.2497 - accuracy: 0.4531 - val_loss: 0.2499 - val_accuracy: 0.5013\n",
      "Epoch 50/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2490 - accuracy: 0.62 - 1s 333ms/step - loss: 0.2488 - accuracy: 0.5625 - val_loss: 0.2496 - val_accuracy: 0.4375\n",
      "Epoch 51/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2481 - accuracy: 0.56 - 1s 297ms/step - loss: 0.2483 - accuracy: 0.5625 - val_loss: 0.2514 - val_accuracy: 0.4370\n",
      "Epoch 52/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2495 - accuracy: 0.50 - 1s 336ms/step - loss: 0.2504 - accuracy: 0.5000 - val_loss: 0.2503 - val_accuracy: 0.4380\n",
      "Epoch 53/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2475 - accuracy: 0.59 - 1s 291ms/step - loss: 0.2463 - accuracy: 0.5781 - val_loss: 0.2499 - val_accuracy: 0.4375\n",
      "Epoch 54/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2504 - accuracy: 0.50 - 1s 350ms/step - loss: 0.2531 - accuracy: 0.4375 - val_loss: 0.2517 - val_accuracy: 0.4370\n",
      "Epoch 55/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2485 - accuracy: 0.50 - 1s 320ms/step - loss: 0.2507 - accuracy: 0.4531 - val_loss: 0.2499 - val_accuracy: 0.5013\n",
      "Epoch 56/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2490 - accuracy: 0.53 - 1s 298ms/step - loss: 0.2480 - accuracy: 0.5781 - val_loss: 0.2498 - val_accuracy: 0.4375\n",
      "Epoch 57/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2495 - accuracy: 0.50 - 1s 340ms/step - loss: 0.2484 - accuracy: 0.5156 - val_loss: 0.2514 - val_accuracy: 0.4370\n",
      "Epoch 58/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2458 - accuracy: 0.56 - 1s 287ms/step - loss: 0.2486 - accuracy: 0.5312 - val_loss: 0.2502 - val_accuracy: 0.4380\n",
      "Epoch 59/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2498 - accuracy: 0.56 - 1s 342ms/step - loss: 0.2458 - accuracy: 0.5781 - val_loss: 0.2495 - val_accuracy: 0.4375\n",
      "Epoch 60/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2496 - accuracy: 0.56 - 1s 286ms/step - loss: 0.2499 - accuracy: 0.5312 - val_loss: 0.2522 - val_accuracy: 0.4370\n",
      "Epoch 61/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2560 - accuracy: 0.37 - 1s 342ms/step - loss: 0.2534 - accuracy: 0.4375 - val_loss: 0.2496 - val_accuracy: 0.4703\n",
      "Epoch 62/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2558 - accuracy: 0.37 - 1s 331ms/step - loss: 0.2512 - accuracy: 0.4531 - val_loss: 0.2494 - val_accuracy: 0.4785\n",
      "Epoch 63/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2441 - accuracy: 0.62 - 1s 286ms/step - loss: 0.2468 - accuracy: 0.5625 - val_loss: 0.2518 - val_accuracy: 0.4992\n",
      "Epoch 64/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2503 - accuracy: 0.56 - 1s 341ms/step - loss: 0.2501 - accuracy: 0.5469 - val_loss: 0.2499 - val_accuracy: 0.4590\n",
      "Epoch 65/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2494 - accuracy: 0.59 - 1s 290ms/step - loss: 0.2497 - accuracy: 0.5312 - val_loss: 0.2492 - val_accuracy: 0.4995\n",
      "Epoch 66/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2455 - accuracy: 0.59 - 1s 338ms/step - loss: 0.2452 - accuracy: 0.5781 - val_loss: 0.2531 - val_accuracy: 0.4472\n",
      "Epoch 67/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2517 - accuracy: 0.50 - 1s 288ms/step - loss: 0.2532 - accuracy: 0.4375 - val_loss: 0.2495 - val_accuracy: 0.4805\n",
      "Epoch 68/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2509 - accuracy: 0.46 - 1s 342ms/step - loss: 0.2520 - accuracy: 0.4531 - val_loss: 0.2494 - val_accuracy: 0.4992\n",
      "Epoch 69/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2497 - accuracy: 0.56 - 1s 336ms/step - loss: 0.2492 - accuracy: 0.5938 - val_loss: 0.2528 - val_accuracy: 0.4785\n",
      "Epoch 70/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2494 - accuracy: 0.46 - 1s 289ms/step - loss: 0.2498 - accuracy: 0.4688 - val_loss: 0.2498 - val_accuracy: 0.4905\n",
      "Epoch 71/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2471 - accuracy: 0.62 - 1s 349ms/step - loss: 0.2489 - accuracy: 0.5625 - val_loss: 0.2493 - val_accuracy: 0.4793\n",
      "Epoch 72/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2508 - accuracy: 0.53 - 1s 286ms/step - loss: 0.2476 - accuracy: 0.5625 - val_loss: 0.2534 - val_accuracy: 0.4575\n",
      "Epoch 73/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2440 - accuracy: 0.56 - 1s 377ms/step - loss: 0.2475 - accuracy: 0.5312 - val_loss: 0.2501 - val_accuracy: 0.4800\n",
      "Epoch 74/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2528 - accuracy: 0.37 - 1s 339ms/step - loss: 0.2524 - accuracy: 0.4531 - val_loss: 0.2493 - val_accuracy: 0.4892\n",
      "Epoch 75/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2518 - accuracy: 0.50 - 1s 291ms/step - loss: 0.2513 - accuracy: 0.4688 - val_loss: 0.2523 - val_accuracy: 0.4782\n",
      "Epoch 76/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2465 - accuracy: 0.56 - 1s 351ms/step - loss: 0.2485 - accuracy: 0.5312 - val_loss: 0.2497 - val_accuracy: 0.4803\n",
      "Epoch 77/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2481 - accuracy: 0.59 - 1s 287ms/step - loss: 0.2477 - accuracy: 0.5781 - val_loss: 0.2492 - val_accuracy: 0.4688\n",
      "Epoch 78/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2497 - accuracy: 0.59 - 1s 342ms/step - loss: 0.2503 - accuracy: 0.4844 - val_loss: 0.2533 - val_accuracy: 0.4890\n",
      "Epoch 79/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2464 - accuracy: 0.59 - 1s 324ms/step - loss: 0.2450 - accuracy: 0.5781 - val_loss: 0.2501 - val_accuracy: 0.4695\n",
      "Epoch 80/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2501 - accuracy: 0.50 - 1s 297ms/step - loss: 0.2530 - accuracy: 0.4688 - val_loss: 0.2492 - val_accuracy: 0.4992\n",
      "Epoch 81/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2484 - accuracy: 0.50 - 1s 350ms/step - loss: 0.2494 - accuracy: 0.5469 - val_loss: 0.2523 - val_accuracy: 0.4782\n",
      "Epoch 82/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2478 - accuracy: 0.59 - 1s 290ms/step - loss: 0.2484 - accuracy: 0.5625 - val_loss: 0.2496 - val_accuracy: 0.5015\n",
      "Epoch 83/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2514 - accuracy: 0.56 - 1s 348ms/step - loss: 0.2511 - accuracy: 0.5312 - val_loss: 0.2492 - val_accuracy: 0.4992\n",
      "Epoch 84/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2475 - accuracy: 0.71 - 1s 285ms/step - loss: 0.2496 - accuracy: 0.5781 - val_loss: 0.2535 - val_accuracy: 0.4785\n",
      "Epoch 85/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2497 - accuracy: 0.59 - 1s 362ms/step - loss: 0.2456 - accuracy: 0.5938 - val_loss: 0.2501 - val_accuracy: 0.4800\n",
      "Epoch 86/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2479 - accuracy: 0.56 - 1s 335ms/step - loss: 0.2499 - accuracy: 0.5312 - val_loss: 0.2490 - val_accuracy: 0.4898\n",
      "Epoch 87/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2541 - accuracy: 0.34 - 1s 301ms/step - loss: 0.2519 - accuracy: 0.4219 - val_loss: 0.2537 - val_accuracy: 0.4887\n",
      "Epoch 88/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2566 - accuracy: 0.31 - 1s 360ms/step - loss: 0.2526 - accuracy: 0.4062 - val_loss: 0.2494 - val_accuracy: 0.4910\n",
      "Epoch 89/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2468 - accuracy: 0.59 - 1s 299ms/step - loss: 0.2478 - accuracy: 0.5625 - val_loss: 0.2490 - val_accuracy: 0.4992\n",
      "Epoch 90/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2483 - accuracy: 0.53 - 1s 352ms/step - loss: 0.2472 - accuracy: 0.5625 - val_loss: 0.2538 - val_accuracy: 0.4785\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 91/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2489 - accuracy: 0.59 - 1s 334ms/step - loss: 0.2497 - accuracy: 0.5625 - val_loss: 0.2500 - val_accuracy: 0.4695\n",
      "Epoch 92/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2398 - accuracy: 0.59 - 1s 283ms/step - loss: 0.2431 - accuracy: 0.5781 - val_loss: 0.2489 - val_accuracy: 0.4480\n",
      "Epoch 93/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2516 - accuracy: 0.50 - 1s 339ms/step - loss: 0.2525 - accuracy: 0.4375 - val_loss: 0.2543 - val_accuracy: 0.4580\n",
      "Epoch 94/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2515 - accuracy: 0.43 - 1s 283ms/step - loss: 0.2516 - accuracy: 0.4844 - val_loss: 0.2493 - val_accuracy: 0.5015\n",
      "Epoch 95/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2516 - accuracy: 0.46 - 1s 366ms/step - loss: 0.2473 - accuracy: 0.5625 - val_loss: 0.2489 - val_accuracy: 0.4782\n",
      "Epoch 96/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2530 - accuracy: 0.50 - 1s 272ms/step - loss: 0.2497 - accuracy: 0.5781 - val_loss: 0.2538 - val_accuracy: 0.4787\n",
      "Epoch 97/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2463 - accuracy: 0.53 - 1s 349ms/step - loss: 0.2487 - accuracy: 0.5000 - val_loss: 0.2497 - val_accuracy: 0.5013\n",
      "Epoch 98/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2494 - accuracy: 0.56 - 1s 327ms/step - loss: 0.2467 - accuracy: 0.5781 - val_loss: 0.2488 - val_accuracy: 0.4582\n",
      "Epoch 99/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2559 - accuracy: 0.56 - 1s 282ms/step - loss: 0.2528 - accuracy: 0.5312 - val_loss: 0.2543 - val_accuracy: 0.4990\n",
      "Epoch 100/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2538 - accuracy: 0.40 - 1s 352ms/step - loss: 0.2510 - accuracy: 0.4844 - val_loss: 0.2493 - val_accuracy: 0.4910\n",
      "Epoch 101/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2554 - accuracy: 0.34 - 1s 285ms/step - loss: 0.2530 - accuracy: 0.4688 - val_loss: 0.2489 - val_accuracy: 0.4577\n",
      "Epoch 102/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2471 - accuracy: 0.56 - 1s 340ms/step - loss: 0.2479 - accuracy: 0.5312 - val_loss: 0.2536 - val_accuracy: 0.4785\n",
      "Epoch 103/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2489 - accuracy: 0.56 - 1s 274ms/step - loss: 0.2476 - accuracy: 0.5625 - val_loss: 0.2496 - val_accuracy: 0.4908\n",
      "Epoch 104/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2504 - accuracy: 0.43 - 1s 340ms/step - loss: 0.2500 - accuracy: 0.5156 - val_loss: 0.2486 - val_accuracy: 0.4682\n",
      "Epoch 105/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2464 - accuracy: 0.59 - 1s 339ms/step - loss: 0.2455 - accuracy: 0.5781 - val_loss: 0.2561 - val_accuracy: 0.4677\n",
      "Epoch 106/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2485 - accuracy: 0.50 - 1s 286ms/step - loss: 0.2527 - accuracy: 0.3906 - val_loss: 0.2493 - val_accuracy: 0.4910\n",
      "Epoch 107/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2509 - accuracy: 0.53 - 1s 344ms/step - loss: 0.2534 - accuracy: 0.4531 - val_loss: 0.2488 - val_accuracy: 0.4782\n",
      "Epoch 108/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2498 - accuracy: 0.59 - 1s 287ms/step - loss: 0.2483 - accuracy: 0.5938 - val_loss: 0.2550 - val_accuracy: 0.4580\n",
      "Epoch 109/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2472 - accuracy: 0.53 - 1s 380ms/step - loss: 0.2480 - accuracy: 0.5312 - val_loss: 0.2495 - val_accuracy: 0.4700\n",
      "Epoch 110/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2467 - accuracy: 0.65 - 1s 360ms/step - loss: 0.2485 - accuracy: 0.5625 - val_loss: 0.2486 - val_accuracy: 0.4787\n",
      "Epoch 111/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2510 - accuracy: 0.43 - 1s 294ms/step - loss: 0.2471 - accuracy: 0.5156 - val_loss: 0.2569 - val_accuracy: 0.4577\n",
      "Epoch 112/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2432 - accuracy: 0.56 - 1s 339ms/step - loss: 0.2493 - accuracy: 0.5312 - val_loss: 0.2497 - val_accuracy: 0.4698\n",
      "Epoch 113/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2542 - accuracy: 0.37 - 1s 282ms/step - loss: 0.2530 - accuracy: 0.4062 - val_loss: 0.2486 - val_accuracy: 0.4782\n",
      "Epoch 114/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2527 - accuracy: 0.43 - 1s 348ms/step - loss: 0.2504 - accuracy: 0.4844 - val_loss: 0.2554 - val_accuracy: 0.4682\n",
      "Epoch 115/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2451 - accuracy: 0.65 - 1s 274ms/step - loss: 0.2467 - accuracy: 0.6094 - val_loss: 0.2494 - val_accuracy: 0.4700\n",
      "Epoch 116/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2488 - accuracy: 0.65 - 1s 345ms/step - loss: 0.2483 - accuracy: 0.5938 - val_loss: 0.2485 - val_accuracy: 0.4682\n",
      "Epoch 117/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2499 - accuracy: 0.46 - 1s 333ms/step - loss: 0.2496 - accuracy: 0.4219 - val_loss: 0.2580 - val_accuracy: 0.4472\n",
      "Epoch 118/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2508 - accuracy: 0.59 - 1s 287ms/step - loss: 0.2475 - accuracy: 0.5781 - val_loss: 0.2500 - val_accuracy: 0.4488\n",
      "Epoch 119/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2471 - accuracy: 0.50 - 1s 357ms/step - loss: 0.2503 - accuracy: 0.4531 - val_loss: 0.2484 - val_accuracy: 0.4577\n",
      "Epoch 120/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2488 - accuracy: 0.56 - 1s 276ms/step - loss: 0.2511 - accuracy: 0.4844 - val_loss: 0.2567 - val_accuracy: 0.4787\n",
      "Epoch 121/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2472 - accuracy: 0.68 - 1s 345ms/step - loss: 0.2457 - accuracy: 0.6719 - val_loss: 0.2495 - val_accuracy: 0.4698\n",
      "Epoch 122/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2521 - accuracy: 0.50 - 1s 295ms/step - loss: 0.2497 - accuracy: 0.5156 - val_loss: 0.2484 - val_accuracy: 0.4577\n",
      "Epoch 123/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2492 - accuracy: 0.53 - 1s 353ms/step - loss: 0.2495 - accuracy: 0.5625 - val_loss: 0.2586 - val_accuracy: 0.4475\n",
      "Epoch 124/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2474 - accuracy: 0.59 - 1s 330ms/step - loss: 0.2446 - accuracy: 0.5938 - val_loss: 0.2501 - val_accuracy: 0.4383\n",
      "Epoch 125/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2473 - accuracy: 0.56 - 1s 292ms/step - loss: 0.2475 - accuracy: 0.5469 - val_loss: 0.2482 - val_accuracy: 0.4372\n",
      "Epoch 126/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2512 - accuracy: 0.43 - 1s 359ms/step - loss: 0.2503 - accuracy: 0.5625 - val_loss: 0.2596 - val_accuracy: 0.4577\n",
      "Epoch 127/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2527 - accuracy: 0.50 - 1s 327ms/step - loss: 0.2503 - accuracy: 0.5156 - val_loss: 0.2492 - val_accuracy: 0.4700\n",
      "Epoch 128/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2502 - accuracy: 0.40 - 1s 395ms/step - loss: 0.2502 - accuracy: 0.4531 - val_loss: 0.2483 - val_accuracy: 0.4682\n",
      "Epoch 129/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2482 - accuracy: 0.62 - 1s 367ms/step - loss: 0.2459 - accuracy: 0.6562 - val_loss: 0.2596 - val_accuracy: 0.4682\n",
      "Epoch 130/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2498 - accuracy: 0.50 - 1s 294ms/step - loss: 0.2491 - accuracy: 0.5156 - val_loss: 0.2500 - val_accuracy: 0.4383\n",
      "Epoch 131/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2516 - accuracy: 0.59 - 1s 351ms/step - loss: 0.2512 - accuracy: 0.5625 - val_loss: 0.2481 - val_accuracy: 0.4372\n",
      "Epoch 132/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2491 - accuracy: 0.53 - 1s 282ms/step - loss: 0.2504 - accuracy: 0.4844 - val_loss: 0.2598 - val_accuracy: 0.4680\n",
      "Epoch 133/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2506 - accuracy: 0.46 - 1s 308ms/step - loss: 0.2505 - accuracy: 0.4375 - val_loss: 0.2493 - val_accuracy: 0.4595\n",
      "Epoch 134/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2503 - accuracy: 0.56 - 1s 362ms/step - loss: 0.2485 - accuracy: 0.5625 - val_loss: 0.2482 - val_accuracy: 0.4682\n",
      "Epoch 135/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2483 - accuracy: 0.62 - 1s 286ms/step - loss: 0.2492 - accuracy: 0.5625 - val_loss: 0.2594 - val_accuracy: 0.4785\n",
      "Epoch 136/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2458 - accuracy: 0.68 - 1s 336ms/step - loss: 0.2472 - accuracy: 0.5938 - val_loss: 0.2501 - val_accuracy: 0.4593\n",
      "Epoch 137/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2499 - accuracy: 0.46 - 1s 277ms/step - loss: 0.2480 - accuracy: 0.5312 - val_loss: 0.2478 - val_accuracy: 0.4372\n",
      "Epoch 138/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2488 - accuracy: 0.56 - 1s 342ms/step - loss: 0.2499 - accuracy: 0.5000 - val_loss: 0.2613 - val_accuracy: 0.4475\n",
      "Epoch 139/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2553 - accuracy: 0.34 - 1s 320ms/step - loss: 0.2525 - accuracy: 0.4844 - val_loss: 0.2497 - val_accuracy: 0.4698\n",
      "Epoch 140/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2513 - accuracy: 0.37 - 1s 291ms/step - loss: 0.2507 - accuracy: 0.4531 - val_loss: 0.2480 - val_accuracy: 0.4577\n",
      "Epoch 141/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2468 - accuracy: 0.59 - 1s 339ms/step - loss: 0.2472 - accuracy: 0.5469 - val_loss: 0.2606 - val_accuracy: 0.4577\n",
      "Epoch 142/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2490 - accuracy: 0.53 - 1s 281ms/step - loss: 0.2486 - accuracy: 0.5000 - val_loss: 0.2503 - val_accuracy: 0.4485\n",
      "Epoch 143/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2488 - accuracy: 0.59 - 1s 342ms/step - loss: 0.2488 - accuracy: 0.5469 - val_loss: 0.2478 - val_accuracy: 0.4372\n",
      "Epoch 144/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2399 - accuracy: 0.62 - 1s 275ms/step - loss: 0.2424 - accuracy: 0.6094 - val_loss: 0.2626 - val_accuracy: 0.4370\n",
      "Epoch 145/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2529 - accuracy: 0.50 - 1s 345ms/step - loss: 0.2534 - accuracy: 0.4688 - val_loss: 0.2500 - val_accuracy: 0.4593\n",
      "Epoch 146/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2528 - accuracy: 0.34 - 1s 277ms/step - loss: 0.2525 - accuracy: 0.3906 - val_loss: 0.2478 - val_accuracy: 0.4682\n",
      "Epoch 147/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2493 - accuracy: 0.46 - 1s 339ms/step - loss: 0.2486 - accuracy: 0.5469 - val_loss: 0.2616 - val_accuracy: 0.4370\n",
      "Epoch 148/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2501 - accuracy: 0.53 - 1s 328ms/step - loss: 0.2495 - accuracy: 0.5156 - val_loss: 0.2501 - val_accuracy: 0.4383\n",
      "Epoch 149/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2453 - accuracy: 0.56 - 1s 287ms/step - loss: 0.2478 - accuracy: 0.5469 - val_loss: 0.2475 - val_accuracy: 0.4375\n",
      "Epoch 150/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2484 - accuracy: 0.53 - 1s 379ms/step - loss: 0.2477 - accuracy: 0.5469 - val_loss: 0.2643 - val_accuracy: 0.4472\n",
      "Epoch 151/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2444 - accuracy: 0.56 - 1s 485ms/step - loss: 0.2497 - accuracy: 0.5156 - val_loss: 0.2507 - val_accuracy: 0.4278\n",
      "Epoch 152/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2533 - accuracy: 0.40 - 1s 399ms/step - loss: 0.2521 - accuracy: 0.4062 - val_loss: 0.2475 - val_accuracy: 0.4580\n",
      "Epoch 153/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2498 - accuracy: 0.56 - 1s 406ms/step - loss: 0.2496 - accuracy: 0.5469 - val_loss: 0.2624 - val_accuracy: 0.4680\n",
      "Epoch 154/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2485 - accuracy: 0.50 - 1s 370ms/step - loss: 0.2491 - accuracy: 0.5156 - val_loss: 0.2503 - val_accuracy: 0.4593\n",
      "Epoch 155/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2484 - accuracy: 0.59 - 1s 290ms/step - loss: 0.2472 - accuracy: 0.5469 - val_loss: 0.2474 - val_accuracy: 0.4480\n",
      "Epoch 156/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2488 - accuracy: 0.59 - 1s 358ms/step - loss: 0.2487 - accuracy: 0.5469 - val_loss: 0.2658 - val_accuracy: 0.4372\n",
      "Epoch 157/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2442 - accuracy: 0.62 - 1s 516ms/step - loss: 0.2468 - accuracy: 0.5938 - val_loss: 0.2512 - val_accuracy: 0.4485\n",
      "Epoch 158/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2492 - accuracy: 0.50 - 1s 476ms/step - loss: 0.2488 - accuracy: 0.5156 - val_loss: 0.2473 - val_accuracy: 0.4580\n",
      "Epoch 159/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2508 - accuracy: 0.56 - 1s 350ms/step - loss: 0.2543 - accuracy: 0.4062 - val_loss: 0.2630 - val_accuracy: 0.4785\n",
      "Epoch 160/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2492 - accuracy: 0.50 - 1s 384ms/step - loss: 0.2503 - accuracy: 0.4688 - val_loss: 0.2505 - val_accuracy: 0.4593\n",
      "Epoch 161/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2519 - accuracy: 0.50 - 1s 426ms/step - loss: 0.2505 - accuracy: 0.5000 - val_loss: 0.2474 - val_accuracy: 0.4475\n",
      "Epoch 162/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2475 - accuracy: 0.50 - 1s 332ms/step - loss: 0.2476 - accuracy: 0.5312 - val_loss: 0.2658 - val_accuracy: 0.4577\n",
      "Epoch 163/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2496 - accuracy: 0.46 - 1s 352ms/step - loss: 0.2451 - accuracy: 0.5469 - val_loss: 0.2512 - val_accuracy: 0.4593\n",
      "Epoch 164/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2523 - accuracy: 0.56 - 1s 347ms/step - loss: 0.2525 - accuracy: 0.5156 - val_loss: 0.2472 - val_accuracy: 0.4475\n",
      "Epoch 165/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2526 - accuracy: 0.40 - 1s 319ms/step - loss: 0.2513 - accuracy: 0.5469 - val_loss: 0.2648 - val_accuracy: 0.4682\n",
      "Epoch 166/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2512 - accuracy: 0.40 - 1s 356ms/step - loss: 0.2495 - accuracy: 0.4844 - val_loss: 0.2503 - val_accuracy: 0.4803\n",
      "Epoch 167/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2530 - accuracy: 0.43 - 1s 322ms/step - loss: 0.2498 - accuracy: 0.5000 - val_loss: 0.2475 - val_accuracy: 0.4580\n",
      "Epoch 168/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2501 - accuracy: 0.43 - 1s 364ms/step - loss: 0.2489 - accuracy: 0.4688 - val_loss: 0.2648 - val_accuracy: 0.4575\n",
      "Epoch 169/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2481 - accuracy: 0.59 - 1s 341ms/step - loss: 0.2485 - accuracy: 0.5469 - val_loss: 0.2517 - val_accuracy: 0.4275\n",
      "Epoch 170/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2503 - accuracy: 0.59 - 1s 292ms/step - loss: 0.2498 - accuracy: 0.5781 - val_loss: 0.2475 - val_accuracy: 0.4375\n",
      "Epoch 171/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2506 - accuracy: 0.50 - 1s 343ms/step - loss: 0.2504 - accuracy: 0.5156 - val_loss: 0.2668 - val_accuracy: 0.4265\n",
      "Epoch 172/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2504 - accuracy: 0.50 - 1s 289ms/step - loss: 0.2526 - accuracy: 0.4844 - val_loss: 0.2508 - val_accuracy: 0.4593\n",
      "Epoch 173/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2449 - accuracy: 0.65 - 1s 349ms/step - loss: 0.2452 - accuracy: 0.6562 - val_loss: 0.2476 - val_accuracy: 0.4270\n",
      "Epoch 174/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2481 - accuracy: 0.46 - 1s 341ms/step - loss: 0.2461 - accuracy: 0.5781 - val_loss: 0.2674 - val_accuracy: 0.4267\n",
      "Epoch 175/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2455 - accuracy: 0.53 - 1s 296ms/step - loss: 0.2466 - accuracy: 0.5625 - val_loss: 0.2521 - val_accuracy: 0.4485\n",
      "Epoch 176/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2489 - accuracy: 0.62 - 1s 355ms/step - loss: 0.2441 - accuracy: 0.6094 - val_loss: 0.2470 - val_accuracy: 0.4585\n",
      "Epoch 177/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2485 - accuracy: 0.56 - 1s 288ms/step - loss: 0.2473 - accuracy: 0.5312 - val_loss: 0.2710 - val_accuracy: 0.4267\n",
      "Epoch 178/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2515 - accuracy: 0.31 - 1s 348ms/step - loss: 0.2511 - accuracy: 0.3750 - val_loss: 0.2515 - val_accuracy: 0.4593\n",
      "Epoch 179/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - ETA: 0s - loss: 0.2568 - accuracy: 0.56 - 1s 288ms/step - loss: 0.2551 - accuracy: 0.5000 - val_loss: 0.2470 - val_accuracy: 0.4787\n",
      "Epoch 180/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2480 - accuracy: 0.50 - 1s 350ms/step - loss: 0.2501 - accuracy: 0.4531 - val_loss: 0.2694 - val_accuracy: 0.4577\n",
      "Epoch 181/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2477 - accuracy: 0.53 - 1s 335ms/step - loss: 0.2463 - accuracy: 0.5469 - val_loss: 0.2518 - val_accuracy: 0.4485\n",
      "Epoch 182/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2480 - accuracy: 0.50 - 1s 306ms/step - loss: 0.2477 - accuracy: 0.5469 - val_loss: 0.2467 - val_accuracy: 0.4270\n",
      "Epoch 183/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2421 - accuracy: 0.59 - 1s 343ms/step - loss: 0.2443 - accuracy: 0.5781 - val_loss: 0.2743 - val_accuracy: 0.4472\n",
      "Epoch 184/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2523 - accuracy: 0.50 - 1s 286ms/step - loss: 0.2507 - accuracy: 0.4844 - val_loss: 0.2519 - val_accuracy: 0.4593\n",
      "Epoch 185/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2529 - accuracy: 0.46 - 1s 341ms/step - loss: 0.2525 - accuracy: 0.4688 - val_loss: 0.2469 - val_accuracy: 0.4787\n",
      "Epoch 186/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2484 - accuracy: 0.53 - 1s 321ms/step - loss: 0.2510 - accuracy: 0.5156 - val_loss: 0.2712 - val_accuracy: 0.4577\n",
      "Epoch 187/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2500 - accuracy: 0.46 - 1s 298ms/step - loss: 0.2484 - accuracy: 0.4844 - val_loss: 0.2519 - val_accuracy: 0.4485\n",
      "Epoch 188/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2466 - accuracy: 0.50 - 1s 343ms/step - loss: 0.2483 - accuracy: 0.4688 - val_loss: 0.2466 - val_accuracy: 0.4585\n",
      "Epoch 189/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2490 - accuracy: 0.59 - 1s 287ms/step - loss: 0.2477 - accuracy: 0.5938 - val_loss: 0.2766 - val_accuracy: 0.4575\n",
      "Epoch 190/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2451 - accuracy: 0.56 - 1s 370ms/step - loss: 0.2499 - accuracy: 0.5312 - val_loss: 0.2533 - val_accuracy: 0.4485\n",
      "Epoch 191/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2490 - accuracy: 0.37 - 1s 408ms/step - loss: 0.2505 - accuracy: 0.4062 - val_loss: 0.2468 - val_accuracy: 0.4270\n",
      "Epoch 192/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2519 - accuracy: 0.40 - 1s 400ms/step - loss: 0.2470 - accuracy: 0.5469 - val_loss: 0.2724 - val_accuracy: 0.4370\n",
      "Epoch 193/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2468 - accuracy: 0.68 - 1s 470ms/step - loss: 0.2483 - accuracy: 0.5781 - val_loss: 0.2524 - val_accuracy: 0.4380\n",
      "Epoch 194/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2484 - accuracy: 0.50 - 1s 400ms/step - loss: 0.2481 - accuracy: 0.5000 - val_loss: 0.2467 - val_accuracy: 0.4585\n",
      "Epoch 195/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2475 - accuracy: 0.46 - 1s 398ms/step - loss: 0.2484 - accuracy: 0.5156 - val_loss: 0.2770 - val_accuracy: 0.4575\n",
      "Epoch 196/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2417 - accuracy: 0.59 - 1s 320ms/step - loss: 0.2461 - accuracy: 0.5781 - val_loss: 0.2538 - val_accuracy: 0.4590\n",
      "Epoch 197/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2513 - accuracy: 0.50 - 1s 364ms/step - loss: 0.2516 - accuracy: 0.5156 - val_loss: 0.2464 - val_accuracy: 0.4375\n",
      "Epoch 198/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2522 - accuracy: 0.37 - 1s 509ms/step - loss: 0.2520 - accuracy: 0.3594 - val_loss: 0.2734 - val_accuracy: 0.4267\n",
      "Epoch 199/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2462 - accuracy: 0.50 - 1s 344ms/step - loss: 0.2459 - accuracy: 0.5625 - val_loss: 0.2535 - val_accuracy: 0.4485\n",
      "Epoch 200/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2518 - accuracy: 0.46 - 1s 410ms/step - loss: 0.2483 - accuracy: 0.5469 - val_loss: 0.2468 - val_accuracy: 0.4585\n",
      "Epoch 201/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2453 - accuracy: 0.62 - 1s 366ms/step - loss: 0.2463 - accuracy: 0.5938 - val_loss: 0.2786 - val_accuracy: 0.4472\n",
      "Epoch 202/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2472 - accuracy: 0.56 - 1s 299ms/step - loss: 0.2461 - accuracy: 0.5625 - val_loss: 0.2555 - val_accuracy: 0.4590\n",
      "Epoch 203/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2486 - accuracy: 0.56 - 1s 359ms/step - loss: 0.2476 - accuracy: 0.5469 - val_loss: 0.2468 - val_accuracy: 0.4480\n",
      "Epoch 204/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2490 - accuracy: 0.50 - 1s 355ms/step - loss: 0.2499 - accuracy: 0.5312 - val_loss: 0.2791 - val_accuracy: 0.4575\n",
      "Epoch 205/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2562 - accuracy: 0.37 - 1s 310ms/step - loss: 0.2525 - accuracy: 0.4531 - val_loss: 0.2537 - val_accuracy: 0.4485\n",
      "Epoch 206/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2501 - accuracy: 0.46 - 1s 362ms/step - loss: 0.2495 - accuracy: 0.4531 - val_loss: 0.2466 - val_accuracy: 0.4480\n",
      "Epoch 207/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2456 - accuracy: 0.56 - 1s 306ms/step - loss: 0.2444 - accuracy: 0.5781 - val_loss: 0.2786 - val_accuracy: 0.4370\n",
      "Epoch 208/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2487 - accuracy: 0.62 - 1s 358ms/step - loss: 0.2480 - accuracy: 0.5781 - val_loss: 0.2560 - val_accuracy: 0.4485\n",
      "Epoch 209/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2518 - accuracy: 0.53 - 1s 363ms/step - loss: 0.2497 - accuracy: 0.5312 - val_loss: 0.2468 - val_accuracy: 0.4375\n",
      "Epoch 210/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2514 - accuracy: 0.53 - 1s 298ms/step - loss: 0.2496 - accuracy: 0.5625 - val_loss: 0.2826 - val_accuracy: 0.4472\n",
      "Epoch 211/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2525 - accuracy: 0.46 - 1s 346ms/step - loss: 0.2536 - accuracy: 0.3906 - val_loss: 0.2543 - val_accuracy: 0.4590\n",
      "Epoch 212/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2470 - accuracy: 0.62 - 1s 327ms/step - loss: 0.2472 - accuracy: 0.6094 - val_loss: 0.2466 - val_accuracy: 0.4480\n",
      "Epoch 213/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2493 - accuracy: 0.50 - 1s 402ms/step - loss: 0.2492 - accuracy: 0.5156 - val_loss: 0.2814 - val_accuracy: 0.4472\n",
      "Epoch 214/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2399 - accuracy: 0.65 - 1s 442ms/step - loss: 0.2428 - accuracy: 0.6250 - val_loss: 0.2575 - val_accuracy: 0.4380\n",
      "Epoch 215/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2488 - accuracy: 0.50 - 1s 444ms/step - loss: 0.2434 - accuracy: 0.5312 - val_loss: 0.2467 - val_accuracy: 0.4375\n",
      "Epoch 216/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2532 - accuracy: 0.56 - 1s 313ms/step - loss: 0.2492 - accuracy: 0.5469 - val_loss: 0.2883 - val_accuracy: 0.4370\n",
      "Epoch 217/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2475 - accuracy: 0.59 - 1s 366ms/step - loss: 0.2492 - accuracy: 0.5625 - val_loss: 0.2572 - val_accuracy: 0.4485\n",
      "Epoch 218/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2577 - accuracy: 0.28 - 1s 354ms/step - loss: 0.2520 - accuracy: 0.4062 - val_loss: 0.2465 - val_accuracy: 0.4480\n",
      "Epoch 219/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2451 - accuracy: 0.65 - 1s 403ms/step - loss: 0.2469 - accuracy: 0.5781 - val_loss: 0.2878 - val_accuracy: 0.4472\n",
      "Epoch 220/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2454 - accuracy: 0.46 - 1s 495ms/step - loss: 0.2419 - accuracy: 0.5469 - val_loss: 0.2586 - val_accuracy: 0.4380\n",
      "Epoch 221/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2418 - accuracy: 0.71 - 1s 393ms/step - loss: 0.2457 - accuracy: 0.6094 - val_loss: 0.2464 - val_accuracy: 0.4375\n",
      "Epoch 222/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2501 - accuracy: 0.56 - 1s 302ms/step - loss: 0.2429 - accuracy: 0.5781 - val_loss: 0.2942 - val_accuracy: 0.4370\n",
      "Epoch 223/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2525 - accuracy: 0.53 - 1s 365ms/step - loss: 0.2509 - accuracy: 0.5312 - val_loss: 0.2590 - val_accuracy: 0.4485\n",
      "Epoch 224/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2459 - accuracy: 0.46 - 1s 338ms/step - loss: 0.2523 - accuracy: 0.3750 - val_loss: 0.2461 - val_accuracy: 0.4480\n",
      "Epoch 225/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2515 - accuracy: 0.37 - 1s 304ms/step - loss: 0.2517 - accuracy: 0.4375 - val_loss: 0.2926 - val_accuracy: 0.4472\n",
      "Epoch 226/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2467 - accuracy: 0.50 - 1s 377ms/step - loss: 0.2487 - accuracy: 0.4844 - val_loss: 0.2588 - val_accuracy: 0.4380\n",
      "Epoch 227/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2435 - accuracy: 0.46 - 1s 297ms/step - loss: 0.2448 - accuracy: 0.5469 - val_loss: 0.2460 - val_accuracy: 0.4375\n",
      "Epoch 228/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2476 - accuracy: 0.43 - 1s 355ms/step - loss: 0.2481 - accuracy: 0.5156 - val_loss: 0.2941 - val_accuracy: 0.4370\n",
      "Epoch 229/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2470 - accuracy: 0.53 - 1s 362ms/step - loss: 0.2465 - accuracy: 0.5312 - val_loss: 0.2596 - val_accuracy: 0.4380\n",
      "Epoch 230/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2469 - accuracy: 0.62 - 1s 307ms/step - loss: 0.2510 - accuracy: 0.4844 - val_loss: 0.2455 - val_accuracy: 0.4585\n",
      "Epoch 231/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2533 - accuracy: 0.50 - 1s 356ms/step - loss: 0.2517 - accuracy: 0.5312 - val_loss: 0.2887 - val_accuracy: 0.4472\n",
      "Epoch 232/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2508 - accuracy: 0.50 - 1s 286ms/step - loss: 0.2484 - accuracy: 0.5156 - val_loss: 0.2585 - val_accuracy: 0.4485\n",
      "Epoch 233/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2459 - accuracy: 0.46 - 1s 371ms/step - loss: 0.2420 - accuracy: 0.5156 - val_loss: 0.2457 - val_accuracy: 0.4375\n",
      "Epoch 234/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2461 - accuracy: 0.68 - 1s 337ms/step - loss: 0.2464 - accuracy: 0.6094 - val_loss: 0.2945 - val_accuracy: 0.4370\n",
      "Epoch 235/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2466 - accuracy: 0.56 - 1s 323ms/step - loss: 0.2439 - accuracy: 0.5625 - val_loss: 0.2614 - val_accuracy: 0.4380\n",
      "Epoch 236/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2537 - accuracy: 0.46 - 1s 365ms/step - loss: 0.2506 - accuracy: 0.5312 - val_loss: 0.2456 - val_accuracy: 0.4480\n",
      "Epoch 237/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2484 - accuracy: 0.50 - 1s 317ms/step - loss: 0.2505 - accuracy: 0.4844 - val_loss: 0.2916 - val_accuracy: 0.4472\n",
      "Epoch 238/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2524 - accuracy: 0.40 - 1s 340ms/step - loss: 0.2493 - accuracy: 0.4844 - val_loss: 0.2602 - val_accuracy: 0.4380\n",
      "Epoch 239/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2439 - accuracy: 0.46 - 1s 340ms/step - loss: 0.2476 - accuracy: 0.4844 - val_loss: 0.2460 - val_accuracy: 0.4375\n",
      "Epoch 240/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2404 - accuracy: 0.50 - 1s 289ms/step - loss: 0.2429 - accuracy: 0.5312 - val_loss: 0.2981 - val_accuracy: 0.4370\n",
      "Epoch 241/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2492 - accuracy: 0.53 - 1s 348ms/step - loss: 0.2490 - accuracy: 0.5625 - val_loss: 0.2631 - val_accuracy: 0.4380\n",
      "Epoch 242/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2495 - accuracy: 0.59 - 1s 295ms/step - loss: 0.2486 - accuracy: 0.5469 - val_loss: 0.2463 - val_accuracy: 0.4375\n",
      "Epoch 243/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2482 - accuracy: 0.53 - 1s 347ms/step - loss: 0.2483 - accuracy: 0.4844 - val_loss: 0.2936 - val_accuracy: 0.4370\n",
      "Epoch 244/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2580 - accuracy: 0.31 - 1s 332ms/step - loss: 0.2546 - accuracy: 0.4219 - val_loss: 0.2594 - val_accuracy: 0.4485\n",
      "Epoch 245/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2424 - accuracy: 0.65 - 1s 289ms/step - loss: 0.2454 - accuracy: 0.5625 - val_loss: 0.2460 - val_accuracy: 0.4375\n",
      "Epoch 246/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2460 - accuracy: 0.53 - 1s 350ms/step - loss: 0.2453 - accuracy: 0.5469 - val_loss: 0.2927 - val_accuracy: 0.4370\n",
      "Epoch 247/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2429 - accuracy: 0.65 - 1s 315ms/step - loss: 0.2437 - accuracy: 0.6250 - val_loss: 0.2638 - val_accuracy: 0.4380\n",
      "Epoch 248/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2511 - accuracy: 0.46 - 1s 347ms/step - loss: 0.2457 - accuracy: 0.5156 - val_loss: 0.2466 - val_accuracy: 0.4375\n",
      "Epoch 249/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2468 - accuracy: 0.53 - 1s 285ms/step - loss: 0.2491 - accuracy: 0.5312 - val_loss: 0.2975 - val_accuracy: 0.4370\n",
      "Epoch 250/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2494 - accuracy: 0.59 - 1s 323ms/step - loss: 0.2510 - accuracy: 0.5625 - val_loss: 0.2610 - val_accuracy: 0.4380\n"
     ]
    }
   ],
   "source": [
    "#Building and training LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, return_sequences=True,\n",
    "                    input_shape=(None, normalized_df1.shape[-1]),\n",
    "                    kernel_initializer='random_uniform'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(LSTM(60, dropout=0.0, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(20,activation='relu'))\n",
    "model.add(layers.Dense(2, activation='softmax'))\n",
    "model.compile(loss='mean_squared_error', optimizer=RMSprop(),metrics=['accuracy'])\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,\n",
    "                       patience=100, min_delta=0.0001, restore_best_weights = True)\n",
    "\n",
    "history = model.fit_generator(train_gen,\n",
    "                              steps_per_epoch=2,\n",
    "                              epochs=250,\n",
    "                              validation_data=val_gen,\n",
    "                              validation_steps=val_steps,\n",
    "                              callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test acc: 0.5833333134651184\n",
      "test_loss: 0.2430008053779602\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate_generator(test_gen, steps=3)\n",
    "print('test acc:', test_acc)\n",
    "print(\"test_loss:\", test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "2/2 [==============================] - ETA: 3s - loss: 0.2512 - accuracy: 0.43 - 5s 2s/step - loss: 0.2508 - accuracy: 0.4844 - val_loss: 0.2577 - val_accuracy: 0.4370\n",
      "Epoch 2/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2479 - accuracy: 0.53 - 1s 364ms/step - loss: 0.2493 - accuracy: 0.5312 - val_loss: 0.2516 - val_accuracy: 0.4380\n",
      "Epoch 3/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2534 - accuracy: 0.53 - 1s 371ms/step - loss: 0.2502 - accuracy: 0.5312 - val_loss: 0.2508 - val_accuracy: 0.4375\n",
      "Epoch 4/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2457 - accuracy: 0.50 - 1s 283ms/step - loss: 0.2489 - accuracy: 0.5156 - val_loss: 0.2564 - val_accuracy: 0.4370\n",
      "Epoch 5/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2496 - accuracy: 0.53 - 1s 494ms/step - loss: 0.2469 - accuracy: 0.5469 - val_loss: 0.2517 - val_accuracy: 0.4380\n",
      "Epoch 6/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2505 - accuracy: 0.43 - 1s 368ms/step - loss: 0.2509 - accuracy: 0.4844 - val_loss: 0.2506 - val_accuracy: 0.4375\n",
      "Epoch 7/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2545 - accuracy: 0.40 - 1s 301ms/step - loss: 0.2549 - accuracy: 0.4219 - val_loss: 0.2556 - val_accuracy: 0.4370\n",
      "Epoch 8/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2425 - accuracy: 0.53 - 1s 356ms/step - loss: 0.2462 - accuracy: 0.5000 - val_loss: 0.2510 - val_accuracy: 0.4273\n",
      "Epoch 9/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2473 - accuracy: 0.59 - 1s 357ms/step - loss: 0.2473 - accuracy: 0.5312 - val_loss: 0.2506 - val_accuracy: 0.4375\n",
      "Epoch 10/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2414 - accuracy: 0.75 - 1s 494ms/step - loss: 0.2446 - accuracy: 0.5938 - val_loss: 0.2546 - val_accuracy: 0.4370\n",
      "Epoch 11/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2506 - accuracy: 0.50 - 1s 482ms/step - loss: 0.2493 - accuracy: 0.5156 - val_loss: 0.2518 - val_accuracy: 0.4380\n",
      "Epoch 12/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2509 - accuracy: 0.56 - 1s 348ms/step - loss: 0.2519 - accuracy: 0.5000 - val_loss: 0.2511 - val_accuracy: 0.4375\n",
      "Epoch 13/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2543 - accuracy: 0.53 - 1s 282ms/step - loss: 0.2550 - accuracy: 0.4062 - val_loss: 0.2551 - val_accuracy: 0.4370\n",
      "Epoch 14/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2520 - accuracy: 0.50 - 1s 344ms/step - loss: 0.2549 - accuracy: 0.4844 - val_loss: 0.2522 - val_accuracy: 0.4380\n",
      "Epoch 15/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2554 - accuracy: 0.46 - 1s 288ms/step - loss: 0.2510 - accuracy: 0.5000 - val_loss: 0.2512 - val_accuracy: 0.4375\n",
      "Epoch 16/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2555 - accuracy: 0.46 - 1s 350ms/step - loss: 0.2496 - accuracy: 0.5312 - val_loss: 0.2551 - val_accuracy: 0.4370\n",
      "Epoch 17/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2469 - accuracy: 0.50 - 1s 289ms/step - loss: 0.2512 - accuracy: 0.4219 - val_loss: 0.2522 - val_accuracy: 0.4380\n",
      "Epoch 18/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2514 - accuracy: 0.50 - 1s 378ms/step - loss: 0.2480 - accuracy: 0.5000 - val_loss: 0.2512 - val_accuracy: 0.4375\n",
      "Epoch 19/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2493 - accuracy: 0.50 - 1s 349ms/step - loss: 0.2533 - accuracy: 0.5000 - val_loss: 0.2560 - val_accuracy: 0.4370\n",
      "Epoch 20/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2497 - accuracy: 0.59 - 1s 287ms/step - loss: 0.2516 - accuracy: 0.5000 - val_loss: 0.2523 - val_accuracy: 0.4380\n",
      "Epoch 21/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2387 - accuracy: 0.68 - 1s 346ms/step - loss: 0.2482 - accuracy: 0.5156 - val_loss: 0.2514 - val_accuracy: 0.4375\n",
      "Epoch 22/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2545 - accuracy: 0.46 - 1s 287ms/step - loss: 0.2574 - accuracy: 0.4219 - val_loss: 0.2551 - val_accuracy: 0.4370\n",
      "Epoch 23/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2487 - accuracy: 0.59 - 1s 350ms/step - loss: 0.2496 - accuracy: 0.5469 - val_loss: 0.2522 - val_accuracy: 0.4380\n",
      "Epoch 24/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2492 - accuracy: 0.40 - 1s 346ms/step - loss: 0.2519 - accuracy: 0.4531 - val_loss: 0.2511 - val_accuracy: 0.4375\n",
      "Epoch 25/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2423 - accuracy: 0.56 - 1s 400ms/step - loss: 0.2427 - accuracy: 0.5781 - val_loss: 0.2563 - val_accuracy: 0.4370\n",
      "Epoch 26/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2557 - accuracy: 0.50 - 1s 396ms/step - loss: 0.2553 - accuracy: 0.4688 - val_loss: 0.2516 - val_accuracy: 0.4483\n",
      "Epoch 27/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2494 - accuracy: 0.50 - 1s 350ms/step - loss: 0.2532 - accuracy: 0.4375 - val_loss: 0.2512 - val_accuracy: 0.4375\n",
      "Epoch 28/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2498 - accuracy: 0.46 - 1s 292ms/step - loss: 0.2510 - accuracy: 0.5000 - val_loss: 0.2584 - val_accuracy: 0.4370\n",
      "Epoch 29/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2544 - accuracy: 0.43 - 1s 391ms/step - loss: 0.2511 - accuracy: 0.5000 - val_loss: 0.2528 - val_accuracy: 0.4380\n",
      "Epoch 30/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2441 - accuracy: 0.65 - 1s 293ms/step - loss: 0.2494 - accuracy: 0.5625 - val_loss: 0.2512 - val_accuracy: 0.4375\n",
      "Epoch 31/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2481 - accuracy: 0.62 - 1s 380ms/step - loss: 0.2516 - accuracy: 0.5781 - val_loss: 0.2592 - val_accuracy: 0.4370\n",
      "Epoch 32/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2568 - accuracy: 0.46 - 1s 359ms/step - loss: 0.2552 - accuracy: 0.5000 - val_loss: 0.2524 - val_accuracy: 0.4380\n",
      "Epoch 33/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2538 - accuracy: 0.40 - 1s 305ms/step - loss: 0.2510 - accuracy: 0.4375 - val_loss: 0.2511 - val_accuracy: 0.4375\n",
      "Epoch 34/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2550 - accuracy: 0.46 - 1s 363ms/step - loss: 0.2534 - accuracy: 0.4688 - val_loss: 0.2589 - val_accuracy: 0.4370\n",
      "Epoch 35/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2486 - accuracy: 0.62 - 1s 296ms/step - loss: 0.2522 - accuracy: 0.5625 - val_loss: 0.2520 - val_accuracy: 0.4380\n",
      "Epoch 36/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2521 - accuracy: 0.46 - 1s 385ms/step - loss: 0.2513 - accuracy: 0.5000 - val_loss: 0.2512 - val_accuracy: 0.4375\n",
      "Epoch 37/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2563 - accuracy: 0.43 - 1s 405ms/step - loss: 0.2529 - accuracy: 0.5000 - val_loss: 0.2580 - val_accuracy: 0.4370\n",
      "Epoch 38/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2452 - accuracy: 0.46 - 1s 296ms/step - loss: 0.2444 - accuracy: 0.5469 - val_loss: 0.2522 - val_accuracy: 0.4380\n",
      "Epoch 39/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2557 - accuracy: 0.46 - 1s 346ms/step - loss: 0.2565 - accuracy: 0.4375 - val_loss: 0.2511 - val_accuracy: 0.4375\n",
      "Epoch 40/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2534 - accuracy: 0.46 - 1s 289ms/step - loss: 0.2524 - accuracy: 0.5000 - val_loss: 0.2583 - val_accuracy: 0.4370\n",
      "Epoch 41/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2404 - accuracy: 0.62 - 1s 344ms/step - loss: 0.2454 - accuracy: 0.6094 - val_loss: 0.2523 - val_accuracy: 0.4380\n",
      "Epoch 42/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2571 - accuracy: 0.40 - 1s 368ms/step - loss: 0.2517 - accuracy: 0.5000 - val_loss: 0.2511 - val_accuracy: 0.4375\n",
      "Epoch 43/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2454 - accuracy: 0.50 - 1s 271ms/step - loss: 0.2484 - accuracy: 0.5000 - val_loss: 0.2570 - val_accuracy: 0.4370\n",
      "Epoch 44/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2482 - accuracy: 0.50 - 1s 343ms/step - loss: 0.2485 - accuracy: 0.5312 - val_loss: 0.2522 - val_accuracy: 0.4380\n",
      "Epoch 45/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2513 - accuracy: 0.56 - 1s 293ms/step - loss: 0.2519 - accuracy: 0.5312 - val_loss: 0.2510 - val_accuracy: 0.4375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2572 - accuracy: 0.40 - 1s 379ms/step - loss: 0.2543 - accuracy: 0.3906 - val_loss: 0.2585 - val_accuracy: 0.4370\n",
      "Epoch 47/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2533 - accuracy: 0.50 - 1s 371ms/step - loss: 0.2507 - accuracy: 0.4688 - val_loss: 0.2525 - val_accuracy: 0.4380\n",
      "Epoch 48/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2447 - accuracy: 0.65 - 1s 308ms/step - loss: 0.2468 - accuracy: 0.6250 - val_loss: 0.2513 - val_accuracy: 0.4375\n",
      "Epoch 49/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2502 - accuracy: 0.53 - 1s 398ms/step - loss: 0.2501 - accuracy: 0.5000 - val_loss: 0.2576 - val_accuracy: 0.4370\n",
      "Epoch 50/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2526 - accuracy: 0.43 - 1s 393ms/step - loss: 0.2498 - accuracy: 0.5312 - val_loss: 0.2533 - val_accuracy: 0.4380\n",
      "Epoch 51/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2526 - accuracy: 0.59 - 1s 301ms/step - loss: 0.2525 - accuracy: 0.5156 - val_loss: 0.2514 - val_accuracy: 0.4375\n",
      "Epoch 52/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2550 - accuracy: 0.43 - 1s 348ms/step - loss: 0.2529 - accuracy: 0.4375 - val_loss: 0.2585 - val_accuracy: 0.4370\n",
      "Epoch 53/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2517 - accuracy: 0.43 - 1s 281ms/step - loss: 0.2526 - accuracy: 0.4688 - val_loss: 0.2537 - val_accuracy: 0.4380\n",
      "Epoch 54/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2475 - accuracy: 0.46 - 1s 356ms/step - loss: 0.2514 - accuracy: 0.4531 - val_loss: 0.2514 - val_accuracy: 0.4375\n",
      "Epoch 55/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2515 - accuracy: 0.56 - 1s 331ms/step - loss: 0.2502 - accuracy: 0.5469 - val_loss: 0.2579 - val_accuracy: 0.4370\n",
      "Epoch 56/250\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2498 - accuracy: 0.50 - 1s 275ms/step - loss: 0.2494 - accuracy: 0.5938 - val_loss: 0.2543 - val_accuracy: 0.4380\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00056: early stopping\n"
     ]
    }
   ],
   "source": [
    "#Building and training a model with GRU\n",
    "model = Sequential()\n",
    "model.add(layers.GRU(32,\n",
    "                     dropout=0.3,\n",
    "                     recurrent_dropout=0.2,\n",
    "                     return_sequences=True,\n",
    "                     input_shape=(None, normalized_df1.shape[-1])))\n",
    "model.add(layers.GRU(64, activation='relu',\n",
    "                     dropout=0.3,\n",
    "                     recurrent_dropout=0.1))\n",
    "model.add(layers.Dense(2, activation='softmax'))\n",
    "model.compile(optimizer=RMSprop(), loss='mean_squared_error', metrics=['accuracy'])\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,\n",
    "                       patience=50, min_delta=0.0001, restore_best_weights = True)\n",
    "    \n",
    "history = model.fit_generator(train_gen,\n",
    "                              steps_per_epoch=2,\n",
    "                              epochs=250,\n",
    "                              validation_data=val_gen,\n",
    "                              validation_steps=val_steps,\n",
    "                              callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test acc: 0.5833333134651184\n",
      "test_loss: 0.24848084151744843\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the model\n",
    "test_loss, test_acc = model.evaluate_generator(test_gen, steps=3)\n",
    "print('test acc:', test_acc)\n",
    "print(\"test_loss:\", test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Without sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalizing data\n",
    "normalized_df1 = normalized_df2\n",
    "\n",
    "mean = normalized_df1.mean(axis = 0)\n",
    "normalized_df1 -= mean\n",
    "std = normalized_df1.std(axis=0)\n",
    "normalized_df1 /= std\n",
    "\n",
    "#adding label: up/down or steady\n",
    "def add_label(df):\n",
    "    idx = len(df.columns)\n",
    "    new_col = np.where(df['Close'] >= df['Close'].shift(1), 1, 0)  \n",
    "    df.insert(loc=idx, column='Label', value=new_col)\n",
    "    df = df.fillna(0)\n",
    "    \n",
    "add_label(normalized_df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#applying function \n",
    "del normalized_df1['compound_mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>-1.233622</td>\n",
       "      <td>-1.220002</td>\n",
       "      <td>-1.207656</td>\n",
       "      <td>-1.197743</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>-1.181476</td>\n",
       "      <td>-1.169168</td>\n",
       "      <td>-1.167312</td>\n",
       "      <td>-1.153690</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>-1.176461</td>\n",
       "      <td>-1.159201</td>\n",
       "      <td>-1.152183</td>\n",
       "      <td>-1.138672</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>-1.168439</td>\n",
       "      <td>-1.177142</td>\n",
       "      <td>-1.154200</td>\n",
       "      <td>-1.167707</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>-1.175459</td>\n",
       "      <td>-1.178139</td>\n",
       "      <td>-1.169329</td>\n",
       "      <td>-1.176718</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>747</td>\n",
       "      <td>2.148868</td>\n",
       "      <td>2.190869</td>\n",
       "      <td>2.181270</td>\n",
       "      <td>2.195363</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>748</td>\n",
       "      <td>2.222073</td>\n",
       "      <td>2.177911</td>\n",
       "      <td>2.205476</td>\n",
       "      <td>2.168330</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>749</td>\n",
       "      <td>2.429656</td>\n",
       "      <td>2.472948</td>\n",
       "      <td>2.451577</td>\n",
       "      <td>2.471697</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>2.548990</td>\n",
       "      <td>2.510825</td>\n",
       "      <td>2.531257</td>\n",
       "      <td>2.527765</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>751</td>\n",
       "      <td>2.441689</td>\n",
       "      <td>2.418127</td>\n",
       "      <td>2.462672</td>\n",
       "      <td>2.443663</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>752 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Open      High       Low     Close  Label\n",
       "0   -1.233622 -1.220002 -1.207656 -1.197743      0\n",
       "1   -1.181476 -1.169168 -1.167312 -1.153690      1\n",
       "2   -1.176461 -1.159201 -1.152183 -1.138672      1\n",
       "3   -1.168439 -1.177142 -1.154200 -1.167707      0\n",
       "4   -1.175459 -1.178139 -1.169329 -1.176718      0\n",
       "..        ...       ...       ...       ...    ...\n",
       "747  2.148868  2.190869  2.181270  2.195363      0\n",
       "748  2.222073  2.177911  2.205476  2.168330      0\n",
       "749  2.429656  2.472948  2.451577  2.471697      1\n",
       "750  2.548990  2.510825  2.531257  2.527765      1\n",
       "751  2.441689  2.418127  2.462672  2.443663      0\n",
       "\n",
       "[752 rows x 5 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_df1 = normalized_df1.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting into tain validation and test set\n",
    "\n",
    "train_gen = generator(normalized_df1,\n",
    "                      lookback=lookback,\n",
    "                      delay=delay,\n",
    "                      min_index=0,\n",
    "                      max_index=round(0.6*len(normalized_df1)),\n",
    "                      shuffle=False,\n",
    "                      step=step,\n",
    "                      batch_size=batch_size)\n",
    "val_gen = generator(normalized_df1,\n",
    "                    lookback=lookback,\n",
    "                    delay=delay,\n",
    "                    min_index=round(0.6*len(normalized_df1))+1,\n",
    "                    max_index=round(0.8*len(normalized_df1)),\n",
    "                    step=step,\n",
    "                    batch_size=batch_size)\n",
    "test_gen = generator(normalized_df1,\n",
    "                     lookback=lookback,\n",
    "                     delay=delay,\n",
    "                     min_index=round(0.8*len(normalized_df1))+1,\n",
    "                     max_index=None,\n",
    "                     step=step,\n",
    "                     batch_size=batch_size)\n",
    "\n",
    "val_steps = (round(0.8*len(normalized_df1)) - round(0.6*len(normalized_df1))+1 - lookback) # how many steps to draw from val_gen in order to see the entire validation set\n",
    "test_steps = (len(normalized_df1) - round(0.8*len(normalized_df1))+1 - lookback)\n",
    "# How many steps to draw from test_gen in order to see the entire test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "2/2 [==============================] - ETA: 2s - loss: 0.2512 - accuracy: 0.37 - 3s 2s/step - loss: 0.2507 - accuracy: 0.4375 - val_loss: 0.2504 - val_accuracy: 0.4380\n",
      "Epoch 2/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2498 - accuracy: 0.50 - 0s 239ms/step - loss: 0.2507 - accuracy: 0.4375 - val_loss: 0.2500 - val_accuracy: 0.4270\n",
      "Epoch 3/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2497 - accuracy: 0.50 - 1s 313ms/step - loss: 0.2497 - accuracy: 0.4844 - val_loss: 0.2495 - val_accuracy: 0.5630\n",
      "Epoch 4/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2502 - accuracy: 0.53 - 1s 281ms/step - loss: 0.2507 - accuracy: 0.4844 - val_loss: 0.2501 - val_accuracy: 0.4585\n",
      "Epoch 5/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2496 - accuracy: 0.53 - 1s 333ms/step - loss: 0.2499 - accuracy: 0.4688 - val_loss: 0.2500 - val_accuracy: 0.4375\n",
      "Epoch 6/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2501 - accuracy: 0.53 - 0s 225ms/step - loss: 0.2501 - accuracy: 0.5156 - val_loss: 0.2506 - val_accuracy: 0.4370\n",
      "Epoch 7/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2499 - accuracy: 0.50 - 1s 274ms/step - loss: 0.2497 - accuracy: 0.5312 - val_loss: 0.2505 - val_accuracy: 0.4380\n",
      "Epoch 8/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2486 - accuracy: 0.56 - 0s 230ms/step - loss: 0.2493 - accuracy: 0.5312 - val_loss: 0.2501 - val_accuracy: 0.4375\n",
      "Epoch 9/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2527 - accuracy: 0.37 - 1s 276ms/step - loss: 0.2519 - accuracy: 0.3906 - val_loss: 0.2507 - val_accuracy: 0.4370\n",
      "Epoch 10/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2508 - accuracy: 0.40 - 1s 320ms/step - loss: 0.2503 - accuracy: 0.4844 - val_loss: 0.2503 - val_accuracy: 0.4380\n",
      "Epoch 11/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2490 - accuracy: 0.59 - 1s 439ms/step - loss: 0.2495 - accuracy: 0.5469 - val_loss: 0.2500 - val_accuracy: 0.4375\n",
      "Epoch 12/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2494 - accuracy: 0.53 - 1s 374ms/step - loss: 0.2494 - accuracy: 0.5469 - val_loss: 0.2512 - val_accuracy: 0.4370\n",
      "Epoch 13/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2497 - accuracy: 0.50 - 1s 316ms/step - loss: 0.2498 - accuracy: 0.5000 - val_loss: 0.2506 - val_accuracy: 0.4380\n",
      "Epoch 14/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2487 - accuracy: 0.59 - 1s 418ms/step - loss: 0.2485 - accuracy: 0.5781 - val_loss: 0.2501 - val_accuracy: 0.4375\n",
      "Epoch 15/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2502 - accuracy: 0.50 - 1s 395ms/step - loss: 0.2514 - accuracy: 0.4375 - val_loss: 0.2512 - val_accuracy: 0.4370\n",
      "Epoch 16/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2500 - accuracy: 0.50 - 1s 350ms/step - loss: 0.2509 - accuracy: 0.4531 - val_loss: 0.2504 - val_accuracy: 0.4380\n",
      "Epoch 17/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2500 - accuracy: 0.50 - 1s 468ms/step - loss: 0.2492 - accuracy: 0.5625 - val_loss: 0.2501 - val_accuracy: 0.4375\n",
      "Epoch 18/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2505 - accuracy: 0.50 - 1s 410ms/step - loss: 0.2498 - accuracy: 0.5156 - val_loss: 0.2513 - val_accuracy: 0.4370\n",
      "Epoch 19/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2485 - accuracy: 0.56 - 1s 467ms/step - loss: 0.2495 - accuracy: 0.5312 - val_loss: 0.2507 - val_accuracy: 0.4380\n",
      "Epoch 20/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2502 - accuracy: 0.50 - 1s 356ms/step - loss: 0.2487 - accuracy: 0.5469 - val_loss: 0.2501 - val_accuracy: 0.4375\n",
      "Epoch 21/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2483 - accuracy: 0.56 - 1s 414ms/step - loss: 0.2492 - accuracy: 0.5312 - val_loss: 0.2516 - val_accuracy: 0.4370\n",
      "Epoch 22/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2541 - accuracy: 0.37 - 1s 402ms/step - loss: 0.2522 - accuracy: 0.4375 - val_loss: 0.2506 - val_accuracy: 0.4380\n",
      "Epoch 23/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2511 - accuracy: 0.40 - 1s 356ms/step - loss: 0.2505 - accuracy: 0.4531 - val_loss: 0.2501 - val_accuracy: 0.4375\n",
      "Epoch 24/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2484 - accuracy: 0.59 - 1s 482ms/step - loss: 0.2497 - accuracy: 0.5469 - val_loss: 0.2512 - val_accuracy: 0.4370\n",
      "Epoch 25/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2491 - accuracy: 0.53 - 1s 491ms/step - loss: 0.2490 - accuracy: 0.5469 - val_loss: 0.2508 - val_accuracy: 0.4380\n",
      "Epoch 26/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2502 - accuracy: 0.50 - 1s 403ms/step - loss: 0.2501 - accuracy: 0.5000 - val_loss: 0.2500 - val_accuracy: 0.4375\n",
      "Epoch 27/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2481 - accuracy: 0.59 - 1s 330ms/step - loss: 0.2478 - accuracy: 0.5781 - val_loss: 0.2521 - val_accuracy: 0.4370\n",
      "Epoch 28/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2495 - accuracy: 0.50 - 1s 398ms/step - loss: 0.2519 - accuracy: 0.4375 - val_loss: 0.2508 - val_accuracy: 0.4380\n",
      "Epoch 29/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2506 - accuracy: 0.50 - 1s 387ms/step - loss: 0.2510 - accuracy: 0.4688 - val_loss: 0.2501 - val_accuracy: 0.4375\n",
      "Epoch 30/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2494 - accuracy: 0.53 - 1s 423ms/step - loss: 0.2476 - accuracy: 0.5781 - val_loss: 0.2516 - val_accuracy: 0.4370\n",
      "Epoch 31/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2500 - accuracy: 0.50 - 1s 329ms/step - loss: 0.2498 - accuracy: 0.5156 - val_loss: 0.2508 - val_accuracy: 0.4380\n",
      "Epoch 32/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2488 - accuracy: 0.56 - 1s 437ms/step - loss: 0.2497 - accuracy: 0.5312 - val_loss: 0.2501 - val_accuracy: 0.4375\n",
      "Epoch 33/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2497 - accuracy: 0.50 - 1s 386ms/step - loss: 0.2485 - accuracy: 0.5469 - val_loss: 0.2521 - val_accuracy: 0.4370\n",
      "Epoch 34/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2465 - accuracy: 0.56 - 1s 346ms/step - loss: 0.2487 - accuracy: 0.5312 - val_loss: 0.2511 - val_accuracy: 0.4380\n",
      "Epoch 35/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2547 - accuracy: 0.37 - 1s 420ms/step - loss: 0.2530 - accuracy: 0.4375 - val_loss: 0.2501 - val_accuracy: 0.4375\n",
      "Epoch 36/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2510 - accuracy: 0.40 - 1s 511ms/step - loss: 0.2503 - accuracy: 0.4531 - val_loss: 0.2513 - val_accuracy: 0.4370\n",
      "Epoch 37/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2466 - accuracy: 0.65 - 1s 460ms/step - loss: 0.2488 - accuracy: 0.5781 - val_loss: 0.2508 - val_accuracy: 0.4380\n",
      "Epoch 38/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2498 - accuracy: 0.53 - 1s 334ms/step - loss: 0.2488 - accuracy: 0.5469 - val_loss: 0.2501 - val_accuracy: 0.4375\n",
      "Epoch 39/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2516 - accuracy: 0.50 - 1s 382ms/step - loss: 0.2507 - accuracy: 0.5000 - val_loss: 0.2520 - val_accuracy: 0.4370\n",
      "Epoch 40/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2466 - accuracy: 0.59 - 1s 373ms/step - loss: 0.2459 - accuracy: 0.5781 - val_loss: 0.2511 - val_accuracy: 0.4380\n",
      "Epoch 41/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2496 - accuracy: 0.50 - 1s 326ms/step - loss: 0.2514 - accuracy: 0.4531 - val_loss: 0.2500 - val_accuracy: 0.4375\n",
      "Epoch 42/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2503 - accuracy: 0.50 - 1s 408ms/step - loss: 0.2515 - accuracy: 0.4531 - val_loss: 0.2511 - val_accuracy: 0.4370\n",
      "Epoch 43/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2506 - accuracy: 0.46 - 1s 465ms/step - loss: 0.2486 - accuracy: 0.5312 - val_loss: 0.2507 - val_accuracy: 0.4380\n",
      "Epoch 44/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2516 - accuracy: 0.50 - 1s 298ms/step - loss: 0.2503 - accuracy: 0.5156 - val_loss: 0.2501 - val_accuracy: 0.4375\n",
      "Epoch 45/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2490 - accuracy: 0.56 - 1s 353ms/step - loss: 0.2498 - accuracy: 0.5312 - val_loss: 0.2516 - val_accuracy: 0.4370\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2503 - accuracy: 0.50 - 1s 364ms/step - loss: 0.2465 - accuracy: 0.5469 - val_loss: 0.2508 - val_accuracy: 0.4380\n",
      "Epoch 47/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2468 - accuracy: 0.56 - 1s 302ms/step - loss: 0.2472 - accuracy: 0.5312 - val_loss: 0.2501 - val_accuracy: 0.4375\n",
      "Epoch 48/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2540 - accuracy: 0.37 - 1s 347ms/step - loss: 0.2523 - accuracy: 0.4688 - val_loss: 0.2513 - val_accuracy: 0.4370\n",
      "Epoch 49/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2530 - accuracy: 0.50 - 1s 284ms/step - loss: 0.2514 - accuracy: 0.5000 - val_loss: 0.2503 - val_accuracy: 0.4380\n",
      "Epoch 50/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2451 - accuracy: 0.68 - 1s 358ms/step - loss: 0.2479 - accuracy: 0.5938 - val_loss: 0.2500 - val_accuracy: 0.4375\n",
      "Epoch 51/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2482 - accuracy: 0.53 - 1s 341ms/step - loss: 0.2481 - accuracy: 0.5625 - val_loss: 0.2510 - val_accuracy: 0.4370\n",
      "Epoch 52/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2514 - accuracy: 0.50 - 1s 325ms/step - loss: 0.2510 - accuracy: 0.5000 - val_loss: 0.2506 - val_accuracy: 0.4380\n",
      "Epoch 53/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2480 - accuracy: 0.53 - 1s 351ms/step - loss: 0.2464 - accuracy: 0.5469 - val_loss: 0.2500 - val_accuracy: 0.4375\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00053: early stopping\n"
     ]
    }
   ],
   "source": [
    "#LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, return_sequences=True,\n",
    "                    input_shape=(None, normalized_df1.shape[-1]),\n",
    "                    kernel_initializer='random_uniform'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(LSTM(60, dropout=0.0, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(20,activation='relu'))\n",
    "model.add(layers.Dense(2, activation='softmax'))\n",
    "model.compile(loss='mean_squared_error', optimizer=RMSprop(),metrics=['accuracy'])\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,\n",
    "                       patience=50, min_delta=0.0001, restore_best_weights = True)\n",
    "\n",
    "history = model.fit_generator(train_gen,\n",
    "                              steps_per_epoch=2,\n",
    "                              epochs=200,\n",
    "                              validation_data=val_gen,\n",
    "                              validation_steps=val_steps,\n",
    "                              callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test acc: 0.40625\n",
      "test_loss: 0.25044193863868713\n"
     ]
    }
   ],
   "source": [
    "#evaluating the model\n",
    "test_loss, test_acc = model.evaluate_generator(test_gen, steps=4)\n",
    "print('test acc:', test_acc)\n",
    "print(\"test_loss:\", test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "2/2 [==============================] - ETA: 2s - loss: 0.2716 - accuracy: 0.37 - 4s 2s/step - loss: 0.2624 - accuracy: 0.4219 - val_loss: 0.2478 - val_accuracy: 0.5620\n",
      "Epoch 2/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2514 - accuracy: 0.46 - 1s 314ms/step - loss: 0.2524 - accuracy: 0.4375 - val_loss: 0.2502 - val_accuracy: 0.5625\n",
      "Epoch 3/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2517 - accuracy: 0.53 - 1s 267ms/step - loss: 0.2504 - accuracy: 0.5625 - val_loss: 0.2458 - val_accuracy: 0.5630\n",
      "Epoch 4/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2521 - accuracy: 0.50 - 1s 344ms/step - loss: 0.2574 - accuracy: 0.4531 - val_loss: 0.2477 - val_accuracy: 0.5620\n",
      "Epoch 5/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2531 - accuracy: 0.40 - 1s 287ms/step - loss: 0.2511 - accuracy: 0.4688 - val_loss: 0.2502 - val_accuracy: 0.5625\n",
      "Epoch 6/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2536 - accuracy: 0.37 - 1s 367ms/step - loss: 0.2507 - accuracy: 0.4375 - val_loss: 0.2451 - val_accuracy: 0.5630\n",
      "Epoch 7/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2520 - accuracy: 0.40 - 1s 344ms/step - loss: 0.2541 - accuracy: 0.4062 - val_loss: 0.2479 - val_accuracy: 0.5620\n",
      "Epoch 8/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2418 - accuracy: 0.62 - 1s 288ms/step - loss: 0.2536 - accuracy: 0.5469 - val_loss: 0.2500 - val_accuracy: 0.5625\n",
      "Epoch 9/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2642 - accuracy: 0.34 - 1s 342ms/step - loss: 0.2625 - accuracy: 0.4062 - val_loss: 0.2457 - val_accuracy: 0.5630\n",
      "Epoch 10/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2457 - accuracy: 0.56 - 1s 285ms/step - loss: 0.2509 - accuracy: 0.4375 - val_loss: 0.2476 - val_accuracy: 0.5620\n",
      "Epoch 11/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2629 - accuracy: 0.43 - 1s 343ms/step - loss: 0.2572 - accuracy: 0.5000 - val_loss: 0.2502 - val_accuracy: 0.5625\n",
      "Epoch 12/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2508 - accuracy: 0.43 - 1s 283ms/step - loss: 0.2478 - accuracy: 0.5312 - val_loss: 0.2450 - val_accuracy: 0.5630\n",
      "Epoch 13/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2480 - accuracy: 0.59 - 1s 354ms/step - loss: 0.2494 - accuracy: 0.5000 - val_loss: 0.2476 - val_accuracy: 0.5620\n",
      "Epoch 14/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2485 - accuracy: 0.53 - 1s 342ms/step - loss: 0.2450 - accuracy: 0.5469 - val_loss: 0.2501 - val_accuracy: 0.5625\n",
      "Epoch 15/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2477 - accuracy: 0.50 - 1s 286ms/step - loss: 0.2571 - accuracy: 0.4375 - val_loss: 0.2450 - val_accuracy: 0.5630\n",
      "Epoch 16/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2508 - accuracy: 0.50 - 1s 346ms/step - loss: 0.2511 - accuracy: 0.5156 - val_loss: 0.2478 - val_accuracy: 0.5620\n",
      "Epoch 17/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2555 - accuracy: 0.40 - 1s 282ms/step - loss: 0.2605 - accuracy: 0.3750 - val_loss: 0.2503 - val_accuracy: 0.5625\n",
      "Epoch 18/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2589 - accuracy: 0.50 - 1s 350ms/step - loss: 0.2525 - accuracy: 0.5469 - val_loss: 0.2450 - val_accuracy: 0.5630\n",
      "Epoch 19/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2488 - accuracy: 0.50 - 1s 286ms/step - loss: 0.2485 - accuracy: 0.5469 - val_loss: 0.2477 - val_accuracy: 0.5620\n",
      "Epoch 20/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2482 - accuracy: 0.56 - 1s 337ms/step - loss: 0.2453 - accuracy: 0.5781 - val_loss: 0.2502 - val_accuracy: 0.5625\n",
      "Epoch 21/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2411 - accuracy: 0.59 - 1s 354ms/step - loss: 0.2480 - accuracy: 0.5469 - val_loss: 0.2454 - val_accuracy: 0.5630\n",
      "Epoch 22/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2566 - accuracy: 0.40 - 1s 287ms/step - loss: 0.2551 - accuracy: 0.4531 - val_loss: 0.2479 - val_accuracy: 0.5620\n",
      "Epoch 23/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2565 - accuracy: 0.34 - 1s 339ms/step - loss: 0.2522 - accuracy: 0.4375 - val_loss: 0.2503 - val_accuracy: 0.5625\n",
      "Epoch 24/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2526 - accuracy: 0.43 - 1s 283ms/step - loss: 0.2522 - accuracy: 0.5312 - val_loss: 0.2451 - val_accuracy: 0.5630\n",
      "Epoch 25/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2492 - accuracy: 0.53 - 1s 349ms/step - loss: 0.2505 - accuracy: 0.4844 - val_loss: 0.2479 - val_accuracy: 0.5620\n",
      "Epoch 26/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2507 - accuracy: 0.46 - 1s 282ms/step - loss: 0.2501 - accuracy: 0.4844 - val_loss: 0.2501 - val_accuracy: 0.5625\n",
      "Epoch 27/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2551 - accuracy: 0.50 - 1s 338ms/step - loss: 0.2570 - accuracy: 0.4844 - val_loss: 0.2460 - val_accuracy: 0.5630\n",
      "Epoch 28/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2445 - accuracy: 0.46 - 1s 335ms/step - loss: 0.2514 - accuracy: 0.3906 - val_loss: 0.2481 - val_accuracy: 0.5620\n",
      "Epoch 29/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2494 - accuracy: 0.53 - 1s 285ms/step - loss: 0.2511 - accuracy: 0.5156 - val_loss: 0.2503 - val_accuracy: 0.5625\n",
      "Epoch 30/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2502 - accuracy: 0.50 - 1s 340ms/step - loss: 0.2505 - accuracy: 0.5000 - val_loss: 0.2449 - val_accuracy: 0.5630\n",
      "Epoch 31/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2455 - accuracy: 0.59 - 1s 283ms/step - loss: 0.2488 - accuracy: 0.4844 - val_loss: 0.2478 - val_accuracy: 0.5620\n",
      "Epoch 32/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2440 - accuracy: 0.59 - 1s 351ms/step - loss: 0.2471 - accuracy: 0.5625 - val_loss: 0.2503 - val_accuracy: 0.5625\n",
      "Epoch 33/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2513 - accuracy: 0.46 - 1s 282ms/step - loss: 0.2468 - accuracy: 0.5312 - val_loss: 0.2448 - val_accuracy: 0.5630\n",
      "Epoch 34/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2486 - accuracy: 0.50 - 1s 345ms/step - loss: 0.2501 - accuracy: 0.4844 - val_loss: 0.2479 - val_accuracy: 0.5620\n",
      "Epoch 35/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2568 - accuracy: 0.40 - 1s 339ms/step - loss: 0.2541 - accuracy: 0.4844 - val_loss: 0.2505 - val_accuracy: 0.5625\n",
      "Epoch 36/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2467 - accuracy: 0.59 - 1s 287ms/step - loss: 0.2461 - accuracy: 0.5312 - val_loss: 0.2452 - val_accuracy: 0.5630\n",
      "Epoch 37/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2404 - accuracy: 0.68 - 1s 382ms/step - loss: 0.2472 - accuracy: 0.6094 - val_loss: 0.2480 - val_accuracy: 0.5620\n",
      "Epoch 38/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2548 - accuracy: 0.43 - 1s 272ms/step - loss: 0.2496 - accuracy: 0.5156 - val_loss: 0.2504 - val_accuracy: 0.5625\n",
      "Epoch 39/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2500 - accuracy: 0.53 - 1s 325ms/step - loss: 0.2494 - accuracy: 0.5156 - val_loss: 0.2451 - val_accuracy: 0.5630\n",
      "Epoch 40/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2454 - accuracy: 0.53 - 1s 257ms/step - loss: 0.2436 - accuracy: 0.6094 - val_loss: 0.2478 - val_accuracy: 0.5620\n",
      "Epoch 41/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2505 - accuracy: 0.56 - 1s 329ms/step - loss: 0.2545 - accuracy: 0.4844 - val_loss: 0.2506 - val_accuracy: 0.5625\n",
      "Epoch 42/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2411 - accuracy: 0.59 - 1s 256ms/step - loss: 0.2461 - accuracy: 0.5312 - val_loss: 0.2447 - val_accuracy: 0.5630\n",
      "Epoch 43/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2432 - accuracy: 0.56 - 1s 357ms/step - loss: 0.2480 - accuracy: 0.5000 - val_loss: 0.2478 - val_accuracy: 0.5620\n",
      "Epoch 44/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2535 - accuracy: 0.46 - 1s 315ms/step - loss: 0.2529 - accuracy: 0.4531 - val_loss: 0.2505 - val_accuracy: 0.5625\n",
      "Epoch 45/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2496 - accuracy: 0.43 - 1s 269ms/step - loss: 0.2485 - accuracy: 0.5000 - val_loss: 0.2443 - val_accuracy: 0.5630\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2484 - accuracy: 0.43 - 1s 334ms/step - loss: 0.2413 - accuracy: 0.5312 - val_loss: 0.2476 - val_accuracy: 0.5620\n",
      "Epoch 47/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2579 - accuracy: 0.40 - 1s 261ms/step - loss: 0.2551 - accuracy: 0.4375 - val_loss: 0.2504 - val_accuracy: 0.5625\n",
      "Epoch 48/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2544 - accuracy: 0.37 - 1s 313ms/step - loss: 0.2514 - accuracy: 0.4375 - val_loss: 0.2442 - val_accuracy: 0.5630\n",
      "Epoch 49/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2530 - accuracy: 0.46 - 1s 257ms/step - loss: 0.2499 - accuracy: 0.5625 - val_loss: 0.2478 - val_accuracy: 0.5620\n",
      "Epoch 50/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2570 - accuracy: 0.53 - 1s 351ms/step - loss: 0.2568 - accuracy: 0.5156 - val_loss: 0.2508 - val_accuracy: 0.5625\n",
      "Epoch 51/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2540 - accuracy: 0.34 - 1s 307ms/step - loss: 0.2525 - accuracy: 0.4219 - val_loss: 0.2444 - val_accuracy: 0.5630\n",
      "Epoch 52/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2478 - accuracy: 0.59 - 1s 371ms/step - loss: 0.2511 - accuracy: 0.4531 - val_loss: 0.2478 - val_accuracy: 0.5620\n",
      "Epoch 53/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2387 - accuracy: 0.65 - 1s 330ms/step - loss: 0.2433 - accuracy: 0.6094 - val_loss: 0.2504 - val_accuracy: 0.5625\n",
      "Epoch 54/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2468 - accuracy: 0.50 - 1s 273ms/step - loss: 0.2525 - accuracy: 0.4844 - val_loss: 0.2445 - val_accuracy: 0.5630\n",
      "Epoch 55/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2468 - accuracy: 0.53 - 1s 316ms/step - loss: 0.2585 - accuracy: 0.4062 - val_loss: 0.2480 - val_accuracy: 0.5620\n",
      "Epoch 56/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2408 - accuracy: 0.68 - 1s 262ms/step - loss: 0.2430 - accuracy: 0.6250 - val_loss: 0.2507 - val_accuracy: 0.5625\n",
      "Epoch 57/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2519 - accuracy: 0.43 - 1s 317ms/step - loss: 0.2524 - accuracy: 0.4531 - val_loss: 0.2443 - val_accuracy: 0.5630\n",
      "Epoch 58/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2472 - accuracy: 0.50 - 1s 271ms/step - loss: 0.2483 - accuracy: 0.4844 - val_loss: 0.2479 - val_accuracy: 0.5620\n",
      "Epoch 59/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2539 - accuracy: 0.40 - 1s 325ms/step - loss: 0.2548 - accuracy: 0.4844 - val_loss: 0.2504 - val_accuracy: 0.5625\n",
      "Epoch 60/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2421 - accuracy: 0.46 - 1s 255ms/step - loss: 0.2419 - accuracy: 0.4688 - val_loss: 0.2448 - val_accuracy: 0.5630\n",
      "Epoch 61/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2575 - accuracy: 0.37 - 1s 357ms/step - loss: 0.2550 - accuracy: 0.4219 - val_loss: 0.2479 - val_accuracy: 0.5620\n",
      "Epoch 62/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2442 - accuracy: 0.65 - 1s 307ms/step - loss: 0.2473 - accuracy: 0.5781 - val_loss: 0.2506 - val_accuracy: 0.5625\n",
      "Epoch 63/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2456 - accuracy: 0.59 - 1s 274ms/step - loss: 0.2457 - accuracy: 0.5625 - val_loss: 0.2444 - val_accuracy: 0.5630\n",
      "Epoch 64/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2482 - accuracy: 0.59 - 1s 320ms/step - loss: 0.2501 - accuracy: 0.5156 - val_loss: 0.2477 - val_accuracy: 0.5620\n",
      "Epoch 65/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2559 - accuracy: 0.37 - 1s 267ms/step - loss: 0.2534 - accuracy: 0.4219 - val_loss: 0.2503 - val_accuracy: 0.5625\n",
      "Epoch 66/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2445 - accuracy: 0.53 - 1s 311ms/step - loss: 0.2360 - accuracy: 0.6250 - val_loss: 0.2451 - val_accuracy: 0.5630\n",
      "Epoch 67/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2545 - accuracy: 0.31 - 1s 269ms/step - loss: 0.2505 - accuracy: 0.4531 - val_loss: 0.2476 - val_accuracy: 0.5620\n",
      "Epoch 68/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2452 - accuracy: 0.56 - 1s 313ms/step - loss: 0.2535 - accuracy: 0.4688 - val_loss: 0.2502 - val_accuracy: 0.5625\n",
      "Epoch 69/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2535 - accuracy: 0.53 - 1s 265ms/step - loss: 0.2483 - accuracy: 0.5469 - val_loss: 0.2451 - val_accuracy: 0.5630\n",
      "Epoch 70/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2431 - accuracy: 0.65 - 1s 318ms/step - loss: 0.2485 - accuracy: 0.5312 - val_loss: 0.2477 - val_accuracy: 0.5620\n",
      "Epoch 71/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2483 - accuracy: 0.53 - 1s 263ms/step - loss: 0.2477 - accuracy: 0.5469 - val_loss: 0.2500 - val_accuracy: 0.5625\n",
      "Epoch 72/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2482 - accuracy: 0.50 - 1s 317ms/step - loss: 0.2430 - accuracy: 0.5469 - val_loss: 0.2454 - val_accuracy: 0.5630\n",
      "Epoch 73/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2482 - accuracy: 0.59 - 1s 255ms/step - loss: 0.2467 - accuracy: 0.5938 - val_loss: 0.2476 - val_accuracy: 0.5620\n",
      "Epoch 74/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2640 - accuracy: 0.40 - 1s 322ms/step - loss: 0.2538 - accuracy: 0.5000 - val_loss: 0.2500 - val_accuracy: 0.5625\n",
      "Epoch 75/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2604 - accuracy: 0.46 - 1s 255ms/step - loss: 0.2575 - accuracy: 0.4688 - val_loss: 0.2447 - val_accuracy: 0.5630\n",
      "Epoch 76/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2433 - accuracy: 0.56 - 1s 323ms/step - loss: 0.2445 - accuracy: 0.6406 - val_loss: 0.2476 - val_accuracy: 0.5620\n",
      "Epoch 77/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2496 - accuracy: 0.50 - 1s 388ms/step - loss: 0.2509 - accuracy: 0.4844 - val_loss: 0.2500 - val_accuracy: 0.5625\n",
      "Epoch 78/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2532 - accuracy: 0.43 - 1s 333ms/step - loss: 0.2522 - accuracy: 0.3906 - val_loss: 0.2453 - val_accuracy: 0.5630\n",
      "Epoch 79/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2427 - accuracy: 0.56 - 1s 416ms/step - loss: 0.2468 - accuracy: 0.5312 - val_loss: 0.2476 - val_accuracy: 0.5620\n",
      "Epoch 80/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2507 - accuracy: 0.53 - 1s 385ms/step - loss: 0.2527 - accuracy: 0.4688 - val_loss: 0.2501 - val_accuracy: 0.5625\n",
      "Epoch 81/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2497 - accuracy: 0.53 - 1s 359ms/step - loss: 0.2557 - accuracy: 0.4375 - val_loss: 0.2447 - val_accuracy: 0.5630\n",
      "Epoch 82/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2565 - accuracy: 0.46 - 1s 371ms/step - loss: 0.2492 - accuracy: 0.5469 - val_loss: 0.2473 - val_accuracy: 0.5620\n",
      "Epoch 83/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2512 - accuracy: 0.53 - 1s 334ms/step - loss: 0.2489 - accuracy: 0.5625 - val_loss: 0.2500 - val_accuracy: 0.5625\n",
      "Epoch 84/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2479 - accuracy: 0.46 - 1s 281ms/step - loss: 0.2487 - accuracy: 0.5000 - val_loss: 0.2444 - val_accuracy: 0.5630\n",
      "Epoch 85/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2484 - accuracy: 0.59 - 1s 332ms/step - loss: 0.2429 - accuracy: 0.5938 - val_loss: 0.2473 - val_accuracy: 0.5620\n",
      "Epoch 86/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2383 - accuracy: 0.59 - 1s 364ms/step - loss: 0.2433 - accuracy: 0.5312 - val_loss: 0.2499 - val_accuracy: 0.5625\n",
      "Epoch 87/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2572 - accuracy: 0.40 - 1s 453ms/step - loss: 0.2551 - accuracy: 0.4688 - val_loss: 0.2447 - val_accuracy: 0.5630\n",
      "Epoch 88/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2550 - accuracy: 0.50 - 1s 363ms/step - loss: 0.2556 - accuracy: 0.4688 - val_loss: 0.2473 - val_accuracy: 0.5620\n",
      "Epoch 89/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2605 - accuracy: 0.37 - 1s 312ms/step - loss: 0.2561 - accuracy: 0.4531 - val_loss: 0.2500 - val_accuracy: 0.5625\n",
      "Epoch 90/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2527 - accuracy: 0.46 - 1s 373ms/step - loss: 0.2501 - accuracy: 0.5000 - val_loss: 0.2451 - val_accuracy: 0.5630\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 91/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2508 - accuracy: 0.53 - 1s 356ms/step - loss: 0.2485 - accuracy: 0.5312 - val_loss: 0.2476 - val_accuracy: 0.5620\n",
      "Epoch 92/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2438 - accuracy: 0.50 - 1s 314ms/step - loss: 0.2477 - accuracy: 0.4844 - val_loss: 0.2499 - val_accuracy: 0.5625\n",
      "Epoch 93/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2507 - accuracy: 0.56 - 1s 365ms/step - loss: 0.2537 - accuracy: 0.5000 - val_loss: 0.2445 - val_accuracy: 0.5630\n",
      "Epoch 94/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2505 - accuracy: 0.59 - 1s 301ms/step - loss: 0.2522 - accuracy: 0.5000 - val_loss: 0.2474 - val_accuracy: 0.5620\n",
      "Epoch 95/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2567 - accuracy: 0.50 - 1s 322ms/step - loss: 0.2513 - accuracy: 0.5156 - val_loss: 0.2499 - val_accuracy: 0.5625\n",
      "Epoch 96/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2508 - accuracy: 0.50 - 1s 305ms/step - loss: 0.2503 - accuracy: 0.5312 - val_loss: 0.2451 - val_accuracy: 0.5630\n",
      "Epoch 97/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2513 - accuracy: 0.50 - 1s 278ms/step - loss: 0.2511 - accuracy: 0.4688 - val_loss: 0.2477 - val_accuracy: 0.5620\n",
      "Epoch 98/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2526 - accuracy: 0.46 - 1s 322ms/step - loss: 0.2451 - accuracy: 0.5312 - val_loss: 0.2497 - val_accuracy: 0.5520\n",
      "Epoch 99/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2480 - accuracy: 0.53 - 1s 263ms/step - loss: 0.2499 - accuracy: 0.5312 - val_loss: 0.2453 - val_accuracy: 0.5630\n",
      "Epoch 100/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2577 - accuracy: 0.43 - 1s 305ms/step - loss: 0.2561 - accuracy: 0.4688 - val_loss: 0.2475 - val_accuracy: 0.5620\n",
      "Epoch 101/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2525 - accuracy: 0.40 - 1s 265ms/step - loss: 0.2487 - accuracy: 0.5156 - val_loss: 0.2498 - val_accuracy: 0.5625\n",
      "Epoch 102/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2448 - accuracy: 0.56 - 1s 366ms/step - loss: 0.2441 - accuracy: 0.5938 - val_loss: 0.2444 - val_accuracy: 0.5630\n",
      "Epoch 103/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2495 - accuracy: 0.50 - 1s 274ms/step - loss: 0.2493 - accuracy: 0.4844 - val_loss: 0.2473 - val_accuracy: 0.5620\n",
      "Epoch 104/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2508 - accuracy: 0.53 - 1s 299ms/step - loss: 0.2483 - accuracy: 0.5312 - val_loss: 0.2497 - val_accuracy: 0.5625\n",
      "Epoch 105/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2493 - accuracy: 0.59 - 1s 275ms/step - loss: 0.2536 - accuracy: 0.5625 - val_loss: 0.2443 - val_accuracy: 0.5630\n",
      "Epoch 106/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2602 - accuracy: 0.53 - 1s 338ms/step - loss: 0.2616 - accuracy: 0.4688 - val_loss: 0.2472 - val_accuracy: 0.5620\n",
      "Epoch 107/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2484 - accuracy: 0.53 - 1s 276ms/step - loss: 0.2525 - accuracy: 0.4688 - val_loss: 0.2499 - val_accuracy: 0.5625\n",
      "Epoch 108/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2378 - accuracy: 0.53 - 1s 355ms/step - loss: 0.2430 - accuracy: 0.5000 - val_loss: 0.2439 - val_accuracy: 0.5630\n",
      "Epoch 109/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2468 - accuracy: 0.46 - 1s 346ms/step - loss: 0.2480 - accuracy: 0.5000 - val_loss: 0.2470 - val_accuracy: 0.5620\n",
      "Epoch 110/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2466 - accuracy: 0.59 - 1s 291ms/step - loss: 0.2477 - accuracy: 0.5781 - val_loss: 0.2498 - val_accuracy: 0.5625\n",
      "Epoch 111/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2485 - accuracy: 0.56 - 1s 304ms/step - loss: 0.2473 - accuracy: 0.5781 - val_loss: 0.2441 - val_accuracy: 0.5630\n",
      "Epoch 112/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2566 - accuracy: 0.50 - 1s 268ms/step - loss: 0.2526 - accuracy: 0.4844 - val_loss: 0.2474 - val_accuracy: 0.5620\n",
      "Epoch 113/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2519 - accuracy: 0.50 - 1s 309ms/step - loss: 0.2517 - accuracy: 0.4844 - val_loss: 0.2498 - val_accuracy: 0.5625\n",
      "Epoch 114/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2538 - accuracy: 0.46 - 1s 283ms/step - loss: 0.2497 - accuracy: 0.4688 - val_loss: 0.2443 - val_accuracy: 0.5630\n",
      "Epoch 115/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2588 - accuracy: 0.37 - 1s 337ms/step - loss: 0.2539 - accuracy: 0.4688 - val_loss: 0.2470 - val_accuracy: 0.5620\n",
      "Epoch 116/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2512 - accuracy: 0.40 - 1s 344ms/step - loss: 0.2531 - accuracy: 0.4219 - val_loss: 0.2497 - val_accuracy: 0.5625\n",
      "Epoch 117/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2482 - accuracy: 0.43 - 1s 310ms/step - loss: 0.2502 - accuracy: 0.3906 - val_loss: 0.2443 - val_accuracy: 0.5630\n",
      "Epoch 118/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2522 - accuracy: 0.50 - 1s 315ms/step - loss: 0.2470 - accuracy: 0.5156 - val_loss: 0.2472 - val_accuracy: 0.5620\n",
      "Epoch 119/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2553 - accuracy: 0.50 - 1s 286ms/step - loss: 0.2537 - accuracy: 0.4844 - val_loss: 0.2499 - val_accuracy: 0.5625\n",
      "Epoch 120/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2454 - accuracy: 0.50 - 1s 303ms/step - loss: 0.2475 - accuracy: 0.5312 - val_loss: 0.2439 - val_accuracy: 0.5630\n",
      "Epoch 121/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2579 - accuracy: 0.31 - 1s 267ms/step - loss: 0.2544 - accuracy: 0.4688 - val_loss: 0.2471 - val_accuracy: 0.5620\n",
      "Epoch 122/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2471 - accuracy: 0.53 - 1s 318ms/step - loss: 0.2466 - accuracy: 0.5781 - val_loss: 0.2499 - val_accuracy: 0.5625\n",
      "Epoch 123/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2518 - accuracy: 0.43 - 1s 273ms/step - loss: 0.2505 - accuracy: 0.4688 - val_loss: 0.2440 - val_accuracy: 0.5630\n",
      "Epoch 124/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2518 - accuracy: 0.43 - 1s 301ms/step - loss: 0.2469 - accuracy: 0.5000 - val_loss: 0.2472 - val_accuracy: 0.5620\n",
      "Epoch 125/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2403 - accuracy: 0.56 - 1s 268ms/step - loss: 0.2440 - accuracy: 0.5625 - val_loss: 0.2500 - val_accuracy: 0.5625\n",
      "Epoch 126/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2644 - accuracy: 0.31 - 1s 354ms/step - loss: 0.2585 - accuracy: 0.3438 - val_loss: 0.2433 - val_accuracy: 0.5630\n",
      "Epoch 127/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2543 - accuracy: 0.40 - 1s 329ms/step - loss: 0.2584 - accuracy: 0.3594 - val_loss: 0.2471 - val_accuracy: 0.5620\n",
      "Epoch 128/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2489 - accuracy: 0.46 - 1s 308ms/step - loss: 0.2526 - accuracy: 0.4531 - val_loss: 0.2500 - val_accuracy: 0.5625\n",
      "Epoch 129/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2483 - accuracy: 0.46 - 1s 332ms/step - loss: 0.2484 - accuracy: 0.5312 - val_loss: 0.2439 - val_accuracy: 0.5630\n",
      "Epoch 130/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2498 - accuracy: 0.50 - 1s 280ms/step - loss: 0.2506 - accuracy: 0.4375 - val_loss: 0.2472 - val_accuracy: 0.5620\n",
      "Epoch 131/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2528 - accuracy: 0.50 - 1s 304ms/step - loss: 0.2523 - accuracy: 0.5312 - val_loss: 0.2498 - val_accuracy: 0.5625\n",
      "Epoch 132/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2538 - accuracy: 0.50 - 1s 266ms/step - loss: 0.2556 - accuracy: 0.4531 - val_loss: 0.2430 - val_accuracy: 0.5630\n",
      "Epoch 133/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2470 - accuracy: 0.56 - 1s 312ms/step - loss: 0.2480 - accuracy: 0.5469 - val_loss: 0.2468 - val_accuracy: 0.5620\n",
      "Epoch 134/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2460 - accuracy: 0.50 - 1s 286ms/step - loss: 0.2495 - accuracy: 0.4375 - val_loss: 0.2499 - val_accuracy: 0.5625\n",
      "Epoch 135/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2455 - accuracy: 0.62 - 1s 311ms/step - loss: 0.2472 - accuracy: 0.5625 - val_loss: 0.2426 - val_accuracy: 0.5630\n",
      "Epoch 136/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2490 - accuracy: 0.56 - 1s 320ms/step - loss: 0.2495 - accuracy: 0.4844 - val_loss: 0.2468 - val_accuracy: 0.5620\n",
      "Epoch 137/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2501 - accuracy: 0.59 - 1s 262ms/step - loss: 0.2476 - accuracy: 0.5469 - val_loss: 0.2498 - val_accuracy: 0.5625\n",
      "Epoch 138/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2522 - accuracy: 0.46 - 1s 392ms/step - loss: 0.2524 - accuracy: 0.5156 - val_loss: 0.2428 - val_accuracy: 0.5630\n",
      "Epoch 139/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2572 - accuracy: 0.40 - 1s 389ms/step - loss: 0.2556 - accuracy: 0.4219 - val_loss: 0.2468 - val_accuracy: 0.5620\n",
      "Epoch 140/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2512 - accuracy: 0.56 - 1s 362ms/step - loss: 0.2525 - accuracy: 0.5156 - val_loss: 0.2500 - val_accuracy: 0.5625\n",
      "Epoch 141/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2474 - accuracy: 0.53 - 1s 327ms/step - loss: 0.2476 - accuracy: 0.5781 - val_loss: 0.2426 - val_accuracy: 0.5630\n",
      "Epoch 142/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2479 - accuracy: 0.50 - 1s 278ms/step - loss: 0.2467 - accuracy: 0.5312 - val_loss: 0.2468 - val_accuracy: 0.5620\n",
      "Epoch 143/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2506 - accuracy: 0.50 - 1s 321ms/step - loss: 0.2502 - accuracy: 0.4844 - val_loss: 0.2500 - val_accuracy: 0.5625\n",
      "Epoch 144/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2502 - accuracy: 0.56 - 1s 300ms/step - loss: 0.2457 - accuracy: 0.6094 - val_loss: 0.2436 - val_accuracy: 0.5630\n",
      "Epoch 145/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2454 - accuracy: 0.43 - 1s 445ms/step - loss: 0.2524 - accuracy: 0.4219 - val_loss: 0.2470 - val_accuracy: 0.5620\n",
      "Epoch 146/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2539 - accuracy: 0.46 - 1s 328ms/step - loss: 0.2539 - accuracy: 0.4531 - val_loss: 0.2499 - val_accuracy: 0.5625\n",
      "Epoch 147/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2519 - accuracy: 0.46 - 1s 285ms/step - loss: 0.2463 - accuracy: 0.5625 - val_loss: 0.2435 - val_accuracy: 0.5630\n",
      "Epoch 148/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2461 - accuracy: 0.62 - 1s 436ms/step - loss: 0.2500 - accuracy: 0.5625 - val_loss: 0.2471 - val_accuracy: 0.5620\n",
      "Epoch 149/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2450 - accuracy: 0.53 - 1s 311ms/step - loss: 0.2498 - accuracy: 0.4219 - val_loss: 0.2498 - val_accuracy: 0.5625\n",
      "Epoch 150/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2488 - accuracy: 0.53 - 1s 285ms/step - loss: 0.2445 - accuracy: 0.5469 - val_loss: 0.2433 - val_accuracy: 0.5630\n",
      "Epoch 151/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2450 - accuracy: 0.53 - 1s 326ms/step - loss: 0.2456 - accuracy: 0.5156 - val_loss: 0.2470 - val_accuracy: 0.5620\n",
      "Epoch 152/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2532 - accuracy: 0.46 - 1s 290ms/step - loss: 0.2560 - accuracy: 0.4375 - val_loss: 0.2499 - val_accuracy: 0.5625\n",
      "Epoch 153/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2608 - accuracy: 0.37 - 1s 319ms/step - loss: 0.2533 - accuracy: 0.4688 - val_loss: 0.2433 - val_accuracy: 0.5630\n",
      "Epoch 154/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2426 - accuracy: 0.65 - 1s 295ms/step - loss: 0.2461 - accuracy: 0.5469 - val_loss: 0.2469 - val_accuracy: 0.5620\n",
      "Epoch 155/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2465 - accuracy: 0.65 - 1s 370ms/step - loss: 0.2472 - accuracy: 0.6094 - val_loss: 0.2499 - val_accuracy: 0.5625\n",
      "Epoch 156/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2522 - accuracy: 0.50 - 1s 410ms/step - loss: 0.2500 - accuracy: 0.5156 - val_loss: 0.2430 - val_accuracy: 0.5630\n",
      "Epoch 157/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2548 - accuracy: 0.62 - 1s 459ms/step - loss: 0.2483 - accuracy: 0.6094 - val_loss: 0.2468 - val_accuracy: 0.5620\n",
      "Epoch 158/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2464 - accuracy: 0.50 - 1s 405ms/step - loss: 0.2533 - accuracy: 0.4688 - val_loss: 0.2500 - val_accuracy: 0.5625\n",
      "Epoch 159/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2438 - accuracy: 0.56 - 1s 526ms/step - loss: 0.2512 - accuracy: 0.4688 - val_loss: 0.2424 - val_accuracy: 0.5630\n",
      "Epoch 160/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2487 - accuracy: 0.53 - 1s 356ms/step - loss: 0.2516 - accuracy: 0.5469 - val_loss: 0.2468 - val_accuracy: 0.5620\n",
      "Epoch 161/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2477 - accuracy: 0.62 - 1s 295ms/step - loss: 0.2518 - accuracy: 0.5156 - val_loss: 0.2499 - val_accuracy: 0.5625\n",
      "Epoch 162/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2454 - accuracy: 0.59 - 1s 446ms/step - loss: 0.2478 - accuracy: 0.5312 - val_loss: 0.2430 - val_accuracy: 0.5630\n",
      "Epoch 163/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2497 - accuracy: 0.43 - 1s 494ms/step - loss: 0.2461 - accuracy: 0.4844 - val_loss: 0.2469 - val_accuracy: 0.5620\n",
      "Epoch 164/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2595 - accuracy: 0.53 - 1s 392ms/step - loss: 0.2588 - accuracy: 0.4844 - val_loss: 0.2498 - val_accuracy: 0.5625\n",
      "Epoch 165/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2536 - accuracy: 0.50 - 1s 573ms/step - loss: 0.2517 - accuracy: 0.5156 - val_loss: 0.2425 - val_accuracy: 0.5630\n",
      "Epoch 166/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2505 - accuracy: 0.43 - 1s 357ms/step - loss: 0.2459 - accuracy: 0.5938 - val_loss: 0.2466 - val_accuracy: 0.5620\n",
      "Epoch 167/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2437 - accuracy: 0.56 - 1s 391ms/step - loss: 0.2499 - accuracy: 0.5312 - val_loss: 0.2499 - val_accuracy: 0.5625\n",
      "Epoch 168/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2475 - accuracy: 0.59 - 1s 368ms/step - loss: 0.2498 - accuracy: 0.5781 - val_loss: 0.2424 - val_accuracy: 0.5630\n",
      "Epoch 169/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2510 - accuracy: 0.53 - 1s 482ms/step - loss: 0.2487 - accuracy: 0.5469 - val_loss: 0.2466 - val_accuracy: 0.5620\n",
      "Epoch 170/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2338 - accuracy: 0.59 - 1s 369ms/step - loss: 0.2367 - accuracy: 0.6094 - val_loss: 0.2497 - val_accuracy: 0.5625\n",
      "Epoch 171/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2603 - accuracy: 0.43 - 1s 349ms/step - loss: 0.2592 - accuracy: 0.3906 - val_loss: 0.2424 - val_accuracy: 0.5630\n",
      "Epoch 172/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2531 - accuracy: 0.46 - 1s 294ms/step - loss: 0.2562 - accuracy: 0.4375 - val_loss: 0.2466 - val_accuracy: 0.5620\n",
      "Epoch 173/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2609 - accuracy: 0.40 - 1s 374ms/step - loss: 0.2588 - accuracy: 0.4219 - val_loss: 0.2498 - val_accuracy: 0.5625\n",
      "Epoch 174/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2465 - accuracy: 0.62 - 1s 493ms/step - loss: 0.2485 - accuracy: 0.5625 - val_loss: 0.2433 - val_accuracy: 0.5630\n",
      "Epoch 175/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2488 - accuracy: 0.53 - 1s 530ms/step - loss: 0.2487 - accuracy: 0.5625 - val_loss: 0.2468 - val_accuracy: 0.5620\n",
      "Epoch 176/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2516 - accuracy: 0.46 - 1s 303ms/step - loss: 0.2421 - accuracy: 0.5312 - val_loss: 0.2498 - val_accuracy: 0.5310\n",
      "Epoch 177/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2529 - accuracy: 0.56 - 1s 357ms/step - loss: 0.2529 - accuracy: 0.5312 - val_loss: 0.2443 - val_accuracy: 0.5425\n",
      "Epoch 178/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2567 - accuracy: 0.37 - 1s 379ms/step - loss: 0.2576 - accuracy: 0.3906 - val_loss: 0.2471 - val_accuracy: 0.5725\n",
      "Epoch 179/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - ETA: 0s - loss: 0.2578 - accuracy: 0.40 - 1s 490ms/step - loss: 0.2517 - accuracy: 0.4688 - val_loss: 0.2499 - val_accuracy: 0.5730\n",
      "Epoch 180/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2541 - accuracy: 0.53 - 1s 405ms/step - loss: 0.2544 - accuracy: 0.4688 - val_loss: 0.2442 - val_accuracy: 0.5630\n",
      "Epoch 181/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2488 - accuracy: 0.50 - 1s 460ms/step - loss: 0.2483 - accuracy: 0.5625 - val_loss: 0.2473 - val_accuracy: 0.5620\n",
      "Epoch 182/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2484 - accuracy: 0.65 - 1s 350ms/step - loss: 0.2488 - accuracy: 0.6094 - val_loss: 0.2499 - val_accuracy: 0.5415\n",
      "Epoch 183/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2405 - accuracy: 0.56 - 1s 287ms/step - loss: 0.2428 - accuracy: 0.5469 - val_loss: 0.2455 - val_accuracy: 0.5323\n",
      "Epoch 184/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2494 - accuracy: 0.50 - 1s 364ms/step - loss: 0.2499 - accuracy: 0.4688 - val_loss: 0.2476 - val_accuracy: 0.5410\n",
      "Epoch 185/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2509 - accuracy: 0.53 - 1s 455ms/step - loss: 0.2475 - accuracy: 0.5781 - val_loss: 0.2500 - val_accuracy: 0.5625\n",
      "Epoch 186/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2515 - accuracy: 0.46 - 1s 539ms/step - loss: 0.2485 - accuracy: 0.5156 - val_loss: 0.2454 - val_accuracy: 0.5425\n",
      "Epoch 187/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2606 - accuracy: 0.34 - 1s 377ms/step - loss: 0.2539 - accuracy: 0.4688 - val_loss: 0.2476 - val_accuracy: 0.5410\n",
      "Epoch 188/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2470 - accuracy: 0.53 - 1s 472ms/step - loss: 0.2499 - accuracy: 0.5000 - val_loss: 0.2498 - val_accuracy: 0.5520\n",
      "Epoch 189/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2495 - accuracy: 0.56 - 1s 350ms/step - loss: 0.2443 - accuracy: 0.5781 - val_loss: 0.2457 - val_accuracy: 0.5425\n",
      "Epoch 190/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2393 - accuracy: 0.56 - 1s 510ms/step - loss: 0.2431 - accuracy: 0.5469 - val_loss: 0.2474 - val_accuracy: 0.5620\n",
      "Epoch 191/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2478 - accuracy: 0.53 - 1s 514ms/step - loss: 0.2531 - accuracy: 0.4531 - val_loss: 0.2499 - val_accuracy: 0.5625\n",
      "Epoch 192/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2515 - accuracy: 0.46 - 1s 430ms/step - loss: 0.2503 - accuracy: 0.5000 - val_loss: 0.2451 - val_accuracy: 0.5630\n",
      "Epoch 193/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2558 - accuracy: 0.50 - 1s 384ms/step - loss: 0.2556 - accuracy: 0.4375 - val_loss: 0.2473 - val_accuracy: 0.5620\n",
      "Epoch 194/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2497 - accuracy: 0.59 - 1s 335ms/step - loss: 0.2472 - accuracy: 0.5938 - val_loss: 0.2499 - val_accuracy: 0.5415\n",
      "Epoch 195/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2554 - accuracy: 0.34 - 1s 301ms/step - loss: 0.2528 - accuracy: 0.4062 - val_loss: 0.2454 - val_accuracy: 0.5630\n",
      "Epoch 196/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2535 - accuracy: 0.53 - 1s 379ms/step - loss: 0.2463 - accuracy: 0.5469 - val_loss: 0.2478 - val_accuracy: 0.5410\n",
      "Epoch 197/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2569 - accuracy: 0.43 - 1s 297ms/step - loss: 0.2542 - accuracy: 0.4219 - val_loss: 0.2500 - val_accuracy: 0.5310\n",
      "Epoch 198/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2573 - accuracy: 0.40 - 1s 352ms/step - loss: 0.2528 - accuracy: 0.5156 - val_loss: 0.2458 - val_accuracy: 0.5630\n",
      "Epoch 199/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2444 - accuracy: 0.62 - 1s 338ms/step - loss: 0.2482 - accuracy: 0.4844 - val_loss: 0.2476 - val_accuracy: 0.5620\n",
      "Epoch 200/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2527 - accuracy: 0.53 - 1s 383ms/step - loss: 0.2504 - accuracy: 0.5469 - val_loss: 0.2499 - val_accuracy: 0.5310\n",
      "Epoch 201/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2520 - accuracy: 0.40 - 1s 492ms/step - loss: 0.2507 - accuracy: 0.4688 - val_loss: 0.2458 - val_accuracy: 0.5528\n",
      "Epoch 202/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2487 - accuracy: 0.53 - 1s 439ms/step - loss: 0.2407 - accuracy: 0.5938 - val_loss: 0.2480 - val_accuracy: 0.5307\n",
      "Epoch 203/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2603 - accuracy: 0.53 - 1s 483ms/step - loss: 0.2582 - accuracy: 0.5000 - val_loss: 0.2499 - val_accuracy: 0.5415\n",
      "Epoch 204/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2571 - accuracy: 0.31 - 1s 321ms/step - loss: 0.2553 - accuracy: 0.3906 - val_loss: 0.2461 - val_accuracy: 0.5630\n",
      "Epoch 205/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2581 - accuracy: 0.40 - 1s 341ms/step - loss: 0.2550 - accuracy: 0.4375 - val_loss: 0.2479 - val_accuracy: 0.5620\n",
      "Epoch 206/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2422 - accuracy: 0.56 - 1s 308ms/step - loss: 0.2449 - accuracy: 0.5469 - val_loss: 0.2499 - val_accuracy: 0.5625\n",
      "Epoch 207/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2480 - accuracy: 0.53 - 1s 310ms/step - loss: 0.2486 - accuracy: 0.5000 - val_loss: 0.2462 - val_accuracy: 0.5528\n",
      "Epoch 208/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2493 - accuracy: 0.59 - 1s 330ms/step - loss: 0.2495 - accuracy: 0.5469 - val_loss: 0.2480 - val_accuracy: 0.5410\n",
      "Epoch 209/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2462 - accuracy: 0.59 - 1s 276ms/step - loss: 0.2488 - accuracy: 0.5938 - val_loss: 0.2500 - val_accuracy: 0.5517\n",
      "Epoch 210/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2525 - accuracy: 0.56 - 1s 332ms/step - loss: 0.2518 - accuracy: 0.5312 - val_loss: 0.2467 - val_accuracy: 0.5530\n",
      "Epoch 211/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2485 - accuracy: 0.65 - 1s 287ms/step - loss: 0.2471 - accuracy: 0.6406 - val_loss: 0.2477 - val_accuracy: 0.5410\n",
      "Epoch 212/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2496 - accuracy: 0.56 - 1s 349ms/step - loss: 0.2485 - accuracy: 0.6250 - val_loss: 0.2499 - val_accuracy: 0.5205\n",
      "Epoch 213/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2515 - accuracy: 0.50 - 1s 288ms/step - loss: 0.2490 - accuracy: 0.5156 - val_loss: 0.2458 - val_accuracy: 0.5323\n",
      "Epoch 214/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2483 - accuracy: 0.50 - 1s 367ms/step - loss: 0.2495 - accuracy: 0.5000 - val_loss: 0.2477 - val_accuracy: 0.5410\n",
      "Epoch 215/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2472 - accuracy: 0.65 - 1s 497ms/step - loss: 0.2463 - accuracy: 0.6094 - val_loss: 0.2499 - val_accuracy: 0.5517\n",
      "Epoch 216/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2530 - accuracy: 0.50 - 1s 325ms/step - loss: 0.2466 - accuracy: 0.5156 - val_loss: 0.2456 - val_accuracy: 0.5323\n",
      "Epoch 217/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2544 - accuracy: 0.43 - 1s 484ms/step - loss: 0.2513 - accuracy: 0.5156 - val_loss: 0.2478 - val_accuracy: 0.5305\n",
      "Epoch 218/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2561 - accuracy: 0.37 - 1s 513ms/step - loss: 0.2466 - accuracy: 0.5625 - val_loss: 0.2499 - val_accuracy: 0.5517\n",
      "Epoch 219/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2518 - accuracy: 0.50 - 1s 398ms/step - loss: 0.2501 - accuracy: 0.5469 - val_loss: 0.2461 - val_accuracy: 0.5425\n",
      "Epoch 220/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2441 - accuracy: 0.75 - 1s 439ms/step - loss: 0.2465 - accuracy: 0.6250 - val_loss: 0.2482 - val_accuracy: 0.5622\n",
      "Epoch 221/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2486 - accuracy: 0.43 - 1s 351ms/step - loss: 0.2490 - accuracy: 0.4844 - val_loss: 0.2501 - val_accuracy: 0.5733\n",
      "Epoch 222/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2424 - accuracy: 0.56 - 1s 363ms/step - loss: 0.2484 - accuracy: 0.5312 - val_loss: 0.2478 - val_accuracy: 0.5733\n",
      "Epoch 223/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2522 - accuracy: 0.46 - 1s 323ms/step - loss: 0.2532 - accuracy: 0.4688 - val_loss: 0.2486 - val_accuracy: 0.5615\n",
      "Epoch 224/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2419 - accuracy: 0.65 - 1s 384ms/step - loss: 0.2481 - accuracy: 0.5781 - val_loss: 0.2500 - val_accuracy: 0.5733\n",
      "Epoch 225/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2552 - accuracy: 0.40 - 1s 363ms/step - loss: 0.2524 - accuracy: 0.5156 - val_loss: 0.2472 - val_accuracy: 0.5733\n",
      "Epoch 226/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2518 - accuracy: 0.56 - 1s 287ms/step - loss: 0.2511 - accuracy: 0.5000 - val_loss: 0.2487 - val_accuracy: 0.5825\n",
      "Epoch 227/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2489 - accuracy: 0.46 - 1s 371ms/step - loss: 0.2492 - accuracy: 0.5156 - val_loss: 0.2502 - val_accuracy: 0.5832\n",
      "Epoch 228/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2532 - accuracy: 0.46 - 1s 298ms/step - loss: 0.2509 - accuracy: 0.5000 - val_loss: 0.2474 - val_accuracy: 0.5838\n",
      "Epoch 229/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2398 - accuracy: 0.65 - 1s 312ms/step - loss: 0.2463 - accuracy: 0.5312 - val_loss: 0.2486 - val_accuracy: 0.5722\n",
      "Epoch 230/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2553 - accuracy: 0.40 - 1s 338ms/step - loss: 0.2564 - accuracy: 0.4062 - val_loss: 0.2501 - val_accuracy: 0.5838\n",
      "Epoch 231/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2648 - accuracy: 0.31 - 1s 305ms/step - loss: 0.2568 - accuracy: 0.4219 - val_loss: 0.2478 - val_accuracy: 0.5733\n",
      "Epoch 232/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2608 - accuracy: 0.37 - 1s 350ms/step - loss: 0.2555 - accuracy: 0.4375 - val_loss: 0.2486 - val_accuracy: 0.5825\n",
      "Epoch 233/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2490 - accuracy: 0.59 - 1s 289ms/step - loss: 0.2504 - accuracy: 0.5469 - val_loss: 0.2501 - val_accuracy: 0.5733\n",
      "Epoch 234/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2534 - accuracy: 0.46 - 1s 358ms/step - loss: 0.2503 - accuracy: 0.5469 - val_loss: 0.2483 - val_accuracy: 0.5730\n",
      "Epoch 235/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2474 - accuracy: 0.53 - 1s 357ms/step - loss: 0.2506 - accuracy: 0.5312 - val_loss: 0.2489 - val_accuracy: 0.5727\n",
      "Epoch 236/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2537 - accuracy: 0.46 - 1s 270ms/step - loss: 0.2536 - accuracy: 0.4219 - val_loss: 0.2500 - val_accuracy: 0.5733\n",
      "Epoch 237/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2572 - accuracy: 0.34 - 1s 355ms/step - loss: 0.2524 - accuracy: 0.3906 - val_loss: 0.2473 - val_accuracy: 0.5523\n",
      "Epoch 238/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2497 - accuracy: 0.53 - 1s 299ms/step - loss: 0.2469 - accuracy: 0.5469 - val_loss: 0.2481 - val_accuracy: 0.5520\n",
      "Epoch 239/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2540 - accuracy: 0.46 - 1s 439ms/step - loss: 0.2509 - accuracy: 0.5312 - val_loss: 0.2499 - val_accuracy: 0.5625\n",
      "Epoch 240/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2465 - accuracy: 0.56 - 1s 351ms/step - loss: 0.2475 - accuracy: 0.5781 - val_loss: 0.2479 - val_accuracy: 0.5523\n",
      "Epoch 241/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2520 - accuracy: 0.46 - 1s 282ms/step - loss: 0.2490 - accuracy: 0.5156 - val_loss: 0.2492 - val_accuracy: 0.5622\n",
      "Epoch 242/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2433 - accuracy: 0.53 - 1s 370ms/step - loss: 0.2438 - accuracy: 0.5469 - val_loss: 0.2501 - val_accuracy: 0.5625\n",
      "Epoch 243/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2580 - accuracy: 0.34 - 1s 285ms/step - loss: 0.2580 - accuracy: 0.4219 - val_loss: 0.2477 - val_accuracy: 0.5733\n",
      "Epoch 244/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2527 - accuracy: 0.40 - 1s 502ms/step - loss: 0.2478 - accuracy: 0.5000 - val_loss: 0.2484 - val_accuracy: 0.5722\n",
      "Epoch 245/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2453 - accuracy: 0.43 - 1s 362ms/step - loss: 0.2484 - accuracy: 0.4688 - val_loss: 0.2499 - val_accuracy: 0.5733\n",
      "Epoch 246/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2490 - accuracy: 0.56 - 1s 303ms/step - loss: 0.2494 - accuracy: 0.5625 - val_loss: 0.2483 - val_accuracy: 0.5625\n",
      "Epoch 247/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2490 - accuracy: 0.53 - 1s 469ms/step - loss: 0.2498 - accuracy: 0.5625 - val_loss: 0.2492 - val_accuracy: 0.5520\n",
      "Epoch 248/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2448 - accuracy: 0.56 - 1s 413ms/step - loss: 0.2460 - accuracy: 0.5312 - val_loss: 0.2501 - val_accuracy: 0.5625\n",
      "Epoch 249/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2554 - accuracy: 0.43 - 1s 372ms/step - loss: 0.2544 - accuracy: 0.4219 - val_loss: 0.2475 - val_accuracy: 0.5733\n",
      "Epoch 250/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2516 - accuracy: 0.50 - 1s 401ms/step - loss: 0.2533 - accuracy: 0.4688 - val_loss: 0.2485 - val_accuracy: 0.5620\n",
      "Epoch 251/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2505 - accuracy: 0.40 - 1s 489ms/step - loss: 0.2531 - accuracy: 0.4688 - val_loss: 0.2498 - val_accuracy: 0.5523\n",
      "Epoch 252/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2526 - accuracy: 0.50 - 1s 346ms/step - loss: 0.2494 - accuracy: 0.5625 - val_loss: 0.2483 - val_accuracy: 0.5523\n",
      "Epoch 253/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2494 - accuracy: 0.53 - 1s 439ms/step - loss: 0.2504 - accuracy: 0.5000 - val_loss: 0.2492 - val_accuracy: 0.5520\n",
      "Epoch 254/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2485 - accuracy: 0.50 - 1s 301ms/step - loss: 0.2394 - accuracy: 0.5781 - val_loss: 0.2499 - val_accuracy: 0.5730\n",
      "Epoch 255/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2536 - accuracy: 0.56 - 1s 331ms/step - loss: 0.2497 - accuracy: 0.5312 - val_loss: 0.2474 - val_accuracy: 0.5628\n",
      "Epoch 256/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2520 - accuracy: 0.50 - 1s 289ms/step - loss: 0.2537 - accuracy: 0.5000 - val_loss: 0.2483 - val_accuracy: 0.5517\n",
      "Epoch 257/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2570 - accuracy: 0.43 - 1s 493ms/step - loss: 0.2556 - accuracy: 0.4531 - val_loss: 0.2499 - val_accuracy: 0.5735\n",
      "Epoch 258/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2535 - accuracy: 0.46 - 1s 362ms/step - loss: 0.2501 - accuracy: 0.5469 - val_loss: 0.2474 - val_accuracy: 0.5730\n",
      "Epoch 259/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2506 - accuracy: 0.46 - 1s 288ms/step - loss: 0.2486 - accuracy: 0.5312 - val_loss: 0.2484 - val_accuracy: 0.5725\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00259: early stopping\n"
     ]
    }
   ],
   "source": [
    "#GRU model\n",
    "model = Sequential()\n",
    "model.add(layers.GRU(32,\n",
    "                     dropout=0.3,\n",
    "                     recurrent_dropout=0.2,\n",
    "                     return_sequences=True,\n",
    "                     input_shape=(None, normalized_df1.shape[-1])))\n",
    "model.add(layers.GRU(64, activation='relu',\n",
    "                     dropout=0.3,\n",
    "                     recurrent_dropout=0.2))\n",
    "model.add(layers.Dense(2, activation='softmax'))\n",
    "model.compile(optimizer=RMSprop(), loss='mean_squared_error', metrics=['accuracy'])\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,\n",
    "                       patience=100, min_delta=0.0001, restore_best_weights = True)\n",
    "    \n",
    "history = model.fit_generator(train_gen,\n",
    "                              steps_per_epoch=2,\n",
    "                              epochs=500,\n",
    "                              validation_data=val_gen,\n",
    "                              validation_steps=val_steps,\n",
    "                              callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test acc: 0.40625\n",
      "test_loss: 0.26183515787124634\n"
     ]
    }
   ],
   "source": [
    "#Evaluating the model \n",
    "test_loss, test_acc = model.evaluate_generator(test_gen, steps=4)\n",
    "print('test acc:', test_acc)\n",
    "print(\"test_loss:\", test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
